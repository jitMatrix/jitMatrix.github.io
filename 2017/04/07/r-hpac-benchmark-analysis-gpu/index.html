<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>R benchmark for High-Performance Analytics and Computing (II): GPU Packages | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="1. OverviewIn the previous post (here), we have analyzed the performance gain of R in the heterogeneous system by accelerators, including NVIDIA GPU and Intel Xeon Phi. Furthermore, GPU accelerated pa">
<meta property="og:type" content="article">
<meta property="og:title" content="R benchmark for High-Performance Analytics and Computing (II): GPU Packages">
<meta property="og:url" content="https://jitmatrix.github.io/oneXPU/2017/04/07/r-hpac-benchmark-analysis-gpu/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="1. OverviewIn the previous post (here), we have analyzed the performance gain of R in the heterogeneous system by accelerators, including NVIDIA GPU and Intel Xeon Phi. Furthermore, GPU accelerated pa">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://www.parallelr.com/wp-content/uploads/2017/03/Figure1-R-benchmark-2-1024x587.png">
<meta property="og:image" content="http://www.parallelr.com/wp-content/uploads/2017/03/Table1-R-benchmark-2-1024x362.png">
<meta property="og:image" content="http://www.parallelr.com/wp-content/uploads/2017/03/Table2-R-benchmark-2-1024x351.png">
<meta property="og:image" content="http://www.parallelr.com/wp-content/uploads/2017/03/Table3-R-benchmark-2-1024x731.png">
<meta property="og:image" content="http://www.parallelr.com/wp-content/uploads/2017/03/Table4-R-benchmark-2-1024x275.png">
<meta property="og:image" content="http://www.parallelr.com/wp-content/uploads/2017/03/Figure2-R-benchmark-2-1024x557.png">
<meta property="og:image" content="http://www.parallelr.com/wp-content/uploads/2017/03/Table5-R-benchmark-2-1024x198.png">
<meta property="og:image" content="http://www.parallelr.com/wp-content/uploads/2017/03/Table6-R-benchmark-2-1024x277.png">
<meta property="og:image" content="http://www.parallelr.com/wp-content/uploads/2017/03/Figure3-R-benchmark-2-1024x634.png">
<meta property="og:image" content="http://www.parallelr.com/wp-content/uploads/2017/03/Table7-R-benchmark-2-1024x274.png">
<meta property="og:image" content="http://www.parallelr.com/wp-content/uploads/2017/03/Figure4-R-benchmark-2-1024x690.png">
<meta property="article:published_time" content="2017-04-07T12:38:10.000Z">
<meta property="article:modified_time" content="2020-12-19T06:31:14.159Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="rstats">
<meta property="article:tag" content="R">
<meta property="article:tag" content="CUDA">
<meta property="article:tag" content="cuBLAS">
<meta property="article:tag" content="GPU">
<meta property="article:tag" content="gmatrix">
<meta property="article:tag" content="gpuR">
<meta property="article:tag" content="gputools">
<meta property="article:tag" content="nvblas">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://www.parallelr.com/wp-content/uploads/2017/03/Figure1-R-benchmark-2-1024x587.png">
  
    <link rel="alternate" href="/oneXPU/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/oneXPU/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/oneXPU/css/style.css">

  
    
<link rel="stylesheet" href="/oneXPU/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/oneXPU/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/oneXPU/">Home</a>
        
          <a class="main-nav-link" href="/oneXPU/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/oneXPU/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://jitmatrix.github.io/oneXPU"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-r-hpac-benchmark-analysis-gpu" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/oneXPU/2017/04/07/r-hpac-benchmark-analysis-gpu/" class="article-date">
  <time class="dt-published" datetime="2017-04-07T12:38:10.000Z" itemprop="datePublished">2017-04-07</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/oneXPU/categories/GPGPU/">GPGPU</a>►<a class="article-category-link" href="/oneXPU/categories/GPGPU/Intel-Xeon-Phi/">Intel Xeon Phi</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      R benchmark for High-Performance Analytics and Computing (II): GPU Packages
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h1><p>In the previous post (<a target="_blank" rel="noopener" href="http://www.parallelr.com/r-hpac-benchmark-analysis/">here</a>), we have analyzed the performance gain of R in the heterogeneous system by accelerators, including NVIDIA GPU and Intel Xeon Phi. Furthermore, GPU accelerated packages can greatly improve the performance of R. Figure 1 shows the download statistics of CRAN over the years. Obviously, GPU is more and more recognized by the R community. <img src="http://www.parallelr.com/wp-content/uploads/2017/03/Figure1-R-benchmark-2-1024x587.png"></p>
<p>Figure 1 Download statistics of CRAN Package applied to the GPGPU environment over the years</p>
<h1 id="2-GPU-accelerated-packages"><a href="#2-GPU-accelerated-packages" class="headerlink" title="2. GPU-accelerated packages"></a>2. GPU-accelerated packages</h1><p>Matrix operation (BLAS) is one of the most important operations in data analysis, such as co-matrices in recommended systems and convolution calculations in deep learning. The matrix and the vector multiplication and other standard operations in R can be accelerated by GPU significantly. The most simple way to use GPU in R is through the nvBLAS (cuBLAS) library provided by NVIDIA, but other GPU accelerated packages (gputools, gmatrix, gpuR) provides a richer software and hardware interface, as shown in Table 1.</p>
<p>Table 1 Function comparison of the R package supported for GPGPU</p>
<p><img src="http://www.parallelr.com/wp-content/uploads/2017/03/Table1-R-benchmark-2-1024x362.png"></p>
<h3 id="2-1-Data-type"><a href="#2-1-Data-type" class="headerlink" title="2.1 Data type"></a>2.1 Data type</h3><p>Double-precision (DP) is used in nvBLAS and gputools to align with the default double precision mode of R. On the other hand, gmatrix and GPU can be more flexible for the user to choice single-precision (SP) or double-precision (DP) in the computations. And even gmatrix can support integer matrix computing. The SP computation mode can significant leverage the capability in low-mid level NVIDIA GPU cards, such as Tesla M40 and Geforce series GPUs, where the main computing power from SP floating-point operations. Therefore, this is a good news common desktop GPU users.</p>
<h3 id="2-2-Data-transfer"><a href="#2-2-Data-transfer" class="headerlink" title="2.2 Data transfer"></a>2.2 Data transfer</h3><p>When using the GPU as a coprocessor to speed up the application, the cost of data transfer usually takes a significant portion of the time, and the size of the GPU’s built-in memory will also limit whether the application can be executed. One of the main advantages of nvBLAS is that it supports block-based data copy and calculations between CPU and GPU, so the memory required from R code can be large than built-in GPU memory. However, the host-to-device memory copy, the calculation, and the final device-to-host results are performed in a synchronized mode so that the user cannot isolate the data transfer and calculation in nvBLAS. gmatrix and gpuR provide the asynchronous mode of communication and calculation, the user can separate the data copy and the real calculation. For example, gpuR provided in the vcl * series API, it will return in the R console immediately and then R will execute the next CPU command, while GPU is computing. In this way, both CPU and GPU are working simultaneously. And we can get much more performance boost.</p>
<h3 id="2-3-Programming-model"><a href="#2-3-Programming-model" class="headerlink" title="2.3 Programming model"></a>2.3 Programming model</h3><p>Frist, nvBLAS, gmatrix are based on the CUDA programming model and it will show better performance in NVIDIA series of GPUs but the shortage is poor portability. Then, gpuR is based on OpenCL, a standard heterogeneous programming interface, and more flexible. The user program of OpenCL can be executed on much more platforms, such as CPU, GPGPU, Intel Xeon Phi and FPGA.</p>
<h1 id="3-Performance-Benchmark-and-Analysis"><a href="#3-Performance-Benchmark-and-Analysis" class="headerlink" title="3. Performance Benchmark and Analysis"></a>3. Performance Benchmark and Analysis</h1><h3 id="3-1-Test-environment"><a href="#3-1-Test-environment" class="headerlink" title="3.1 Test environment"></a>3.1 Test environment</h3><p>The test is performed on the Ali cloud HPC platform: G2 server with NVIDIA Tesla K40m, G4 server with Tesla M40. Ali cloud HPC provides independent physical server + GPU accelerator card without virtualization overhead for computing-intensive applications. Regarding GPU equipment, K40m is designed with Kepler architecture while M40 with Maxwell architecture. The M40 targets for deep learning markets especially for training. Its SP floating-point peak performance reaches 7TFlops, but DP is only 0.2TFlops.</p>
<p>Table 2 Ali cloud hardware platform configuration</p>
<p><img src="http://www.parallelr.com/wp-content/uploads/2017/03/Table2-R-benchmark-2-1024x351.png"></p>
<p>Table 3 List of M40 and K40m hardware parameters</p>
<p><img src="http://www.parallelr.com/wp-content/uploads/2017/03/Table3-R-benchmark-2-1024x731.png"></p>
<p>Table 4 Software of test used</p>
<p><img src="http://www.parallelr.com/wp-content/uploads/2017/03/Table4-R-benchmark-2-1024x275.png"></p>
<h3 id="3-2-Performance-Analysis-on-K40-for-double-precision"><a href="#3-2-Performance-Analysis-on-K40-for-double-precision" class="headerlink" title="3.2 Performance Analysis on K40 for double precision"></a>3.2 Performance Analysis on K40 for double precision</h3><p>First, let’s compare the double-precision performance of each package on the Tesla series. We use nvblas performance as the baseline and compare the calculation time of three different sizes of matrix multiplications. In the testing code as below, we only counted the execution time of core API (% <em>%, gemm, gpuMatMult) following depth analysis. \</em>R code for gpuR, gmatrix, gputools and nvblas with DP calculation mode [code language=”r”] library(gpuR) for(i in seq(1:7)) { ORDER = 256*(2^i) A = matrix(rnorm(ORDER^2), nrow=ORDER) B = matrix(rnorm(ORDER^2), nrow=ORDER) gpuA = gpuMatrix(A, type=”double”) gpuB = gpuMatrix(B, type=”double”) cputime = system.time({gpuC = gpuA %<em>% gpuB})[3] } library(gmatrix) for(i in seq(1:7)) { ORDER = 256</em>(2^i) A = gmatrix(rnorm(ORDER^2),ORDER,ORDER) B = gmatrix(rnorm(ORDER^2),ORDER,ORDER) C = gmatrix(0,ORDER,ORDER) cputime = system.time({gmm(A,B,C)})[3] } library(gputools) for(i in seq(1:7)) { ORDER = 256*(2^i) A = matrix(rnorm(ORDER^2), nrow=ORDER) B = matrix(rnorm(ORDER^2), nrow=ORDER) cputime = system.time({C = gpuMatMult(A, B)})[3] } # nvblas, native code + PRE_LOADED for(i in seq(1:7)) { ORDER = 256*(2^i) A = matrix(rnorm(ORDER^2), nrow=ORDER) B = matrix(rnorm(ORDER^2), nrow=ORDER) cputime = system.time({C = A %*% B})[3] } [/code]   <img src="http://www.parallelr.com/wp-content/uploads/2017/03/Figure2-R-benchmark-2-1024x557.png"></p>
<p>Figure 2 Performance of the software package with the size change</p>
<p>In general, nvblas, gputools and gmatrix show very similar performance, because they use cuBLAS as the backend. gpuR’s performance is relatively low and variety with input sizes, such as the 4096 matrix only achieves 20% of nvblas performance but 8192 matrices can reach ~70%. From computation pattern, gputools and gmatrix apply dgemm_sm_heavy_ldg_nn API interfaces of cuBLAS to complete the matrix calculations, and computational efficiency will be slightly higher than nvblas of the block matrix calculation mode. From memory usage, as Figure 2, nvblas is the only one able to complete the large memory (out of cores/memory) calculation. For the largest matrix 32768, GPU packages (gputools, gmatrix, gpuR) will throw an exception of memory overflow. More details In Table 5, input matrix are divided into many small pieces, and then are transmitted to the GPU for computations by nvblas.</p>
<p>Table 5. Analysis of memory copy times from nvprof</p>
<p><img src="http://www.parallelr.com/wp-content/uploads/2017/03/Table5-R-benchmark-2-1024x198.png"> For gmatrix, matrix (A, B, C for C = A*B) are copied to GPU and C matrix stored in the GPU side after the calculation, involving three times host-to-device data transfer and without device-to-host transfer. For gputools matrix (A, B) are copied to GPU, the result matrix ( C ) is copied back to the host side so totally twice host-to-device and once device-to-host data transfer. Because the host-to-device data transfer is faster than device-to-host, gmatrix could get better performance than gputools as table 6 shown. Finally, we take a look at gpuR performance. The matrix calculation leverages OpenCL API that the performance is less optimized on NVIDIA GPU in table 6. GEMM compute kernel _prod_TT is much slower than gputools and gmatrix. Take 8192 for example, the calculation time of cublas API is 911.4 ms and 912.3 ms for gputools and gmatrix while OpenCL is 2172.5 ms for gpuR.</p>
<p>Table 6 Time overhead on GPU side at matrix size of 8192 * 8192</p>
<p><img src="http://www.parallelr.com/wp-content/uploads/2017/03/Table6-R-benchmark-2-1024x277.png"></p>
<h3 id="3-3-Performance-Analysis-on-M40-for-single-precision"><a href="#3-3-Performance-Analysis-on-M40-for-single-precision" class="headerlink" title="3.3 Performance Analysis on M40 for single precision"></a>3.3 Performance Analysis on M40 for single precision</h3><p>Single precision is quite important for data scientists but openBLAS, nvblas, and gputools use default double-precision (DP) calculation mode of R. So, it will lack competition in some hardware such as Tesla M40 where the DP performance is only 0.2T. In this parts, we will show you how to leverage SP performance in R by gmatrix and gpuR. In the blow testing, we take openBLAS performance results as the baseline. *R code of gmatrix and gpuR with SP calculation mode [code language=”r”] library(gpuR) for(i in seq(1:7)) { ORDER = 256*(2^i) A = matrix(rnorm(ORDER^2), nrow=ORDER) B = matrix(rnorm(ORDER^2), nrow=ORDER) gpuA = gpuMatrix(A, type=”float”) gpuB = gpuMatrix(B, type=”float”) cputime = system.time({gpuC = gpuA %<em>% gpuB})[3] } library(gmatrix) for(i in seq(1:7)) { ORDER = 256</em>(2^i) A = gmatrix(rnorm(ORDER^2),ORDER,ORDER, type=”single”) B = gmatrix(rnorm(ORDER^2),ORDER,ORDER, type=”single”) C = gmatrix(0,ORDER,ORDER, type=”single”) cputime = system.time({ gmm(A,B,C); h(C); })[3] } [/code] In Figure 3, gmatrix and gpuR with SP calculation model show a very good performance boost. For the 4096 matrix size, gmatrix is <strong>18X faster</strong> than openBLAS and <strong>37X faster</strong> (18.22 / 0.51) than nvblas. <img src="http://www.parallelr.com/wp-content/uploads/2017/03/Figure3-R-benchmark-2-1024x634.png"></p>
<p>Figure 3 Performance with SP mode on M40</p>
<p>More details in Table 7, it is obvious that the computation time of SP is much less than the calculation time of DP. The calculation time of DP is about 6000 ms (nvblas, gputools), while the calculation time of SP is only about 200 ms (gmatrix) and 500 ms (gpuR). From the memory point of view, gpuR on CPU uses SP data type and gmatrix on CPU is still DP. From Table 7, we can see that memory transfer time of gputools and gmatrix is almost same, and gpuR memory transfer time is only half of it (gmatrix 153.4 ms .vs. gpuR 77.7 ms). So, gpuR are more efficient in memory usage for SP and will good for the small size of computations. <em>Note, gmatrix does not use MEM D2H by default. In order to compare memory transfer performance with other packages, H (C) is added into the source code to make a consistent comparison.</em></p>
<p>Table 7 SP/DP performance of each Package on the M40 with matrix size of 8192*8192</p>
<p><img src="http://www.parallelr.com/wp-content/uploads/2017/03/Table7-R-benchmark-2-1024x274.png"></p>
<p>Note: GEMM kernel API on M40 is magma_lds128_dgemm_kernel.</p>
<h3 id="3-4-Asynchronous-Mode"><a href="#3-4-Asynchronous-Mode" class="headerlink" title="3.4 Asynchronous Mode"></a>3.4 Asynchronous Mode</h3><p>For the advanced user, gpuR provides a set of asynchronous mode interface. By using asynchronous interfaces, the R program will immediately return to the CPU program side after calling the interface of vcl <em>, and the user can continue to perform other tasks on the CPU. When the user explicitly accesses and use vcl * data, if the calculation has not yet completed, R will continue to wait; if the calculation has been completed, users can directly use. Therefore, users can use concurrency of CPU and GPU to hide the communication and computing time on GPU. In Figure 4, we compared the computing time between gpuR in asynchronous mode and gmatrix in synchronous mode (gmatrix shows the best performance in synchronous mode testing). As figure 4 shown, the sync-API execution time increases as the computational task increases but async-API keep a very tiny cost for all input size because the async-API do not include any actual calculations and just returns immediately. So, in the best case, we can hide all GPU execution time with CPU computation with a very tiny overhead. *gpuR running code with SP in asynchronous mode [code language=”r”] library(gpuR) for(i in seq(1:7)) { ORDER = 256\</em>(2^i) vclA_f = vclMatrix(A, nrow = ORDER, type=”float”) vclB_f = vclMatrix(B, nrow = ORDER, type=”float”) cputime = system.time({vclC_f = vclA_f %*% vclB_f})[3] } [/code]   <img src="http://www.parallelr.com/wp-content/uploads/2017/03/Figure4-R-benchmark-2-1024x690.png"></p>
<p>Figure 4. Performance comparison between gpuR in asynchronous mode and gmatrix in synchronization mode</p>
<h1 id="4-Conclusions-and-recommendations"><a href="#4-Conclusions-and-recommendations" class="headerlink" title="4. Conclusions and recommendations"></a>4. Conclusions and recommendations</h1><p>In this blog, we analyze the performance of the most popular GPU computing package. Each package has its own unique, but also have their own advantages and disadvantages. In practices, we need to choose according to specific needs. Based on the calculation platform, the calculation mode and the ease of use, it is recommended as follows: 1） nvblas is suitable for: • NVIDIA GPU card • Double precision calculation • Large memory consumption of the calculation, nvblas provides a very good performance and scalability; • Beginners 2） gputools is suitable for: • NVIDIA GPU card • Double precision calculation • Easy to use, and same API interface with R • Beginners 3） gmatrix is suitable for： • NVIDIA GPU card • Single/Double precision calculation • Multilevel BLAS interface（level 1，2，3） • More extension in GPU (colsum, sort) • Memory transfer optimization but the user needs to know where the memory is saved • Intermediate/Senior users  or R developers 4） gpuR is suitable for: • Single/Double precision calculation • Multilevel BLAS interface（level 1，2，3） • Heterogeneous systems work on most of the platforms such as AMD, Intel Xeon Phi, Intel GPUs • Asynchronous calculation mode, you can better hide the communication time • Intermediate/Senior users or R developers</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://jitmatrix.github.io/oneXPU/2017/04/07/r-hpac-benchmark-analysis-gpu/" data-id="ckivbq13e000r1jov8f0x25me" data-title="R benchmark for High-Performance Analytics and Computing (II): GPU Packages" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/CUDA/" rel="tag">CUDA</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/GPU/" rel="tag">GPU</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/R/" rel="tag">R</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/cuBLAS/" rel="tag">cuBLAS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/gmatrix/" rel="tag">gmatrix</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/gpuR/" rel="tag">gpuR</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/gputools/" rel="tag">gputools</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/nvblas/" rel="tag">nvblas</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/rstats/" rel="tag">rstats</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/oneXPU/2017/01/24/parallel-computation-with-r-and-xgboost/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Parallel Computation with R and XGBoost</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/">Accelerators</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/GPGPU/">GPGPU</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/GPGPU/MultiCores/">MultiCores</a></li><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/GPGPU/Performance-Optimizaiton/">Performance Optimizaiton</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/General/">General</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/General/GPGPU/">GPGPU</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/General/GPGPU/MultiCores/">MultiCores</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/General/GPGPU/MultiCores/Performance-Optimizaiton/">Performance Optimizaiton</a></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/General/MPI/">MPI</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/General/MPI/MultiCores/">MultiCores</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/General/MPI/MultiCores/Performance-Optimizaiton/">Performance Optimizaiton</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/General/MPI/MultiCores/Performance-Optimizaiton/Vectorization/">Vectorization</a></li></ul></li></ul></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/MultiCores/">MultiCores</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/MultiCores/Performance-Optimizaiton/">Performance Optimizaiton</a></li><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/MultiCores/Vectorization/">Vectorization</a></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/GPGPU/">GPGPU</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/GPGPU/Intel-Xeon-Phi/">Intel Xeon Phi</a></li><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/GPGPU/MultiCores/">MultiCores</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/GPGPU/MultiCores/Performance-Optimizaiton/">Performance Optimizaiton</a></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/General/">General</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/General/MultiCores/">MultiCores</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Uncategorized/">Uncategorized</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/BLAS/" rel="tag">BLAS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/CRAN/" rel="tag">CRAN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/CUDA/" rel="tag">CUDA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/GEMM/" rel="tag">GEMM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/GPU/" rel="tag">GPU</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/H2O/" rel="tag">H2O</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/HPAC/" rel="tag">HPAC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/HPC/" rel="tag">HPC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/MIC/" rel="tag">MIC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/MKL/" rel="tag">MKL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/MNIST/" rel="tag">MNIST</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/Maximum-Likelihood/" rel="tag">Maximum Likelihood</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/R/" rel="tag">R</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/Rcpp/" rel="tag">Rcpp</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/SSE/" rel="tag">SSE</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/Xeon/" rel="tag">Xeon</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/Xeon-Phi/" rel="tag">Xeon Phi</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/benchmark/" rel="tag">benchmark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/big-data/" rel="tag">big data</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/boost/" rel="tag">boost</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/classification/" rel="tag">classification</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/cuBLAS/" rel="tag">cuBLAS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/data-analytics/" rel="tag">data analytics</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/deep-learning/" rel="tag">deep learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/dnn/" rel="tag">dnn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/doMC/" rel="tag">doMC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/doParallel/" rel="tag">doParallel</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/foreach/" rel="tag">foreach</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/gmatrix/" rel="tag">gmatrix</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/gpuR/" rel="tag">gpuR</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/gputools/" rel="tag">gputools</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/high-performance/" rel="tag">high performance</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/iris/" rel="tag">iris</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/knn/" rel="tag">knn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/lightboost/" rel="tag">lightboost</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/machine-learing/" rel="tag">machine learing</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/machine-learning/" rel="tag">machine learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/mcapply/" rel="tag">mcapply</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/memory-usage/" rel="tag">memory usage</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/multicores/" rel="tag">multicores</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/multit/" rel="tag">multit</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/multithreads/" rel="tag">multithreads</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/mutlithreading/" rel="tag">mutlithreading</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/mutltiGPU/" rel="tag">mutltiGPU</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/neural-network/" rel="tag">neural network</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/nvblas/" rel="tag">nvblas</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/openMP/" rel="tag">openMP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/openblas/" rel="tag">openblas</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/parallel/" rel="tag">parallel</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/parallel-computing/" rel="tag">parallel computing</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/parlapply/" rel="tag">parlapply</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/performance-optimization/" rel="tag">performance optimization</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/profiling/" rel="tag">profiling</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/rstats/" rel="tag">rstats</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/sklearn/" rel="tag">sklearn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/snow/" rel="tag">snow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/xgboost/" rel="tag">xgboost</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/oneXPU/tags/BLAS/" style="font-size: 10px;">BLAS</a> <a href="/oneXPU/tags/CRAN/" style="font-size: 10px;">CRAN</a> <a href="/oneXPU/tags/CUDA/" style="font-size: 12px;">CUDA</a> <a href="/oneXPU/tags/GEMM/" style="font-size: 12px;">GEMM</a> <a href="/oneXPU/tags/GPU/" style="font-size: 14px;">GPU</a> <a href="/oneXPU/tags/H2O/" style="font-size: 12px;">H2O</a> <a href="/oneXPU/tags/HPAC/" style="font-size: 10px;">HPAC</a> <a href="/oneXPU/tags/HPC/" style="font-size: 12px;">HPC</a> <a href="/oneXPU/tags/MIC/" style="font-size: 10px;">MIC</a> <a href="/oneXPU/tags/MKL/" style="font-size: 14px;">MKL</a> <a href="/oneXPU/tags/MNIST/" style="font-size: 10px;">MNIST</a> <a href="/oneXPU/tags/Maximum-Likelihood/" style="font-size: 10px;">Maximum Likelihood</a> <a href="/oneXPU/tags/R/" style="font-size: 20px;">R</a> <a href="/oneXPU/tags/Rcpp/" style="font-size: 10px;">Rcpp</a> <a href="/oneXPU/tags/SSE/" style="font-size: 10px;">SSE</a> <a href="/oneXPU/tags/Xeon/" style="font-size: 10px;">Xeon</a> <a href="/oneXPU/tags/Xeon-Phi/" style="font-size: 10px;">Xeon Phi</a> <a href="/oneXPU/tags/benchmark/" style="font-size: 10px;">benchmark</a> <a href="/oneXPU/tags/big-data/" style="font-size: 10px;">big data</a> <a href="/oneXPU/tags/boost/" style="font-size: 10px;">boost</a> <a href="/oneXPU/tags/classification/" style="font-size: 10px;">classification</a> <a href="/oneXPU/tags/cuBLAS/" style="font-size: 16px;">cuBLAS</a> <a href="/oneXPU/tags/data-analytics/" style="font-size: 12px;">data analytics</a> <a href="/oneXPU/tags/deep-learning/" style="font-size: 14px;">deep learning</a> <a href="/oneXPU/tags/dnn/" style="font-size: 12px;">dnn</a> <a href="/oneXPU/tags/doMC/" style="font-size: 10px;">doMC</a> <a href="/oneXPU/tags/doParallel/" style="font-size: 10px;">doParallel</a> <a href="/oneXPU/tags/foreach/" style="font-size: 10px;">foreach</a> <a href="/oneXPU/tags/gmatrix/" style="font-size: 10px;">gmatrix</a> <a href="/oneXPU/tags/gpuR/" style="font-size: 10px;">gpuR</a> <a href="/oneXPU/tags/gputools/" style="font-size: 10px;">gputools</a> <a href="/oneXPU/tags/high-performance/" style="font-size: 10px;">high performance</a> <a href="/oneXPU/tags/iris/" style="font-size: 10px;">iris</a> <a href="/oneXPU/tags/knn/" style="font-size: 10px;">knn</a> <a href="/oneXPU/tags/lightboost/" style="font-size: 10px;">lightboost</a> <a href="/oneXPU/tags/machine-learing/" style="font-size: 10px;">machine learing</a> <a href="/oneXPU/tags/machine-learning/" style="font-size: 12px;">machine learning</a> <a href="/oneXPU/tags/mcapply/" style="font-size: 10px;">mcapply</a> <a href="/oneXPU/tags/memory-usage/" style="font-size: 10px;">memory usage</a> <a href="/oneXPU/tags/multicores/" style="font-size: 18px;">multicores</a> <a href="/oneXPU/tags/multit/" style="font-size: 10px;">multit</a> <a href="/oneXPU/tags/multithreads/" style="font-size: 10px;">multithreads</a> <a href="/oneXPU/tags/mutlithreading/" style="font-size: 10px;">mutlithreading</a> <a href="/oneXPU/tags/mutltiGPU/" style="font-size: 10px;">mutltiGPU</a> <a href="/oneXPU/tags/neural-network/" style="font-size: 10px;">neural network</a> <a href="/oneXPU/tags/nvblas/" style="font-size: 10px;">nvblas</a> <a href="/oneXPU/tags/openMP/" style="font-size: 12px;">openMP</a> <a href="/oneXPU/tags/openblas/" style="font-size: 12px;">openblas</a> <a href="/oneXPU/tags/parallel/" style="font-size: 10px;">parallel</a> <a href="/oneXPU/tags/parallel-computing/" style="font-size: 18px;">parallel computing</a> <a href="/oneXPU/tags/parlapply/" style="font-size: 10px;">parlapply</a> <a href="/oneXPU/tags/performance-optimization/" style="font-size: 16px;">performance optimization</a> <a href="/oneXPU/tags/profiling/" style="font-size: 12px;">profiling</a> <a href="/oneXPU/tags/python/" style="font-size: 10px;">python</a> <a href="/oneXPU/tags/rstats/" style="font-size: 20px;">rstats</a> <a href="/oneXPU/tags/sklearn/" style="font-size: 10px;">sklearn</a> <a href="/oneXPU/tags/snow/" style="font-size: 10px;">snow</a> <a href="/oneXPU/tags/xgboost/" style="font-size: 10px;">xgboost</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/oneXPU/archives/2017/04/">April 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/oneXPU/archives/2017/01/">January 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/oneXPU/archives/2016/09/">September 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/oneXPU/archives/2016/08/">August 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/oneXPU/archives/2016/07/">July 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/oneXPU/archives/2016/05/">May 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/oneXPU/archives/2016/04/">April 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/oneXPU/archives/2016/03/">March 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/oneXPU/archives/2016/02/">February 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/oneXPU/2017/04/07/r-hpac-benchmark-analysis-gpu/">R benchmark for High-Performance Analytics and Computing (II): GPU Packages</a>
          </li>
        
          <li>
            <a href="/oneXPU/2017/01/24/parallel-computation-with-r-and-xgboost/">Parallel Computation with R and XGBoost</a>
          </li>
        
          <li>
            <a href="/oneXPU/2016/09/10/r-with-parallel-computing/">R with Parallel Computing from User Perspectives</a>
          </li>
        
          <li>
            <a href="/oneXPU/2016/08/15/r-cran-package-modernization-openmp/">R and OpenMP:  CRAN Package Modernization</a>
          </li>
        
          <li>
            <a href="/oneXPU/2016/07/26/r-and-openmp-boosting-compiled-code-on-multi-core-cpu-s/">R and openMP: boosting compiled code on multi-core cpu-s</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2020 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/oneXPU/" class="mobile-nav-link">Home</a>
  
    <a href="/oneXPU/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/oneXPU/js/jquery-3.4.1.min.js"></script>



  
<script src="/oneXPU/fancybox/jquery.fancybox.min.js"></script>




<script src="/oneXPU/js/script.js"></script>





  </div>
</body>
</html>
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://jitmatrix.github.io/oneXPU/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://jitmatrix.github.io/oneXPU"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-r-hpac-benchmark-analysis-gpu" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2017/04/07/r-hpac-benchmark-analysis-gpu/" class="article-date">
  <time class="dt-published" datetime="2017-04-07T12:38:10.000Z" itemprop="datePublished">2017-04-07</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/GPGPU/">GPGPU</a>►<a class="article-category-link" href="/categories/GPGPU/Intel-Xeon-Phi/">Intel Xeon Phi</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2017/04/07/r-hpac-benchmark-analysis-gpu/">R benchmark for High-Performance Analytics and Computing (II): GPU Packages</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h1><p>In the previous post (<a target="_blank" rel="noopener" href="http://www.parallelr.com/r-hpac-benchmark-analysis/">here</a>), we have analyzed the performance gain of R in the heterogeneous system by accelerators, including NVIDIA GPU and Intel Xeon Phi. Furthermore, GPU accelerated packages can greatly improve the performance of R. Figure 1 shows the download statistics of CRAN over the years. Obviously, GPU is more and more recognized by the R community. <img src="http://www.parallelr.com/wp-content/uploads/2017/03/Figure1-R-benchmark-2-1024x587.png"></p>
<p>Figure 1 Download statistics of CRAN Package applied to the GPGPU environment over the years</p>
<h1 id="2-GPU-accelerated-packages"><a href="#2-GPU-accelerated-packages" class="headerlink" title="2. GPU-accelerated packages"></a>2. GPU-accelerated packages</h1><p>Matrix operation (BLAS) is one of the most important operations in data analysis, such as co-matrices in recommended systems and convolution calculations in deep learning. The matrix and the vector multiplication and other standard operations in R can be accelerated by GPU significantly. The most simple way to use GPU in R is through the nvBLAS (cuBLAS) library provided by NVIDIA, but other GPU accelerated packages (gputools, gmatrix, gpuR) provides a richer software and hardware interface, as shown in Table 1.</p>
<p>Table 1 Function comparison of the R package supported for GPGPU</p>
<p><img src="http://www.parallelr.com/wp-content/uploads/2017/03/Table1-R-benchmark-2-1024x362.png"></p>
<h3 id="2-1-Data-type"><a href="#2-1-Data-type" class="headerlink" title="2.1 Data type"></a>2.1 Data type</h3><p>Double-precision (DP) is used in nvBLAS and gputools to align with the default double precision mode of R. On the other hand, gmatrix and GPU can be more flexible for the user to choice single-precision (SP) or double-precision (DP) in the computations. And even gmatrix can support integer matrix computing. The SP computation mode can significant leverage the capability in low-mid level NVIDIA GPU cards, such as Tesla M40 and Geforce series GPUs, where the main computing power from SP floating-point operations. Therefore, this is a good news common desktop GPU users.</p>
<h3 id="2-2-Data-transfer"><a href="#2-2-Data-transfer" class="headerlink" title="2.2 Data transfer"></a>2.2 Data transfer</h3><p>When using the GPU as a coprocessor to speed up the application, the cost of data transfer usually takes a significant portion of the time, and the size of the GPU’s built-in memory will also limit whether the application can be executed. One of the main advantages of nvBLAS is that it supports block-based data copy and calculations between CPU and GPU, so the memory required from R code can be large than built-in GPU memory. However, the host-to-device memory copy, the calculation, and the final device-to-host results are performed in a synchronized mode so that the user cannot isolate the data transfer and calculation in nvBLAS. gmatrix and gpuR provide the asynchronous mode of communication and calculation, the user can separate the data copy and the real calculation. For example, gpuR provided in the vcl * series API, it will return in the R console immediately and then R will execute the next CPU command, while GPU is computing. In this way, both CPU and GPU are working simultaneously. And we can get much more performance boost.</p>
<h3 id="2-3-Programming-model"><a href="#2-3-Programming-model" class="headerlink" title="2.3 Programming model"></a>2.3 Programming model</h3><p>Frist, nvBLAS, gmatrix are based on the CUDA programming model and it will show better performance in NVIDIA series of GPUs but the shortage is poor portability. Then, gpuR is based on OpenCL, a standard heterogeneous programming interface, and more flexible. The user program of OpenCL can be executed on much more platforms, such as CPU, GPGPU, Intel Xeon Phi and FPGA.</p>
<h1 id="3-Performance-Benchmark-and-Analysis"><a href="#3-Performance-Benchmark-and-Analysis" class="headerlink" title="3. Performance Benchmark and Analysis"></a>3. Performance Benchmark and Analysis</h1><h3 id="3-1-Test-environment"><a href="#3-1-Test-environment" class="headerlink" title="3.1 Test environment"></a>3.1 Test environment</h3><p>The test is performed on the Ali cloud HPC platform: G2 server with NVIDIA Tesla K40m, G4 server with Tesla M40. Ali cloud HPC provides independent physical server + GPU accelerator card without virtualization overhead for computing-intensive applications. Regarding GPU equipment, K40m is designed with Kepler architecture while M40 with Maxwell architecture. The M40 targets for deep learning markets especially for training. Its SP floating-point peak performance reaches 7TFlops, but DP is only 0.2TFlops.</p>
<p>Table 2 Ali cloud hardware platform configuration</p>
<p><img src="http://www.parallelr.com/wp-content/uploads/2017/03/Table2-R-benchmark-2-1024x351.png"></p>
<p>Table 3 List of M40 and K40m hardware parameters</p>
<p><img src="http://www.parallelr.com/wp-content/uploads/2017/03/Table3-R-benchmark-2-1024x731.png"></p>
<p>Table 4 Software of test used</p>
<p><img src="http://www.parallelr.com/wp-content/uploads/2017/03/Table4-R-benchmark-2-1024x275.png"></p>
<h3 id="3-2-Performance-Analysis-on-K40-for-double-precision"><a href="#3-2-Performance-Analysis-on-K40-for-double-precision" class="headerlink" title="3.2 Performance Analysis on K40 for double precision"></a>3.2 Performance Analysis on K40 for double precision</h3><p>First, let’s compare the double-precision performance of each package on the Tesla series. We use nvblas performance as the baseline and compare the calculation time of three different sizes of matrix multiplications. In the testing code as below, we only counted the execution time of core API (% <em>%, gemm, gpuMatMult) following depth analysis. \</em>R code for gpuR, gmatrix, gputools and nvblas with DP calculation mode [code language=”r”] library(gpuR) for(i in seq(1:7)) { ORDER = 256*(2^i) A = matrix(rnorm(ORDER^2), nrow=ORDER) B = matrix(rnorm(ORDER^2), nrow=ORDER) gpuA = gpuMatrix(A, type=”double”) gpuB = gpuMatrix(B, type=”double”) cputime = system.time({gpuC = gpuA %<em>% gpuB})[3] } library(gmatrix) for(i in seq(1:7)) { ORDER = 256</em>(2^i) A = gmatrix(rnorm(ORDER^2),ORDER,ORDER) B = gmatrix(rnorm(ORDER^2),ORDER,ORDER) C = gmatrix(0,ORDER,ORDER) cputime = system.time({gmm(A,B,C)})[3] } library(gputools) for(i in seq(1:7)) { ORDER = 256*(2^i) A = matrix(rnorm(ORDER^2), nrow=ORDER) B = matrix(rnorm(ORDER^2), nrow=ORDER) cputime = system.time({C = gpuMatMult(A, B)})[3] } # nvblas, native code + PRE_LOADED for(i in seq(1:7)) { ORDER = 256*(2^i) A = matrix(rnorm(ORDER^2), nrow=ORDER) B = matrix(rnorm(ORDER^2), nrow=ORDER) cputime = system.time({C = A %*% B})[3] } [/code]   <img src="http://www.parallelr.com/wp-content/uploads/2017/03/Figure2-R-benchmark-2-1024x557.png"></p>
<p>Figure 2 Performance of the software package with the size change</p>
<p>In general, nvblas, gputools and gmatrix show very similar performance, because they use cuBLAS as the backend. gpuR’s performance is relatively low and variety with input sizes, such as the 4096 matrix only achieves 20% of nvblas performance but 8192 matrices can reach ~70%. From computation pattern, gputools and gmatrix apply dgemm_sm_heavy_ldg_nn API interfaces of cuBLAS to complete the matrix calculations, and computational efficiency will be slightly higher than nvblas of the block matrix calculation mode. From memory usage, as Figure 2, nvblas is the only one able to complete the large memory (out of cores/memory) calculation. For the largest matrix 32768, GPU packages (gputools, gmatrix, gpuR) will throw an exception of memory overflow. More details In Table 5, input matrix are divided into many small pieces, and then are transmitted to the GPU for computations by nvblas.</p>
<p>Table 5. Analysis of memory copy times from nvprof</p>
<p><img src="http://www.parallelr.com/wp-content/uploads/2017/03/Table5-R-benchmark-2-1024x198.png"> For gmatrix, matrix (A, B, C for C = A*B) are copied to GPU and C matrix stored in the GPU side after the calculation, involving three times host-to-device data transfer and without device-to-host transfer. For gputools matrix (A, B) are copied to GPU, the result matrix ( C ) is copied back to the host side so totally twice host-to-device and once device-to-host data transfer. Because the host-to-device data transfer is faster than device-to-host, gmatrix could get better performance than gputools as table 6 shown. Finally, we take a look at gpuR performance. The matrix calculation leverages OpenCL API that the performance is less optimized on NVIDIA GPU in table 6. GEMM compute kernel _prod_TT is much slower than gputools and gmatrix. Take 8192 for example, the calculation time of cublas API is 911.4 ms and 912.3 ms for gputools and gmatrix while OpenCL is 2172.5 ms for gpuR.</p>
<p>Table 6 Time overhead on GPU side at matrix size of 8192 * 8192</p>
<p><img src="http://www.parallelr.com/wp-content/uploads/2017/03/Table6-R-benchmark-2-1024x277.png"></p>
<h3 id="3-3-Performance-Analysis-on-M40-for-single-precision"><a href="#3-3-Performance-Analysis-on-M40-for-single-precision" class="headerlink" title="3.3 Performance Analysis on M40 for single precision"></a>3.3 Performance Analysis on M40 for single precision</h3><p>Single precision is quite important for data scientists but openBLAS, nvblas, and gputools use default double-precision (DP) calculation mode of R. So, it will lack competition in some hardware such as Tesla M40 where the DP performance is only 0.2T. In this parts, we will show you how to leverage SP performance in R by gmatrix and gpuR. In the blow testing, we take openBLAS performance results as the baseline. *R code of gmatrix and gpuR with SP calculation mode [code language=”r”] library(gpuR) for(i in seq(1:7)) { ORDER = 256*(2^i) A = matrix(rnorm(ORDER^2), nrow=ORDER) B = matrix(rnorm(ORDER^2), nrow=ORDER) gpuA = gpuMatrix(A, type=”float”) gpuB = gpuMatrix(B, type=”float”) cputime = system.time({gpuC = gpuA %<em>% gpuB})[3] } library(gmatrix) for(i in seq(1:7)) { ORDER = 256</em>(2^i) A = gmatrix(rnorm(ORDER^2),ORDER,ORDER, type=”single”) B = gmatrix(rnorm(ORDER^2),ORDER,ORDER, type=”single”) C = gmatrix(0,ORDER,ORDER, type=”single”) cputime = system.time({ gmm(A,B,C); h(C); })[3] } [/code] In Figure 3, gmatrix and gpuR with SP calculation model show a very good performance boost. For the 4096 matrix size, gmatrix is <strong>18X faster</strong> than openBLAS and <strong>37X faster</strong> (18.22 / 0.51) than nvblas. <img src="http://www.parallelr.com/wp-content/uploads/2017/03/Figure3-R-benchmark-2-1024x634.png"></p>
<p>Figure 3 Performance with SP mode on M40</p>
<p>More details in Table 7, it is obvious that the computation time of SP is much less than the calculation time of DP. The calculation time of DP is about 6000 ms (nvblas, gputools), while the calculation time of SP is only about 200 ms (gmatrix) and 500 ms (gpuR). From the memory point of view, gpuR on CPU uses SP data type and gmatrix on CPU is still DP. From Table 7, we can see that memory transfer time of gputools and gmatrix is almost same, and gpuR memory transfer time is only half of it (gmatrix 153.4 ms .vs. gpuR 77.7 ms). So, gpuR are more efficient in memory usage for SP and will good for the small size of computations. <em>Note, gmatrix does not use MEM D2H by default. In order to compare memory transfer performance with other packages, H (C) is added into the source code to make a consistent comparison.</em></p>
<p>Table 7 SP/DP performance of each Package on the M40 with matrix size of 8192*8192</p>
<p><img src="http://www.parallelr.com/wp-content/uploads/2017/03/Table7-R-benchmark-2-1024x274.png"></p>
<p>Note: GEMM kernel API on M40 is magma_lds128_dgemm_kernel.</p>
<h3 id="3-4-Asynchronous-Mode"><a href="#3-4-Asynchronous-Mode" class="headerlink" title="3.4 Asynchronous Mode"></a>3.4 Asynchronous Mode</h3><p>For the advanced user, gpuR provides a set of asynchronous mode interface. By using asynchronous interfaces, the R program will immediately return to the CPU program side after calling the interface of vcl <em>, and the user can continue to perform other tasks on the CPU. When the user explicitly accesses and use vcl * data, if the calculation has not yet completed, R will continue to wait; if the calculation has been completed, users can directly use. Therefore, users can use concurrency of CPU and GPU to hide the communication and computing time on GPU. In Figure 4, we compared the computing time between gpuR in asynchronous mode and gmatrix in synchronous mode (gmatrix shows the best performance in synchronous mode testing). As figure 4 shown, the sync-API execution time increases as the computational task increases but async-API keep a very tiny cost for all input size because the async-API do not include any actual calculations and just returns immediately. So, in the best case, we can hide all GPU execution time with CPU computation with a very tiny overhead. *gpuR running code with SP in asynchronous mode [code language=”r”] library(gpuR) for(i in seq(1:7)) { ORDER = 256\</em>(2^i) vclA_f = vclMatrix(A, nrow = ORDER, type=”float”) vclB_f = vclMatrix(B, nrow = ORDER, type=”float”) cputime = system.time({vclC_f = vclA_f %*% vclB_f})[3] } [/code]   <img src="http://www.parallelr.com/wp-content/uploads/2017/03/Figure4-R-benchmark-2-1024x690.png"></p>
<p>Figure 4. Performance comparison between gpuR in asynchronous mode and gmatrix in synchronization mode</p>
<h1 id="4-Conclusions-and-recommendations"><a href="#4-Conclusions-and-recommendations" class="headerlink" title="4. Conclusions and recommendations"></a>4. Conclusions and recommendations</h1><p>In this blog, we analyze the performance of the most popular GPU computing package. Each package has its own unique, but also have their own advantages and disadvantages. In practices, we need to choose according to specific needs. Based on the calculation platform, the calculation mode and the ease of use, it is recommended as follows: 1） nvblas is suitable for: • NVIDIA GPU card • Double precision calculation • Large memory consumption of the calculation, nvblas provides a very good performance and scalability; • Beginners 2） gputools is suitable for: • NVIDIA GPU card • Double precision calculation • Easy to use, and same API interface with R • Beginners 3） gmatrix is suitable for： • NVIDIA GPU card • Single/Double precision calculation • Multilevel BLAS interface（level 1，2，3） • More extension in GPU (colsum, sort) • Memory transfer optimization but the user needs to know where the memory is saved • Intermediate/Senior users  or R developers 4） gpuR is suitable for: • Single/Double precision calculation • Multilevel BLAS interface（level 1，2，3） • Heterogeneous systems work on most of the platforms such as AMD, Intel Xeon Phi, Intel GPUs • Asynchronous calculation mode, you can better hide the communication time • Intermediate/Senior users or R developers</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://jitmatrix.github.io/oneXPU/2017/04/07/r-hpac-benchmark-analysis-gpu/" data-id="ckivbjgok000r6lol5yia46g0" data-title="R benchmark for High-Performance Analytics and Computing (II): GPU Packages" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CUDA/" rel="tag">CUDA</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GPU/" rel="tag">GPU</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/R/" rel="tag">R</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cuBLAS/" rel="tag">cuBLAS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/gmatrix/" rel="tag">gmatrix</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/gpuR/" rel="tag">gpuR</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/gputools/" rel="tag">gputools</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/nvblas/" rel="tag">nvblas</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/rstats/" rel="tag">rstats</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-parallel-computation-with-r-and-xgboost" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2017/01/24/parallel-computation-with-r-and-xgboost/" class="article-date">
  <time class="dt-published" datetime="2017-01-24T05:05:13.000Z" itemprop="datePublished">2017-01-24</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/General/">General</a>►<a class="article-category-link" href="/categories/General/MultiCores/">MultiCores</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2017/01/24/parallel-computation-with-r-and-xgboost/">Parallel Computation with R and XGBoost</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p><a target="_blank" rel="noopener" href="https://github.com/dmlc/xgboost">XGBoost</a> is a comprehensive machine learning library for gradient boosting. It began from the Kaggle community for online machine learning challenges, and then maintained by the collaborative efforts from the developers in the community. It is well known for its accuracy, efficiency and flexibility for various interfaces: the computational module is implemented in C++, and currently provides interfaces for R, python, Julia and Java. Its corresponding R package, <a target="_blank" rel="noopener" href="https://github.com/dmlc/xgboost/tree/master/R-package">xgboost</a>, in this sense is non-typical in terms of the design and structure. Although it is common that an R package is a wrapper of another tool, not many packages have the backend supporting many ways of parallel computation. The structure of the Project can be illustrated as follows: <img src="http://www.parallelr.com/wp-content/uploads/2016/11/xgboost-struct-1-1024x570.png"> Although it is common that an R package is a wrapper of another tool, not many packages have the backend supporting many ways of parallel computation. In xgboost, most works are done in the C++ part in the above Figure. Since all interfaces share the same computational backend, there is not really a difference in terms of the accuracy or efficiency of the results from different interfaces. Users only need to prepare the data and parameters in the preferred language, then call the corresponding interface and wait for the training and prediction. This design puts most heavy works to the background, and only asks for the minimum support from each interface. For this reason, we can expect in the future there will be more languages wrapping the XGBoost backend and their users can enjoy the parallel training power. XGBoost implements a gradient boosting trees algorithm. A gradient boosting trees model trains a lot of decision trees or regression trees in a sequence, where only one tree is added to the model at a time, and every new tree depends on the previous trees. This nature limits the level of parallel computation, since we cannot build multiple trees simultaneously. Therefore, the parallel computation is introduced in a lower level, i.e. in the tree-building process at each step. <img src="http://www.parallelr.com/wp-content/uploads/2017/01/xgboost-2.png"> Specifically, the parallel computation takes place in the operation where the model scans through all features on each internal node and set a threshold. Say we have a 4-core CPU for the training computation, then XGBoost separate the features into 4 groups. For the splitting operation on a node, XGBoost distributes the operation on each feature to their corresponding core. The training data is stored in a piece of shared memory, each core only needs to access one group of features, and perform the computation individually. The implementation is done in C++ with the help of OpenMP. It is obvious that users can benefit fully from the parallel computation if the number of features is larger than the number of threads of the CPU. XGBoost also supports training on a cluster, or with external memory. We will briefly introduce them in the following parts.</p>
<hr>
<p>In the following part, we will demonstrate the performance of the R package with different parallel strategies. We hope this introduction can be an example of a computational efficient R package.</p>
<h2 id="1-Multi-threading-on-a-single-machine"><a href="#1-Multi-threading-on-a-single-machine" class="headerlink" title="1. Multi-threading on a single machine"></a><strong>1. Multi-threading on a single machine</strong></h2><p>XGBoost offers the option to parallel the training process in an implicit style on a single machine, which could be a workstation or even your own laptop. This is one of the reasons that the Kaggle community loves it. In R, the switch of multi-threading computation is just a parameter nthread: [code language=”r”]&gt;require(xgboost) &gt; x = matrix(rnorm(100*10000), 10000, 100) &gt; y = x %*% rnorm(100) + rnorm(1000) &gt; &gt;system.time({ + bst = xgboost(data = x, label = y, nthread = 1, nround = 100, verbose = FALSE) + }) user system elapsed 10.98 0.05 11.06 &gt; &gt;system.time({ + bst = xgboost(data = x, label = y, nthread = 4, nround = 100, verbose = FALSE) + }) user system elapsed 20.80 0.67 3.37 [/code] In the results from the toy example, there is a noticeable difference between the one-thread and four-thread trainings. As a comparison, we made the following figure from a competition data(<a target="_blank" rel="noopener" href="https://www.kaggle.com/c/higgs-boson/data">https://www.kaggle.com/c/higgs-boson/data</a>) on Kaggle. The experiments were run on a laptop with an i7-4700m CPU. <img src="http://www.parallelr.com/wp-content/uploads/2016/11/SpeedFigure-1024x843.png" alt="speedfigure"> The marks R and python are the vanilla gradient boosting machine implementation. XGBoost is the fastest when using only one thread. By employing 4 threads the process can be almost 4x faster. To reproduce the above results, one can find related scripts at:<a target="_blank" rel="noopener" href="https://github.com/dmlc/xgboost/tree/master/demo/kaggle-higgs">https://github.com/dmlc/xgboost/tree/master/demo/kaggle-higgs</a>. Note that the plot was made in 2015, thus the results may vary due to changes in the packages.</p>
<h2 id="2-Parallel-on-a-Cluster"><a href="#2-Parallel-on-a-Cluster" class="headerlink" title="2. Parallel on a Cluster"></a><strong>2. Parallel on a Cluster</strong></h2><p>For some cases where the size of data is too large to fit into the memory, people may set up a cluster to parallel the training process. However, a uniformed API of multi-nodes parallel computation for different interface languages is still left to be developed. The current standard way to parallel the training is to use the C++ backend with a configuration file which manages the model parameters and then submit it to Yarn. For further information, please read the official documentation: <a target="_blank" rel="noopener" href="http://xgboost.readthedocs.io/en/latest/tutorials/aws_yarn.html">http://xgboost.readthedocs.io/en/latest/tutorials/aws_yarn.html</a>. It is also possible to distribute the computation in one’s own cluster, but there’s no documentation provided yet. One thing worth noticing is that when performing multi-node parallel computation, the data is split by the rows, thus on each node it is (almost) impossible to search for the exact best splitting point. As a result, XGBoost switches to an approximate algorithm mentioned in <a target="_blank" rel="noopener" href="http://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf">this paper</a>. Briefly speaking, the approximate algorithm creates a histogram to represent each feature based on its numerial distribution. It reduces the amount of calculation on slaves, makes the reduce step easier, and maintains the precision at the same time.</p>
<h2 id="3-External-Memory"><a href="#3-External-Memory" class="headerlink" title="3. External Memory"></a><strong>3. External Memory</strong></h2><p>External memory is a compromise of large size of input and insufficient computational resources. The basic idea is simple: store the input data on an SSD, which is cheaper than memory and faster than HDD, and repeatedly load a chunk of data into memory to train the model partially. Comparing to the parallel training on a cluster, this strategy also uses the approximate algorithm, but is more convenient to configure and call, and is also cheaper for most users. To enable the external memory for R, we need to make sure that the compiler on your machine supports it. Usually it is fine with the latest gcc/clang. For windows users with mingw, however, is not able to try it out. The data files also need to be in the libsvm format on the disk. Files used in this demo can be downloaded at <a target="_blank" rel="noopener" href="https://github.com/dmlc/xgboost/tree/master/demo/data">https://github.com/dmlc/xgboost/tree/master/demo/data</a>. Here’s the usual way to load the data into memory with xgboost’s own data structure: [code language=”r”]&gt;dtrain = xgb.DMatrix(‘agaricus.txt.train’) [15:57:38] 6513x127 matrix with 143286 entries loaded from agaricus.txt.train &gt;dtest = xgb.DMatrix(‘agaricus.txt.test’) [15:57:38] 1611x127 matrix with 35442 entries loaded from agaricus.txt.test &gt; &gt;model = xgboost(data = dtrain, nround = 2, objective = “binary:logistic”) [1] train-error:0.000614 [2] train-error:0.001228 [/code] Now if we add the suffix: [code language=”r”]&gt;dtrain = xgb.DMatrix(‘agaricus.txt.train#train.cache’) [15:57:45] SparsePage::Writer Finished writing to train.r0-1.cache.row.page [15:57:45] SparsePageSource: Finished writing to train.r0-1.cache [15:57:45] 6513x127 matrix with 143286 entries loaded from agaricus.txt.train#train.cache &gt;dtest = xgb.DMatrix(‘agaricus.txt.test#test.cache’) [15:57:45] SparsePage::Writer Finished writing to test.r0-1.cache.row.page [15:57:45] SparsePageSource: Finished writing to test.r0-1.cache [15:57:45] 1611x127 matrix with 35442 entries loaded from agaricus.txt.test#test.cache &gt; &gt;model = xgboost(data = dtrain, nround = 2, objective = “binary:logistic”) [15:57:45] SparsePage::Writer Finished writing to train.r0-1.cache.col.page [1] train-error:0.000614 [2] train-error:0.001228 [/code] Note the only difference is just the suffix: A “#” and the string following. The suffix can be arbitrary string as the prefix of the generated cache files, as printed in the output. With the suffix, the function automatically marks the file for external memory training. In the external memory mode we can also perform multi-threading training for each chunk of data, because the chunks are taken into the training process in a linear relationship. More details are included in <a target="_blank" rel="noopener" href="http://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf">this paper</a>.</p>
<hr>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>XGBoost puts effort in the three popular parallel computation solutions, multithreading, distributed parallel and <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Out-of-core_algorithm">out-of-cores</a> computations. The idea of this project is to only expose necessary APIs for different language interface design, and hide most computational details in the backend. So far the library is fast and user-friendly, we wish it could inspire more R package developers to balance the design and efficiency. The development will be continued, and contributions on code and ideas are always welcome :)</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://jitmatrix.github.io/oneXPU/2017/01/24/parallel-computation-with-r-and-xgboost/" data-id="ckivbjgo2000a6lol4r98cxy1" data-title="Parallel Computation with R and XGBoost" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/boost/" rel="tag">boost</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/lightboost/" rel="tag">lightboost</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/memory-usage/" rel="tag">memory usage</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/multicores/" rel="tag">multicores</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/rstats/" rel="tag">rstats</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/sklearn/" rel="tag">sklearn</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/xgboost/" rel="tag">xgboost</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-r-with-parallel-computing" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2016/09/10/r-with-parallel-computing/" class="article-date">
  <time class="dt-published" datetime="2016-09-10T09:10:31.000Z" itemprop="datePublished">2016-09-10</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Accelerators/">Accelerators</a>►<a class="article-category-link" href="/categories/Accelerators/General/">General</a>►<a class="article-category-link" href="/categories/Accelerators/General/GPGPU/">GPGPU</a>►<a class="article-category-link" href="/categories/Accelerators/General/GPGPU/MultiCores/">MultiCores</a>►<a class="article-category-link" href="/categories/Accelerators/General/GPGPU/MultiCores/Performance-Optimizaiton/">Performance Optimizaiton</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2016/09/10/r-with-parallel-computing/">R with Parallel Computing from User Perspectives</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <hr>
<p><em>This article is originally published in <a target="_blank" rel="noopener" href="http://cos.name/">Capital of Statistic</a> by Chinese [<a target="_blank" rel="noopener" href="http://cos.name/2016/09/r-and-parallel-computing/">link</a>] and I would like to thank <a target="_blank" rel="noopener" href="http://www.sfu.ca/~hetongh/">He Tong</a> for lots of great suggestions.</em> <em>All code in this post can be found on GitHub [<a target="_blank" rel="noopener" href="https://github.com/PatricZhao/ParallelR/tree/master/PP_for_COS">link</a>].</em></p>
<hr>
<p>Data scientists are already very familiar with statistical software like <a target="_blank" rel="noopener" href="https://www.r-project.org/">R</a>, <a target="_blank" rel="noopener" href="http://www.sas.com/en_hk/home.html">SAS</a>, <a target="_blank" rel="noopener" href="http://www.ibm.com/analytics/us/en/technology/spss/">SPSS</a>, <a target="_blank" rel="noopener" href="http://www.mathworks.com/products/matlab">MATLAB</a>; however, some of them are relatively inexperienced in parallel computing. So, in this post, I will introduce you some basic concepts on the use of parallel computing in R.</p>
<h1 id="What-is-Parallel-Computing？"><a href="#What-is-Parallel-Computing？" class="headerlink" title="What is Parallel Computing？"></a>What is Parallel Computing？</h1><p><a target="_blank" rel="noopener" href="https://computing.llnl.gov/tutorials/parallel_comp/">Parallel computing</a>, specifically, should include <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Supercomputer">high-performance computers</a> and <a target="_blank" rel="noopener" href="http://whatis.techtarget.com/definition/parallel-processing-software">parallel software</a>. The peak performance of high-performance computers increases quickly. In the most recent ranking of the world’s TOP500 supercomputers, Chinese Sunway Taihu Light topped the list with 93 PFLOPS (<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Supercomputer">here</a>). For most individuals, small and medium enterprises, high-performance computers are too expensive. So, the application of high-performance computers is indeed limited, mainly in the field of national defense, military, aerospace and research areas. In recent years, with the rapid developments of multicore CPU, cheap cluster, and various accelerators (<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Nvidia_Tesla">NVIDIA GPU</a>, <a target="_blank" rel="noopener" href="http://www.intel.com/content/www/us/en/processors/xeon/xeon-phi-detail.html">Intel Xeon Phi</a>, <a target="_blank" rel="noopener" href="http://www.xilinx.com/training/fpga/fpga-field-programmable-gate-array.htm">FPGA</a>), personal computers has been comparable to high-performance computers. <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/09/sunway-taihulight.jpg"><img src="http://www.parallelr.com/wp-content/uploads/2016/09/sunway-taihulight.jpg" alt="sunway-taihulight"></a> On the other hand, the software changes lag a lot. Imagine what software you’re using  supported parallel operations, Chrome, Visual Studio or R? <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/09/cs.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/09/cs.png" alt="common software"></a> Software parallelization requires more research and development supports. It is called <a target="_blank" rel="noopener" href="https://software.intel.com/en-us/articles/what-is-code-modernization">code modernization</a> for the procedure of changing the serial code to parallel, which sounds a very interesting work. But, in practice, a large number of bug fixes, data structure rewrite, uncertain software behaviors and cross-platform issues greatly increase the software development and maintenance costs.</p>
<h1 id="Why-R-Needs-Parallel-Computing"><a href="#Why-R-Needs-Parallel-Computing" class="headerlink" title="Why R Needs Parallel Computing?"></a>Why R Needs Parallel Computing?</h1><p>Let’s come back to R. As one of the most popular statistical software, R has a lot of advantages, such as a wealth of statistical models, data processing tools, and powerful visualization capabilities. However, with an increasing amount of data, R’s memory usage and computation mode limit R to scale. From the memory perspective, R uses in-memory calculation mode. All data need to be processed in the main memory (RAM). Obviously, its advantages are high computational efficiency and speed, but the drawback is that the size of the problem can be handled by R is very limited (&lt;RAM ). Secondly, R core is a single-threaded program. Thus, in the modern multi-core processors,  R can not effectively use all the computing cores. If the R went to the Sunway CPU of 260 computing cores, single-threaded R only take 1/260 computing power and waste other computing cores of 259/260.</p>
<h2 id="Solution？Parallel-Computing"><a href="#Solution？Parallel-Computing" class="headerlink" title="Solution？Parallel Computing!"></a><strong>Solution？Parallel Computing!</strong></h2><p>Parallel computing technology can solve the problem that single-core and memory capacity can not meet the application needs. Thus, the parallel computing technology will be extremely expansion of the use of R.  From R 2.14 (Feb 2012), ‘<a target="_blank" rel="noopener" href="https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf">parallel</a>‘ package is installed by default. Obviously, R core development team also attached great importance to parallelization. <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/09/RPP.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/09/RPP.png"></a></p>
<h1 id="How-to-Use-Parallel-Computing"><a href="#How-to-Use-Parallel-Computing" class="headerlink" title="How to Use Parallel Computing?"></a>How to Use Parallel Computing?</h1><p>From the user’s view, parallel computing in R can be divided into implicit and explicit computing mode.</p>
<h2 id="Implicit-Mode"><a href="#Implicit-Mode" class="headerlink" title="Implicit Mode"></a>Implicit Mode</h2><p>Implicit computing hides most of the details for the user. It is not necessary to know how to allocate hardware resources, distribute workloads and gather results. The computations will start automatically based on the current hardware resources. Obviously, this mode is the most favorable. We can achieve higher performance without changing the calculation mode and our codes. Common implicit parallel mode includes:</p>
<ul>
<li>  Using Parallel Libraries</li>
</ul>
<p>Parallel libraries, such as <a target="_blank" rel="noopener" href="https://software.intel.com/en-us/intel-mkl">Intel MKL</a>，<a target="_blank" rel="noopener" href="https://developer.nvidia.com/cublas">NVIDIA cuBLAS</a>,  <a target="_blank" rel="noopener" href="http://www.openblas.net/">OpenBLAS</a> are usually provided by the hardware manufacturer with in-depth optimizations based on the corresponding hardwares, so its performance is hugely better than R libraries. It is recommended choosing a high-performance R library at compile time or loading by LD_PRELOAD at runtime. The details of compiling, loading and using BLAS libraries can be found in the one of our previous blog (in <a target="_blank" rel="noopener" href="http://www.parallelr.com/r-hpac-benchmark-analysis/">here</a>). In the first diagram, the matrix calculation experiments, parallel libraries on 1 or 2 CPUs is a hundred times faster than R original library. On the second, we can see the GPU math library shows remarkable speed for some common analysis algorithms as well. <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/09/mm.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/09/mm.png" alt="GEMM"></a> <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/09/ca.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/09/ca.png" alt="GPU for R"></a> Now, let’s run an interesting example in which we didn’t call GEMM function explicitly but still get lots of performance improvements from parallel BLAS library. In below example, we train <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a> dataset by DBN,deep belief network, (SRBM,Stacked Restricted Boltzmann Machine ) with <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/deepnet/index.html">deepnet</a> package. This example refers the blog of “<a target="_blank" rel="noopener" href="http://basicstatistics.tistory.com/entry/training-MNIST-data-with-the-package-deepnet">training MNIST data with the package deepnet</a>“ where the author got the accuracy of 0.004% on training data and 2% on testing data. Because the original network of <code>c(500,500,250,125)</code> is too huge to run, I simplified the network architecture in our case and the code of <code>deepnet_mnist.R</code> in <a target="_blank" rel="noopener" href="https://github.com/PatricZhao/ParallelR/blob/master/PP_for_COS/deepnet_mnist.R">here</a>. [code language=”r”] #install.packages(“data.table”) #install.packages(“deepnet”) library(data.table) library(deepnet) # download MNIST dataset in below links # <a target="_blank" rel="noopener" href="https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/train.csv.gz">https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/train.csv.gz</a> # <a target="_blank" rel="noopener" href="https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/test.csv.gz">https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/test.csv.gz</a> mnist.train &lt;- as.matrix(fread(“./train.csv”, header=F)) mnist.test &lt;- as.matrix(fread(“./test.csv”, header=F)) # V785 is the label x &lt;- mnist.train[, 1:784]/255 y &lt;- model.matrix(~as.factor(mnist.train[, 785])-1) system.time( nn &lt;- dbn.dnn.train(x,y, hidden=c(64), #hidden=c(500,500,250,125), output=”softmax”, batchsize=128, numepochs=100, learningrate = 0.1) ) [/code] Thus, we run this piece of code twice. We got **3.7X **and  **2.5X**speedup (**runtime of <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/09/deepnet_Rnative-1.png">2581</a> sec .vs. <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/09/deepnet_MKL.png">693</a> sec and <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/09/deepnet_OpenBLAS.png">1213</a> sec** )  by Intel MKL and OpenBLAS library on Intel SandyBridge E-2670. [code language=”r”] &gt; R CMD BATCH deepnet_mnist.R &gt; cat deepnet_mnist.Rout deep nn has been trained. user system elapsed 2574.013 1.404 2581.882 &gt; env LD_PRELOAD=/…/tools/OpenBLAS/lib/libopenblas.so R CMD BATCH deepnet_mnist.R &gt; cat deepnet_mnist.Rout deep nn has been trained. user system elapsed 4752.005 25881.221 1213.644 # Compiled with Intel Compiler and MKL &gt; R CMD BATCH deepnet_mnist.R &gt; cat deepnet_mnist.Rout deep nn has been trained. user system elapsed 10770.641 290.486 693.146 [/code]</p>
<ul>
<li>  Using MultiThreading Functions</li>
</ul>
<p><a target="_blank" rel="noopener" href="http://openmp.org/wp/">OpenMP</a> is a multithreading library based on shared memory architecture for application acceleration. The latest R has been opened OpenMP options (-fopenmp) at compile time on Linux, which means that some of the calculations can be run in multithreaded mode. For example , <code>dist</code> is implemented by multithreading with OpenMP. The example code as below (<a target="_blank" rel="noopener" href="https://github.com/PatricZhao/ParallelR/blob/master/PP_for_COS/ImplicitParallel_MT.R">ImplicitParallel_MT.R</a>): [code language=”r”] # Comparison of single thread and multiple threads run # using Internal function to set thread numbers, not very grace, but don’t find a good way till now. # Ang suggestion? setNumThreads &lt;- function(nums=1) { .Internal(setMaxNumMathThreads(nums)) .Internal(setNumMathThreads(nums)) } # dataset from 2^6 to 2^11 for(i in 6:11) { ORDER &lt;- 2^i m &lt;- matrix(rnorm(ORDER*ORDER),ORDER,ORDER) setNumThreads(1) res &lt;- system.time(d &lt;- dist(m)) print(res) setNumThreads(20) res &lt;- system.time(d &lt;- dist(m)) print(res) } [/code] <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/09/dist.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/09/dist.png"></a></p>
<ul>
<li>  Using Parallel Packages</li>
</ul>
<p>In the list of <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/views/HighPerformanceComputing.html">R high-performance computing</a>, there are lots of parallel packages and tools. These parallel packages can be used like any other R packages quickly and conveniently. R users can always focus on the problem itself, without having to think too much about parallelism implementations and performance issues. Take <a target="_blank" rel="noopener" href="http://www.h2o.ai/">H2O.ai</a> for example, it takes Java as the backend to achieve multi-threading and multi-nodes computing. Users only need to load the package, and then initialize H2O with thread number. After that subsequent calculations, such as GBM, GLM, DeepLearning algorithm, will automatically be assigned to multiple threads and multiple CPUs.   [code language=”r”] library(h2o) h2o.init(nthreads = 4) # Connection successful! # R is connected to the H2O cluster: # H2O cluster uptime: 1 hours 53 minutes # H2O cluster version: 3.8.3.3 # H2O cluster name: H2O_started_from_R_patricz_ywj416 # H2O cluster total nodes: 1 # H2O cluster total memory: 1.55 GB # H2O cluster total cores: 4 # H2O cluster allowed cores: 4 # H2O cluster healthy: TRUE # H2O Connection ip: localhost # H2O Connection port: 54321 # H2O Connection proxy: NA # R Version: R version 3.3.0 (2016-05-03) [/code]  </p>
<h2 id="Explicit-Mode"><a href="#Explicit-Mode" class="headerlink" title="Explicit Mode"></a>Explicit Mode</h2><p>Explicit parallel computing requires the user to be able to deal with more details, including data partitions, task distributions, and final results collections. Users not only need to understand their own algorithms but also need to have a certain understanding of hardware and software stack. Thus, it’s a little difficult for users. Fortunately, parallel computing framework in R, such as <code>parallel</code>,<code>Rmpi</code> and <code>foreach</code>, provides the simple parallel programming approach by mapping structure. R users only need to transfer the code into the form of <code>*apply</code> or <code>for</code>, and then replace them by parallel APIs such as <code>mc*apply</code> or <code>foreach</code>. For more complex calculation flow, the user can repeat the process of map-and-reduce. <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/09/mapping-1.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/09/mapping-1.png" alt="R Parallel Approaches"></a> Now, we show you a parallel example by solving quadratic equation with <code>*apply</code> and <code>for</code> style. The whole code in <a target="_blank" rel="noopener" href="https://github.com/PatricZhao/ParallelR/blob/master/PP_for_COS/ExplicitParallel.R">ExplicitParallel.R</a>. First, we present a non- vectorized function for solving the equation, which can handle several special cases, such as second quadratic coefficient is zero, or second and first quadratic term are zero, or the number of the square root is negative. [code language=”r”] # Not vectorized function # Quadratic Equation: a*x^2 + b*x + c = 0 solve.quad.eq &lt;- function(a, b, c) { # Not validate eqution: a and b are almost ZERO if(abs(a) &lt; 1e-8 &amp;&amp; abs(b) &lt; 1e-8) return(c(NA, NA) ) # Not quad equation if(abs(a) &lt; 1e-8 &amp;&amp; abs(b) &gt; 1e-8) return(c(-c/b, NA)) # No Solution if(b*b - 4*a*c &lt; 0) return(c(NA,NA)) # Return solutions x.delta &lt;- sqrt(b*b - 4*a*c) x1 &lt;- (-b + x.delta)/(2*a) x2 &lt;- (-b - x.delta)/(2*a) return(c(x1, x2)) } [/code] And then, we randomly generated three big vectors to storage three coefficients. [code language=”r”] # Generate data len &lt;- 1e8 a &lt;- runif(len, -10, 10) a[sample(len, 100,replace=TRUE)] &lt;- 0 b &lt;- runif(len, -10, 10) c &lt;- runif(len, -10, 10) [/code]</p>
<h3 id="apply-IMPLEMENTATION"><a href="#apply-IMPLEMENTATION" class="headerlink" title="*apply IMPLEMENTATION:"></a>*<strong>apply IMPLEMENTATION:</strong></h3><p>First, we look at the serial code. The data is mapped into solver function,<code>solve.quad.eq </code>by <code>lapply</code>, and the results are saved into list finally. [code language=”r”] # serial code system.time( res1.s &lt;- lapply(1:len, FUN = function(x) { solve.quad.eq(a[x], b[x], c[x])}) ) [/code] Next, we use the function of <code>mcLapply</code> (multicores) in <code>parallel</code> package to parallelize calculations in <code>lapply</code>. From the API interface, the usage of <code>mcLapply</code> is really similar with <code>lapply</code> in addition to specifying the core numbers. <code>mcLapply</code> creates multiple copies of the current R session based on Linux fork mechanism, and evenly assign compute tasks into multiple processes regarding with input index. Finally, the master R session will collect the results from all worker sessions. If we specify two worker processes, one process calculated <code>1:(len/2)</code> while another computing <code>(len/2+1):len</code>, and finally two parts of results will be merged into <code>res1.p</code>. However, due to the use of Linux mechanisms, this version can’t be executed on Windows platform. [code language=”r”] # parallel, Linux and MAC platform library(parallel) # multicores on Linux system.time( res1.p &lt;- mclapply(1:len, FUN = function(x) { solve.quad.eq(a[x], b[x], c[x]) }, mc.cores = 4) ) [/code] For non-Linux users, we can use <code>parLapply</code> function in <code>parallel</code> package to achieve parallelism. <code>parLapply</code> function supports different platforms including Windows, Linux and Mac with better portability, but its usage is a little complicated than <code>mclapply</code>. Before using <code>parLapply</code> function, we need to create a computing group (cluster) first. Computing group is a software-level concept, which means how many R worker processes we need to create (Note: <code>par*apply</code> package will create several new R processes rather than copies of R master process from <code>mc*apply</code>). Theoretically, the size of the computing group is not affected by the hardware configuration.For example, we can create a group with 1000 R worker processes on any machine. In practice, we usually use the same size of computing group with hardware resources (such as physical cores) so that each worker process of R can be mapped to a physical core. In the following example, we start with <code>detectCores</code> function to determine the number of computing cores in the machine.It is noteworthy that <code>detectCores()</code> returns the number of <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Hyper-threading">Hyper-Threading</a> rather than real physical cores.For example, there are two physical cores on my laptop, and each core can simulate two hyperthreading , so <code>detectCores()</code> return value is 4. However, for many compute-intensive tasks, the Hyper-Threading is not much helpful for improving performance, so we use the parameter of <code>logical=FALSE</code> to get the actual number of physical cores and then create the same number group.Since the worker processes in the group is new R sessions, the data and functions of the parent process is not visible. Therefore, we have to broadcast the data and functions to all worker processes by <code>clusterExport</code> function. Finally <code>parLapply</code> will distribute the tasks to all R worker processes evenly, and then gather results back. [code language=”r”] # cluster on Windows cores &lt;- detectCores(logical = FALSE) cl &lt;- makeCluster(cores) clusterExport(cl, c(‘solve.quad.eq’, ‘a’, ‘b’, ‘c’)) system.time( res1.p &lt;- parLapply(cl, 1:len, function(x) { solve.quad.eq(a[x], b[x], c[x]) }) ) stopCluster(cl) [/code]</p>
<h3 id="for-IMPLEMENTATION"><a href="#for-IMPLEMENTATION" class="headerlink" title="for IMPLEMENTATION:"></a><strong>for IMPLEMENTATION:</strong></h3><p>The computation approach of <code>for</code> is very similar with <code>*apply</code>. In the following serial implementation, we created a matrix for storage results and update the results one by one in the inner loop. [code language=”r”] # for style: serial code res2.s &lt;- matrix(0, nrow=len, ncol = 2) system.time( for(i in 1:len) { res2.s[i,] &lt;- solve.quad.eq(a[i], b[i], c[i]) } ) [/code] For the for loop parallelization, we can use <code>%dopar%</code> in <code>foreach</code> package to distribute the computations to multiple R workers. <code>foreach</code> package provides a method of data mapping, but does not include the establishment of computing group.Therefore, we need to create a computing group by <code>doParallel</code> or <code>doMC</code> package. Creating computing group is as same as before, except setting backend of computations by <code>registerDoParallel</code>. Now we consider the data decomposition. Actually, we want each R worker process to deal with continuous computing tasks. Suppose we have two R worker processes, the process 1 computes from <code>1:len/2</code>, another process for <code>(len/2+1):len</code>. Therefore, in the following example code, we evenly distribute the vectors to computing group and each process calculates the size of <code>chunk.size</code>. Another important skill is using local matrix to save partial results in each process. Last, combine local results together by <code>.combine=&#39;rbind&#39;</code> parameter. [code language=”r”] # foreach, work on Linux/Windows/Mac library(foreach) library(doParallel) # Real physical cores in my computer cores &lt;- detectCores(logical = FALSE) cl &lt;- makeCluster(cores) registerDoParallel(cl, cores=cores) # clusterSplit are very convience to split data but it takes lots of extra memory # chunks &lt;- clusterSplit(cl, 1:len) # split data by ourselves chunk.size &lt;- len/cores system.time( res2.p &lt;- foreach(i=1:cores, .combine=’rbind’) %dopar% { # local data for results res &lt;- matrix(0, nrow=chunk.size, ncol=2) for(x in ((i-1)*chunk.size+1):(i*chunk.size)) { res[x - (i-1)*chunk.size,] &lt;- solve.quad.eq(a[x], b[x], c[x]) } # return local results res } ) stopImplicitCluster() stopCluster(cl) [/code] Finally, we tested the code on Linux platform with 4 threads and can gain more than **3X speedup ** for every parallel implementation! <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/09/EP.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/09/EP.png" alt="R explicit parallel mode"></a></p>
<h1 id="Challenges-and-Prospects"><a href="#Challenges-and-Prospects" class="headerlink" title="Challenges and Prospects"></a>Challenges and Prospects</h1><p><strong>Challenges</strong> :In practice, the problem needed to be resolved by parallel computing is not such simple as our examples. To parallelize R and its eco-system are still very difficult because,</p>
<ul>
<li>  R is a decentralized and non-commercial software</li>
</ul>
<p>R is not developed by a compact organization or company while most of R’s packages are contributed by users. It means that it is difficult to adjust and unify software architecture and design with the same philosophy. On the other hand, commercial software, such as Matlab, with unified development, maintenance, and management, can be relatively easier to restructure and reconstruct. Therefore, after several times update, the parallelism of commercial software will be much higher.</p>
<ul>
<li>  The infrastructure design of R is still single-threaded</li>
</ul>
<p>R was originally designed for single-threaded so that many of the underlying data structures and functions are not thread-safe. Therefore, lots of codes need to be rewritten or adjust for high-level parallel algorithms. But it likely will destroy the original design patterns.</p>
<ul>
<li>  The packages are highly dependent</li>
</ul>
<p>Assume that we use package B in R, and B depends on some functions of package A. If package B is improved by multithreading first; after that package A is also enhanced by parallelization. So, it is likely to appear hybrid parallel when we use package B. It may lead lots of strange errors (BUGs) and performance decrease if there is no comprehensive design and testing during developments. <strong>Prospects:</strong> How will the future of parallelism in R ?</p>
<ul>
<li>  High-performance components from commercial and research organizations</li>
</ul>
<p>Essentially, software developments are inseparable from the human and financial investments. The packages, such as <a target="_blank" rel="noopener" href="http://www.h2o.ai/">H2O</a>, <a target="_blank" rel="noopener" href="https://github.com/dmlc/mxnet">MXNet</a>, <a target="_blank" rel="noopener" href="https://software.intel.com/en-us/blogs/daal">Intel DAAL</a>, improve the performance significantly from parallelism with long-term supports.</p>
<ul>
<li>  Cloud Platform</li>
</ul>
<p>With the rise of cloud computing ,Data Analyst as a Services (DAAS) and Machine Learning as a Service (MLAS) are more and more popular.The major cloud providers optimize their tools, including R, from hardware deployments, database, high-level algorithms and explore much more parallelism in application level. For example, Microsoft recently launched a series supports for R in their cloud (<a target="_blank" rel="noopener" href="http://www.zdnet.com/article/microsofts-r-strategy/">here</a>). Therefore, parallel in R will be more transparent. The user does the same things in R, but the real computing will be distributed to the cloud.</p>
<hr>
<h2 id="Other-Articles-and-Slides-about-R-and-Parallel-Computing"><a href="#Other-Articles-and-Slides-about-R-and-Parallel-Computing" class="headerlink" title="Other Articles and Slides about R and Parallel Computing"></a><strong>Other Articles and Slides about R and Parallel Computing</strong></h2><ul>
<li>  Max Gordon, How-to go parallel in R – basics + tips, <a target="_blank" rel="noopener" href="http://gforge.se/2015/02/how-to-go-parallel-in-r-basics-tips/">here</a></li>
<li>  Marcus,A brief foray into parallel processing with R, <a target="_blank" rel="noopener" href="https://beckmw.wordpress.com/2014/01/21/a-brief-foray-into-parallel-processing-with-r/">here</a></li>
<li>  John Mount, A gentle introduction to parallel computing in R, <a target="_blank" rel="noopener" href="http://www.win-vector.com/blog/2016/01/parallel-computing-in-r/">here</a></li>
<li>  Guilherme Ludwig, Parallel computing with R, <a target="_blank" rel="noopener" href="http://www.stat.wisc.edu/~gvludwig/327-5/parallel.html#/">here</a></li>
<li>  Norman Matloff, GPU TUTORIAL, WITH R INTERFACING, <a target="_blank" rel="noopener" href="https://matloff.wordpress.com/2015/01/23/gpu-tutorial-with-r-interfacing/">here</a></li>
<li>  Grey, Running R in Parallel (the easy way), <a target="_blank" rel="noopener" href="http://blog.yhat.com/posts/running-r-in-parallel.html">here</a></li>
<li>  NIMBioS,Tutorial: Using R for HPC, <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLRyq_4VPZ9g_g4b3An6VTkRX_c0tAHoAj">video</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://jitmatrix.github.io/oneXPU/2016/09/10/r-with-parallel-computing/" data-id="ckivbjgr2006c6lol3y4t3xrt" data-title="R with Parallel Computing from User Perspectives" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/BLAS/" rel="tag">BLAS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MKL/" rel="tag">MKL</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/R/" rel="tag">R</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cuBLAS/" rel="tag">cuBLAS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/doMC/" rel="tag">doMC</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/doParallel/" rel="tag">doParallel</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/foreach/" rel="tag">foreach</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/mcapply/" rel="tag">mcapply</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/multicores/" rel="tag">multicores</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/parallel/" rel="tag">parallel</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/parallel-computing/" rel="tag">parallel computing</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/parlapply/" rel="tag">parlapply</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/rstats/" rel="tag">rstats</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-r-cran-package-modernization-openmp" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2016/08/15/r-cran-package-modernization-openmp/" class="article-date">
  <time class="dt-published" datetime="2016-08-15T02:55:52.000Z" itemprop="datePublished">2016-08-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Accelerators/">Accelerators</a>►<a class="article-category-link" href="/categories/Accelerators/MultiCores/">MultiCores</a>►<a class="article-category-link" href="/categories/Accelerators/MultiCores/Performance-Optimizaiton/">Performance Optimizaiton</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2016/08/15/r-cran-package-modernization-openmp/">R and OpenMP:  CRAN Package Modernization</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <hr>
<p><em>Indeed, a major current trend in R is what might be called “R+X”, where X is some other language or library.</em> <em>–Norman Matloff, Parallel Computing for Data Science</em></p>
<hr>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p><strong>Nowadays,</strong> there are more than 8000 packages in CRAN (till April 2016 from Revolution Analytics  <a target="_blank" rel="noopener" href="http://blog.revolutionanalytics.com/2016/04/a-segmented-model-of-cran-package-growth.html">Blog</a>) and even more, packages locate in GitHub and R-forge. Meanwhile, most of us heavily rely on these packages for our daily analytical work. With the increasing amount of data as well as the diversification of data sources (such as image, speech, and video), data scientists will be experience performance issues.  Though there already have parallel solutions for us (check out <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/views/HighPerformanceComputing.html">HPCR list</a>), we will fall into the case soon or later that no parallelized package is available in CRAN or GitHub. Therefore, we have to consider to implement specified parallel algorithm by ourselves. In general, you can design and implement the parallel algorithm from scratch or parallelize the existing packages.  If you are working on writing the parallel code from scratch, I recommend Prof. Matloff’s new book_, <a target="_blank" rel="noopener" href="https://www.amazon.com/Parallel-Computing-Data-Science-Examples/dp/1466587016">Parallel Computing for Data Science</a>_, and Dr. OTT’s blog <em><a target="_blank" rel="noopener" href="http://www.parallelr.com/r-and-openmp-boosting-compiled-code-on-multi-core-cpu-s/">R and openMP: boosting compiled code on multi-core cpu-s</a>.</em> In this post, I will introduce the basic workflow and skills for accelerating legacy R package by OpenMP, and this produce is called <a target="_blank" rel="noopener" href="https://software.intel.com/en-us/articles/what-is-code-modernization">code modernization</a>. Sometimes, acceleration legacy code is more challenge than writing parallel algorithm by yourself because we have to work on original software architecture and implementation, and try to minimal changes as much as possible. In below cycle chart, I illustrate several basic steps of code modernization and I am going to introduce each of steps in the below chapters with <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/class/class.pdf">KNN {class}</a> function, written by Prof. Brian Ripley, for example.   <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/05/cycle-2.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/05/cycle-2.png"></a>  </p>
<h1 id="Understand-Algorithm-and-Code"><a href="#Understand-Algorithm-and-Code" class="headerlink" title="Understand Algorithm and Code"></a>Understand Algorithm and Code</h1><p>First and most important step before speedup code is to understand the original algorithm and implementation as much as possible ranging from data structure, functionality to coding. Now, let us start with our example of <em>class</em> package and the major function of K nearest neighbors algorithm (<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">KNN</a>). The algorithm of KNN can be shown as below chart. There are three steps. First, compute the distance from each observation of train dataset to each of test dataset, and then select K nearest from distance results (the direct approach is to sort the results and select first K elements). Finally, most voted group will win, or if several groups have the same accounts, just <strong>RANDOM</strong> breaking (Notes: this random breaking will cause little trouble in parallel version, consider why, and I will give more details later). <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/05/KNN-3.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/05/KNN-3.png"></a>   Next, we’re going through the source code and match each step of KNN with their implementations. Checking the body of R Knn, you can see the real algorithm is locate in C level and R function is only a wrapper. [code language=”r”] &gt;library(class) &gt;knn function (train, test, cl, k = 1, l = 0, prob = FALSE, use.all = TRUE) { # skip Z &lt;- .C(VR_knn, as.integer(k), as.integer(l), as.integer(ntr),….)) # skip } [/code] So, switching to VR_knn function in your local repository (class.c) or from GitHub (<a target="_blank" rel="noopener" href="https://github.com/cran/class/blob/master/src/class.c">here</a>), totally there are about 100 lines. And the code is organized very clearly and we can quickly identify the 3 steps with my highlight comments. [code language=”C”] VR_knn(Sint *kin, Sint *lin, Sint *pntr, Sint *pnte, Sint *p, double *train, Sint *class, double *test, Sint *res, double *pr, Sint *votes, Sint *nc, Sint *cv, Sint *use_all) { // skip … // Peng: main loop for (npat = 0; npat &lt; nte; npat++) { // Peng: Step.1 – compute distance for (k = 0; k &lt; *p; k++) { tmp = test[npat + k * nte] - train[j + k * ntr]; dist += tmp * tmp; } // skip … // Peng: Step.2 – select K nearest /* Use ‘fuzz’ since distance computed could depend on order of coordinates */ if (dist &lt;= nndist[kinit - 1] * (1 + EPS)) for (k = 0; k &lt;= kn; k++) if (dist &lt; nndist[k]) { for (k1 = kn; k1 &gt; k; k1–) { nndist[k1] = nndist[k1 - 1]; pos[k1] = pos[k1 - 1]; } // Peng: Step.3 – voting and breaking for (j = 0; j &lt;= *nc; j++) votes[j] = 0; if (*use_all) { for (j = 0; j &lt;; kinit; j++) // skip … } &lt;span class=”pl-k”&gt;else&lt;/span&gt; { &lt;span class=”pl-c”&gt;/* break ties at random */ &lt;/span&gt; // skip … } // Peng: Final results res[npat] = index; pr[npat] = (double) mm / (kinit + extras); } // Peng: end of main loop RANDOUT; } [/code]</p>
<h1 id="Identify-Bottleneck"><a href="#Identify-Bottleneck" class="headerlink" title="Identify Bottleneck"></a>Identify Bottleneck</h1><p>Two kinds of applications for parallelization can be divided into computer and memory intensive. OpenMP is used to resolve compute-intensive applications with multiple threads while message passing interface (<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Message_Passing_Interface">MPI</a>) can be applied reduce local memory requirements by distributing the application in different machines. Therefore, we need to determine whether the package we are dealing with is compute-intensive and where the most time-consuming parts are. To do this, we can do a simple complexity analysis from algorithm view. Still using KNN for example as our description, we see the computational complexity in Step 1 is O(M*N*Q) since we iterate through train (M lines) and test (N lines) dataset and compute the distance with K features. Step 2 is sorting and the worst computational complexity is O(M*K) and Step 3 is only the linear complexity of O(K). Regarding with memory requirements, the most size is the two input dataset with O(M*Q) and O(N*Q) in Step 1. Totally, the computational complexity of KNN is <strong>THREE</strong> power while the memory usage is only <strong>TWO</strong> power. So, the OpenMP is appropriate.</p>
<h1 id="Specify-Parallel-Region"><a href="#Specify-Parallel-Region" class="headerlink" title="Specify Parallel Region"></a>Specify Parallel Region</h1>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://jitmatrix.github.io/oneXPU/2016/08/15/r-cran-package-modernization-openmp/" data-id="ckivbjgo9000g6lolb1js0zc4" data-title="R and OpenMP:  CRAN Package Modernization" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CRAN/" rel="tag">CRAN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/R/" rel="tag">R</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/knn/" rel="tag">knn</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/openMP/" rel="tag">openMP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/rstats/" rel="tag">rstats</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-r-and-openmp-boosting-compiled-code-on-multi-core-cpu-s" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2016/07/26/r-and-openmp-boosting-compiled-code-on-multi-core-cpu-s/" class="article-date">
  <time class="dt-published" datetime="2016-07-26T18:10:38.000Z" itemprop="datePublished">2016-07-26</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Accelerators/">Accelerators</a>►<a class="article-category-link" href="/categories/Accelerators/MultiCores/">MultiCores</a>►<a class="article-category-link" href="/categories/Accelerators/MultiCores/Vectorization/">Vectorization</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2016/07/26/r-and-openmp-boosting-compiled-code-on-multi-core-cpu-s/">R and openMP: boosting compiled code on multi-core cpu-s</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Sometimes you just need more speed. And sometime plain R does not provide it. This article is about boosting your R code with C++ and openMP. OpenMP is a parallel processing framework for shared memory systems. This is an excellent way to use all the cpu cores that are sitting, and often just idling, in any modern desktop and laptop. Below, I will take a simple, even trivial problem—ML estimation of normal distribution parameters—and solve it first in R, thereafter I write the likelihood function in standard single-threaded C++, and finally in parallel using C++ and openMP. Obviously, there are easier ways to find sample mean and variance but this is not the point. Read it, and try to write your own openMP program that does something useful!</p>
<h1 id="R-for-Simplicity"><a href="#R-for-Simplicity" class="headerlink" title="R for Simplicity"></a>R for Simplicity</h1><p>Assume we have a sample of random normals and let’s estimate the parameters (mean and standard deviation) by Maximum Likelihood (ML). We start with pure R. The log-likelihood function may look like this: [code language=”r”] llR &lt;- function(par, x) { mu &lt;- par[1] sigma &lt;- par[2] sum(-1/2*log(2*pi) - log(sigma) - 1/2*((x - mu)^2)/sigma^2) } [/code] Note that this code is fully vectorized (<code>(x-mu)</code> is written with no explicit loop) and hence very fast. Obviously, this is a trivial example, but it is easy to understand and parallelize. Now generate some data [code language=”r”] x &lt;- rnorm(1e6) [/code] and start values, a bit off to give the computer more work: [code language=”r”] start &lt;- c(1,1) [/code] Estimate it (using maxLik package): [code language=”r”] library(maxLik) system.time(m &lt;- maxLik(llR, start=start, x=x)) [/code]</p>
<p>   user system elapsed<br>  2.740  0.184   2.931</p>
<p>[code language=”r”] summary(m) [/code]</p>
<p>--------------------------------------------<br>Maximum Likelihood estimation<br>Newton-Raphson maximisation, 6 iterations<br>Return code 2: successive function values within tolerance limit<br>Log-Likelihood: -1419125<br>2 free parameters<br>Estimates:<br>Estimate Std. error t value Pr(&gt; t)<br>[1,] 0.0010318 0.0010001 1.032 0.302<br>[2,] 1.0001867 0.0007072 1414.236 &lt;2e-16 <strong>*<br>---<br>Signif. codes: 0 ‘*</strong>’ 0.001 ‘*<em>’ 0.01 ‘</em>’ 0.05 ‘.’ 0.1 ‘ ’ 1<br>--------------------------------------------</p>
<p>The code runs 2.6s on an i5-2450M laptop using a single cpu core. First, let’s squeeze more out of the R code. Despite being vectorized, the function can be improved by moving the repeated calculations out of the (vectorized) loop. We can re-write it as: [code language=”r”] llROpt &lt;- function(par, x) { mu &lt;- par[1] sigma &lt;- par[2] N &lt;- length(x) -N*(0.5*log(2*pi) + log(sigma)) - 0.5*sum((x - mu)^2)/sigma^2 } [/code] Now only <code>(x - mu)^2</code> is computed as vectors. Run it: [code language=”r”] library(maxLik) system.time(m &lt;- maxLik(llROpt, start=start, x=x)) [/code]</p>
<p>   user  system elapsed<br>  0.816   0.000   0.818 </p>
<p>You see—just a simple optimization gave a more than three–fold speed improvement! I don’t report the results any more, as those are virtually identical.</p>
<h1 id="C-for-Speed"><a href="#C-for-Speed" class="headerlink" title="C for Speed"></a>C for Speed</h1><p>Now let’s implement the same function in C++. R itself is written in C, however there is an excellent library, Rcpp, that makes integrating R and C++ code very easy. It is beyond the scope of this post to teach readers C and explain the differences between C and C++. But remember that Rcpp (and hence C++) offers a substantially easier interface for exchanging data between R and compiled code than the default R API. Let’s save the log-likelihood function in file loglik.cpp. It might look like this: [code language=”cpp”] #include <cmath> #include &lt;Rcpp.h&gt; using namespace Rcpp; RcppExport SEXP loglik(SEXP s_beta, SEXP s_x) { NumericVector x(s_x); NumericVector beta(s_beta); // make Rcpp vector out of R SEXP double mu = beta[0]; // first element is 0 in C++ double sigma = beta[1]; double ll = 0; for(int i = 0; i &lt; x.length(); i++) { ll -= (x[i] - mu)<em>(x[i] - mu); } ll <em>= 0.5/sigma/sigma; ll -= (0.5\</em>log(2\</em>M_PI) + log(sigma))*x.length(); NumericVector result(1, ll); // create ‘numeric’ vector of length 1, filled with // ll values return result; } [/code] The function takes two parameters, <code>s_beta</code> and <code>s_x</code>. These are passed as R general vectors, denoted <code>SEXP</code> in C. As SEXP-s are complicated to handle, the following two lines transform those to ‘NumericVector’s, essentially equivalent to R ‘numeric()’. The following code is easy to understand. We loop over all iterations and add (x[i] - mu)^2. Loops are cheap in C++. Afterwards, we add the constant terms only once. Note that unlike R, indices in C++ start from zero. This program must be compiled first. Normally the command [code language=”bash”] R CMD SHLIB loglik.cpp [/code] takes care of all the R-specific dependencies, and if everything goes well, it results in the DLL file. Rcpp requires additional include files which must be specified when compiling, the location of which can be queried with <code>Rcpp:::CxxFlags()</code> command in R, or as a bash one-liner, one may compile with [code language=”bash”] PKG_CXXFLAGS=$(echo ‘Rcpp:::CxxFlags()’| R –vanilla –slave) R CMD SHLIB loglik.cpp [/code] Now we have to create the R-side of the log-likelihood function. It may look like this: [code language=”r”] llc &lt;- function(par, x) { library(Rcpp) dyn.load(“loglik.so”) # extension ‘.so’ is platform-specific! res &lt;- .Call(“loglik”, par, x) res } [/code] It takes arguments ‘par’ and ‘x’, and passes these down to the DLL. Before we can invoke (<code>.Call</code>) the compiled code, we must load the DLL. You may need to adjust the exact name according to your platform here. Note also that I haven’t introduced any security checks neither at the R nor the C++ side. This is a quick recipe for crashing your session, but let’s avoid it here in order to keep the code simple. Now let’s run it: [code language=”r”] system.time(m &lt;- maxLik(llc, start=start, x=x)) [/code]</p>
<p>   user  system elapsed<br>  0.896   0.020   0.913 </p>
<p>The C code runs almost exactly as fast as the optimized R. In case of well vectorized computations, there seems to be little scope for improving the speed by switching to C.</p>
<h1 id="Parallelizing-the-code-on-multicore-CPUs"><a href="#Parallelizing-the-code-on-multicore-CPUs" class="headerlink" title="Parallelizing the code on multicore CPUs"></a>Parallelizing the code on multicore CPUs</h1><p>Now it is time to write a parallel version of the program. Take the C++ version as the point of departure and re-write it like this: [code language=”cpp”] #include <cmath> #include &lt;Rcpp.h&gt; #include &lt;omp.h&gt; using namespace Rcpp; RcppExport SEXP loglik_MP(SEXP s_beta, SEXP s_x, SEXP s_nCpu) { NumericVector x(s_x); NumericVector beta(s_beta); int n_cpu = IntegerVector(s_nCpu)[0]; double mu = beta[0]; double sigma = beta[1]; double ll = 0; omp_set_dynamic(0); // Explicitly disable dynamic teams omp_set_num_threads(n_cpu); // Use n_cpu threads for all // consecutive parallel regions #pragma omp parallel { double ll_thread = 0; #pragma omp for for(int i = 0; i &lt; x.length(); i++) { ll_thread -= (x[i] - mu)<em>(x[i] - mu); } #pragma omp critical { ll += ll_thread; } } ll <em>= 0.5/sigma/sigma; ll -= (0.5\</em>log(2\</em>M_PI) + log(sigma))<em>x.length(); NumericVector result(1, ll); return result; } [/code] The code structure is rather similar to the previous example. The most notable novelties are the ‘#pragma omp’ directives. These tell the compiler to insert parallelized code here. Not all compilers understand it, and others may need special flags, such as <code>-fopenmp</code> in case of gcc, to enable openMP support. Otherwise, gcc just happily ignores the directives and you will get a single-threaded application. The likelihood function also includes the argument _n_Cpu_, and the commands <code>omp_set_dynamic(0)</code> and <code>omp_set_num_threads(n_cpu)</code>. This allows to manipulate the number of threads explicitly, it is usually not necessary in the production code. For compiling the program, we can add <code>-fopenmp</code> to our one-liner above: [code language=”bash”] PKG_CXXFLAGS=”$(echo ‘Rcpp:::CxxFlags()’| R –vanilla –slave) -fopenmp” R CMD SHLIB loglikMP.cpp [/code] assuming it was saved in “loglikMP.cpp”. But now you should seriously consider writing a makefile instead. We use three openMP directives here: [code language=”cpp”] #pragma omp parallel { /</em> code block <em>/ } [/code] This is the most important omp directive. It forces the code block to be run in multiple threads, by all threads simultaneously. In particular, variable _ll_thread_ is declared in all threads separately and is thus a thread-specific variable. As OMP is a shared-memory parallel framework, all data declared before <em>#pragma omp parallel</em> is accessible by all threads. This is very convenient as long as we only read it. The last directive is closely related: [code language=”cpp”] #pragma omp critical { /</em> code block <em>/ } [/code] This denotes a piece of threaded code that must be run by only one thread simultaneously. In the example above all threads execute <code>ll += ll_thread</code>, but only one at a time, waiting for the previous thread to finish if necessary. This is because now we are writing to shared memory: variable <em>ll</em> is defined before we split the code into threads. Allowing multiple threads to simultaneously write in the same shared variable almost always leads to trouble. Finally, [code language=”cpp”] #pragma omp for for(…) { /</em> code block */ } [/code] splits the for loop between threads in a way that each thread will only go through a fraction of the full loop. For instance, in our case the full loop goes over 1M observations, but in case of 8 threads, each will receive only 125k. As the compiler has to generate code for this type of loop sharing, parallel loops are less flexible than ordinary single-threaded loops. For many data types, summing the thread–specific values we did with <code>#pragma omp critical</code> can be achieved directly in the loop by specifying <code>#pragma omp parallel for reduction(+:ll)</code> instead. As all the parallel work is done at C level, the R code remains essentially unchanged. We may write the corresponding loglik function as [code language=”r”] llcMP &lt;- function(par, nCpu=1, x) { library(Rcpp) dyn.load(“loglikMP.so”) res &lt;- .Call(“loglik_MP”, par, x, as.integer(nCpu)) res } [/code] How fast is this? [code language=”r”] system.time(m &lt;- maxLik(llcMP, start=start, nCpu=4, x=x)) [/code]</p>
<p>   user  system elapsed<br>  0.732   0.016   0.203 </p>
<p>On 2-core/4-thread cpu, we got a more than four–fold speed boost. This is impressive, given the cpu does have 2 complete cores only. Obviously, the performance improvement depends on the task. This particular problem is embarrasingly parallel, the threads can work completely independent of each other.</p>
<h1 id="Timing-Examples"><a href="#Timing-Examples" class="headerlink" title="Timing Examples"></a>Timing Examples</h1><p>As an extended timing example, we run all the (optimized) examples above using a Xeon-L5420 cpu with 8 cores, single thread per core. The figure below depicts the compute time for single-threaded R and C++ code, and for C++/openMP code with 8 threads, as a function of data size. <img src="http://www.parallelr.com/wp-content/uploads/2016/07/timings-1-1024x585.png" alt="timings"> The figure reveals several facts. First, for non-parallelized code we can see that</p>
<ol>
<li> Optimized R and C++ code are of virtually identical speed.</li>
<li> compute time grows linearily in data size.</li>
</ol>
<p>For openMP code the figure tells</p>
<ol start="3">
<li> openMP with 8 threads is substantially slower for data size less than about 100k. For larger data, multi-threaded approach is clearly faster.</li>
<li> openMP execution time is almost constant for data size up to 4M. For larger data vectors, it increases linearily. This suggests that for smaller data size, openMP execution time is dominated by thread creation and management overheads, not by computations.</li>
</ol>
<p>Finally, let’s compare the computation times for different number of threads for 8M data size. <img src="http://www.parallelr.com/wp-content/uploads/2016/07/timings_n-2-1024x585.png" alt="timings_n"> The figure shows the run time for single threaded versions of the code (R and C), and multi-threaded openMP versions with 1 to 9 threads (OMP.1 to OMP.9).</p>
<ol>
<li> More cpus give us shorter execution times. 1-thread OMP will run almost 1.7 times slower than 8-threaded version (3.9 and 2.3 s respectively).</li>
<li> The gain of more cpu cores working on the problem levels off quickly. Little noticeable gain is visible for more than 3 cores. It indicates that the calculations are only partly limited by computing-power. Another major bottleneck may be memory speed.</li>
<li> Last, and most strikingly, even the single threaded OMP version of the code is 4.8 times faster than single-threaded C++ version with no OMP (18.6 and 3.9 s respectively)! This is a feature of the particular task, the compiler and the processor architecture. OMP parallel for–loops allow the compiler to deduce that the loops are in fact independent, and use faster SSE instruction set. This substantially boosts the speed but requires more memory bandwidth.</li>
</ol>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>With the examples above I wanted to show that for many tasks, openMP is not hard to use. If you know some C++, parallelizing your code may be quite easy. True, the examples above are easy to parallelize at R-level as well, but there are many tasks where this is not true. Obviously, in the text above I just scratched the surface of openMP. If you consider using it, there are many excellent sources on the web. Take a look!</p>
<p>I am grateful to Peng Zhao for explaining the parallel loops and SSE instruction set.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://jitmatrix.github.io/oneXPU/2016/07/26/r-and-openmp-boosting-compiled-code-on-multi-core-cpu-s/" data-id="ckivbjgo6000d6lol98u50bhq" data-title="R and openMP: boosting compiled code on multi-core cpu-s" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Maximum-Likelihood/" rel="tag">Maximum Likelihood</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/R/" rel="tag">R</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Rcpp/" rel="tag">Rcpp</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SSE/" rel="tag">SSE</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/multicores/" rel="tag">multicores</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/multit/" rel="tag">multit</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/openMP/" rel="tag">openMP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/parallel-computing/" rel="tag">parallel computing</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/performance-optimization/" rel="tag">performance optimization</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/rstats/" rel="tag">rstats</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-r-dnn-cuda-multigpu" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2016/05/09/r-dnn-cuda-multigpu/" class="article-date">
  <time class="dt-published" datetime="2016-05-09T00:00:08.000Z" itemprop="datePublished">2016-05-09</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Accelerators/">Accelerators</a>►<a class="article-category-link" href="/categories/Accelerators/GPGPU/">GPGPU</a>►<a class="article-category-link" href="/categories/Accelerators/GPGPU/Performance-Optimizaiton/">Performance Optimizaiton</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2016/05/09/r-dnn-cuda-multigpu/">R for Deep Learning (III): CUDA and MultiGPUs Acceleration</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Notes: 1. The entire source code of this post in <a target="_blank" rel="noopener" href="https://github.com/PatricZhao/ParallelR/blob/master/ParDNN">here</a> 2. The PDF version of this post in <a target="_blank" rel="noopener" href="http://www.parallelr.com/materials/4_CUDA/CUDA_DNN.pdf">here</a></p>
<hr>
<p>In previous two blogs (<a target="_blank" rel="noopener" href="http://www.parallelr.com/r-deep-neural-network-from-scratch/">here</a> and <a target="_blank" rel="noopener" href="http://www.parallelr.com/r-dnn-parallel-acceleration/">here</a>), we illustrated several skills to build and optimize artificial neural network (ANN) with R and speed up by parallel BLAS libraries in modern hardware platform including Intel Xeon and NVIDIA GPU. Nowadays, multiple GPU accelerations are crucial for learning huge networks, one example, as Microsoft won ImageNet competition with huge network up to 1000 layers in 2015, [<a target="_blank" rel="noopener" href="http://www.i-programmer.info/news/105-artificial-intelligence/9266-microsoft-wins-imagenet-using-extremely-deep-neural-networks.html">here</a> and <a target="_blank" rel="noopener" href="http://image-net.org/challenges/LSVRC/2015/results">here</a>]. In this blog, I will focus on applying CUDA implementation into our neural network offloading the computationally intensive parts into GPU and then we can easily extend CUDA implementation from single GPU to multiple GPUs under ‘<a target="_blank" rel="noopener" href="https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf">parallel</a>’ packages of R.  P.S: If you want to go through all of these contents quickly, check out my presentation in GTC16 in <a target="_blank" rel="noopener" href="http://www.parallelr.com/GTC16/GTC16_PatricZhao_Unlock_DNN_Perf_CUDA.pdf">here</a>.  </p>
<h2 id="CUDA-INTEGRATION"><a href="#CUDA-INTEGRATION" class="headerlink" title="CUDA INTEGRATION"></a>CUDA INTEGRATION</h2><p>Now, we begin to introduce our <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Ninja">ninja</a> skills : CUDA After  combined DNN code with CUDA BLAS library and several optimizations, we get the follow results in the table <a target="_blank" rel="noopener" href="http://www.parallelr.com/r-dnn-parallel-acceleration/">in the previous blog</a> and leave one question for readers:</p>
<p><strong>What is your opinion about the next step of optimizations?</strong></p>
<p><a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/03/orig.png"></a><a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/03/orig-1.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/03/orig-1.png" alt="R_ANN"></a> It’s obvious that the function ‘pmax’ accounts for lots of runtimes (<strong>31.58</strong> secs) following ‘%<em>%’ (<strong>53.72</strong> secs) since ‘pmax’ is implemented by R and it will be very slow when the data size increase. (btw, you can try to increase the number of neurons in hidden layer to 1024 and profiling the code again to figure out the ratio of ‘pmax’ in the whole computation).  Reviewing the functionality of ‘pmax’ in our DNN case, we implement the ReLU function and get the maximum value among input value and 0 for every neuron in hidden layer. Furthermore, because our algorithm is vectorized by matrices for high performance,  the input of ‘pmax’ is a two-dimensional matrix and we can parallel maximum function into each element easily. So, let’s start to parallel the ReLu function by CUDA. I will skip the details of CUDA programming in this blog, you can refer programming guide in <a target="_blank" rel="noopener" href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/#axzz47CbpOe1j">here</a>. First, we can write the CUDA code to compare the input value with ZERO. Despite it  is a very naïve implementation, it is still very fast than original R code. Throughout this kernel code can be executed in NVIDIA GPU, it is written by CUDA C rather than R. Next, we need to call it from R environment. In general, we have to write wrapper functions to bridge R,  C/C++, and CUDA. <a target="_blank" rel="noopener" href="http://heather.cs.ucdavis.edu/matloff.html">Prof. Matloff</a> and I have written the blog introduced linking R with CUDA step by step regarding with .Call() and .C() function with two-level wrappers from R to C/C++ and C/C++ to CUDA (<a target="_blank" rel="noopener" href="https://devblogs.nvidia.com/parallelforall/accelerate-r-applications-cuda/">here</a>  and <a target="_blank" rel="noopener" href="http://blog.revolutionanalytics.com/2015/01/parallel-programming-with-gpus-and-r.html">here</a>).  A brief summary about the major difference of .C() and .Call() is shown in below table. <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/03/C_Call.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/03/C_Call.png" alt="R_Cal_ function"></a> From the performance view, the .Call() function is selected since little overhead between R and C/C++ by avoiding explicit copying data from R to C/C++ and then from C/C++ back to R. In below code, we access data by a pointer and heavily use R internal structures with very efficient way. [code language=”C”] // difinition for R extern “C” { SEXP pmax_cuda(SEXP A, SEXP threshold, SEXP devID); } //CUDA: simple implementation of pmax __global__ void pmax_kernel(double \</em>A, const int M, const int N, const double threshold) { int tid = blockIdx.x * blockDim.x + threadIdx.x; if(tid &lt; M*N) { A[tid] = (A[tid]&gt;threshold)?A[tid]:0; } return; } // Wrapper code between R and CUDA SEXP pmax_cuda(SEXP A, SEXP threshold, SEXP devID) { // data structure for GPU double *A_host = NULL; double *A_d = NULL; double gw = 0; int mm = 0, nn = 0; int gpuID = 0; // data transfer from R to C by pointers A_host = REAL(A); SEXP Rdim = getAttrib(A, R_DimSymbol); mm = INTEGER(Rdim)[0]; nn = INTEGER(Rdim)[1]; gw = REAL(threshold)[0]; gpuID = INTEGER(devID)[0]; // for multiple GPU case cudaSetDevice(gpuID); // return value, allocated in C and can be used in R directly SEXP Rval; PROTECT(Rval = allocVector(REALSXP, mm*nn)); // GPU memory allocation cudaMalloc(&amp;A_d, mm*nn*sizeof(double)); if(NULL == A_d) { printf(“\nNo RAM space in GPU!\n”); UNPROTECT(1); return R_NilValue; } // memory copy from CPU to GPU cudaMemcpy(A_d, A_host, mm*nn*sizeof(double), cudaMemcpyHostToDevice); // CUDA: pmax, really computation parts pmax_kernel&lt;&lt;&lt;(mm*nn-1)/512+1, 512&gt;&gt;&gt;(A_d, mm, nn, gw); cudaMemcpy(REAL(Rval), A_d, mm*nn*sizeof(double), cudaMemcpyDeviceToHost); cudaDeviceSynchronize(); // Free unused memory of GPU if(A_d) {cudaFree(A_d); A_d=NULL;} UNPROTECT(1); return Rval; } [/code] Next, compile the C/C++ and CUDA code together to a shared object file (.so) or dynamic link library (.dll) for loading in R.</p>
<blockquote>
<p>nvcc -O3 -arch=sm_35 -G -I…/CUDA-v7.5.18/include -I…/R-3.2.0/bin/lib64/R/include/ -L…/R/lib64/R/lib –shared -Xcompiler -fPIC -o cudaR.so cudaR.cu</p>
</blockquote>
<p>Finally, the CUDA version of ‘pmax’ can be called in R as simple as R builtin function with R’s wrapper, and,  for infrastructure engineer, writing a nice wrapper is still an important job :)</p>
<p># preload static object file<br><a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/dyn.load">dyn.load</a>(“cudaR.so”)<br> <br># GPU version of pmax<br>pmax.cuda &lt;- <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/function">function</a>(A, threshold, devID=0L)<br>{<br>  rst &lt;- <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/.Call">.Call</a>(“pmax_cuda”,<br>                A,<br>                threshold,<br>                <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/as.integer">as.integer</a>(devID)<br>          )<br>  <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/dim">dim</a>(rst) &lt;- <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/dim">dim</a>(A)<br>  <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/return">return</a>(rst)<br>}</p>
<p>Show our performance now!  By replacing ‘pmax’ with  new ‘pmax.cuda’,  the execution time of pmax reduces to <strong>6.7</strong> seconds from original 31.58 so it’s <strong>5X speedup</strong> and totally the <strong>1.2X speedup</strong> gains. <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/03/cuda.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/03/cuda-300x291.png" alt="cuda pmax "></a>  </p>
<h2 id="Scale-out-to-MultiGPU"><a href="#Scale-out-to-MultiGPU" class="headerlink" title="Scale out to MultiGPU"></a><strong>Scale out to MultiGPU</strong></h2><p>Parallel computing is not a novel concept in R community. Data scientist is familiar with parallel strategies both in speedup their model construction and inference. In fact, the requirement of parallel computing in R is even higher than C/C++.  The C/C++ implementations always focus on low-level instructions and optimizations such as memory locality, communication, computation efficiency and much more while R aims to fast, easy and portability from high-level programming. Popular R packages handle most of low-level details and R users only focus on dataset decomposition and functional programming. Specifically, in this blog, we will show you parallel training of DNN with ‘<a target="_blank" rel="noopener" href="https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf">parallel</a>’ package and extend it to multiGPUs.</p>
<h4 id="HOGWILD"><a href="#HOGWILD" class="headerlink" title="HOGWILD!"></a>HOGWILD!</h4><p><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/1106.5730v2.pdf">HOGWILD!</a> is a data parallel model designed for stochastic gradient descent. It’s a lock-free approach with the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/MapReduce">MapReduce</a>-like parallel-processing framework and can be used in DNN training . Thus, the training processing is designed as below: 1.Launch N workers 2.Each worker updates local weights/bias based on parts (1/N) of data 3.Master collects and averages all weights/bias from each worker 4.Each worker updates its weights/bias from master</p>
<h4 id="Parallel-in-R"><a href="#Parallel-in-R" class="headerlink" title="Parallel in R"></a>Parallel in R</h4><p>In R, this flow can be implemented by ‘multicores’ packages (currently, ‘parallel’ package includes both ‘multicores’ and ‘snow’ in CRAN). In below, flow chart is the standard workflow of ‘multicore’ packages with our DNN. ‘mclapply’ function creates two processors which shares memory by copy-on-write and each processor train the network by parts of data. After several steps, the master processor will do a reduce step to collect  weights from two child processors and average them.In next iteration, two children will use the new weights. <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/03/mclappy.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/03/mclappy-1024x555.png" alt="mclappy with GPU"></a> Now, let’s see the details of how R code handles this data parallel model based on below real codes . 1. ‘mclapply’ creates N (devNum) workers based on ‘mc.cores’ and each worker will execute the same function, train.dnn.cublas, with different index (1:devNum); 2. the data is divided into N (devNum) parts and each worker will load their data simultaneously by their ID then the computation, even writing, can be ideally parallelized; 3. all workers exit when ‘mclapply’ is done and the results from every worker will be saved in a list (res).  Master continues to remain parts and then calculate the average of all weights and bias. 4. in the next loop, the ‘mclapply’ will use the averaged model (para.model) to train again.</p>
<p># Parallel Training<br>res &lt;- <strong>mclapply</strong>(1:devNum, <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/function">function</a>(id) { train.dnn.cublas(x, y,<br>                                         omodel=para.model,<br>                                         taindata=traindata[N.start[id]:N.end[id],],<br>                                         devType=“<strong>GPU</strong>”, devID=(id-1), . . .) },<br>                mc.cores=devNum,<br>                mc.preschedule=TRUE)<br> <br> # Construct new model with parallel weights<br>    <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/stats/D">D</a> &lt;- res[[1]]$D<br>    H &lt;- res[[1]]$H<br>    K &lt;- res[[1]]$K<br>    for(i in 2:devNum) {<br>            res[[1]]$W1 &lt;- res[[1]]$W1 + res[[i]]$W1<br>            res[[1]]$W2 &lt;- res[[1]]$W2 + res[[i]]$W2<br>            res[[1]]$b1 &lt;- res[[1]]$b1 + res[[i]]$b1<br>            res[[1]]$b2 &lt;- res[[1]]$b2 + res[[i]]$b2<br>    }<br> <br>    para.model &lt;- <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/list">list</a>( <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/stats/D">D</a> = <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/stats/D">D</a>,<br>                        H = H,<br>                        K = K,<br>                        # weights and bias<br>                        W1= res[[1]]$W1/devNum,<br>                        b1= res[[1]]$b1/devNum,<br>                        W2= res[[1]]$W2/devNum,<br>                        b2= res[[1]]$b2/devNum)</p>
<h4 id="Extent-to-MultiGPU"><a href="#Extent-to-MultiGPU" class="headerlink" title="Extent to MultiGPU"></a>Extent to MultiGPU</h4><p>Then scale to multiple GPUs, the workflow is almost as similar as CPU and only different is that each worker needs to set GPU ID explicitly and then run previous CUDA accelerated code. In other words, users still are able to access the same CUDA codes that they usually use (almost) without any change!  In our implementation, we adopt the strategy of a consistent one-to-one match between CPU worker with GPU by setting GPU index as below. [code language=”C”] // for multiple GPU case cudaSetDevice(gpuID); [/code]</p>
<h4 id="Performance-Showcase"><a href="#Performance-Showcase" class="headerlink" title="Performance Showcase"></a>Performance Showcase</h4><p>Finally, we analyzed the performance of CPU and GPU code. The line plot shows the strong scalability of native R code (1 hidden layer and 512 neurons). And compared with H2O, native R exhibits good scalability with the number of thread increase. Look at GPU parts, one thread with one GPU is faster than 20 threads CPU implementation (both native R and H2O). Next, look at GPU scalability in bar plot where <strong>5 times</strong> speedup under 6 GPUs are reached and our algorithm achieved <strong>160 times</strong> speedup compared with original R code. Testing on : CPU:  Ivy Bridge E5-2690 v2 @ 3.00GHz, dual socket 10-core, 128G RAM;  GPU: NVIDIA K40m,  12G RAM <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/03/MultiGPU_Runtime.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/03/MultiGPU_Runtime.png" alt="MultiGPU_Runtime"></a></p>
<p><a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/03/MultiGPU_Speedup.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/03/MultiGPU_Speedup.png" alt="MultiGPU_Speedup"></a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://jitmatrix.github.io/oneXPU/2016/05/09/r-dnn-cuda-multigpu/" data-id="ckivbjgod000j6lol5z65frxb" data-title="R for Deep Learning (III): CUDA and MultiGPUs Acceleration" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CUDA/" rel="tag">CUDA</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/H2O/" rel="tag">H2O</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/R/" rel="tag">R</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deep-learning/" rel="tag">deep learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/" rel="tag">machine learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/multicores/" rel="tag">multicores</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/mutltiGPU/" rel="tag">mutltiGPU</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/parallel-computing/" rel="tag">parallel computing</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/profiling/" rel="tag">profiling</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/rstats/" rel="tag">rstats</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/snow/" rel="tag">snow</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-r-gpu-programming-for-all-with-gpur" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2016/05/04/r-gpu-programming-for-all-with-gpur/" class="article-date">
  <time class="dt-published" datetime="2016-05-04T13:52:08.000Z" itemprop="datePublished">2016-05-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/GPGPU/">GPGPU</a>►<a class="article-category-link" href="/categories/GPGPU/MultiCores/">MultiCores</a>►<a class="article-category-link" href="/categories/GPGPU/MultiCores/Performance-Optimizaiton/">Performance Optimizaiton</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2016/05/04/r-gpu-programming-for-all-with-gpur/">R - GPU Programming for All with &#39;gpuR&#39;</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>GPUs (Graphic Processing Units) have become much more popular in recent years for computationally intensive calculations.  Despite these gains, the use of this hardware has been very limited in the R programming language.  Although possible, the prospect of programming in either OpenCL or CUDA is difficult for many programmers unaccustomed to working with such a low-level interface.  Creating bindings for R’s high-level programming that abstracts away the complex GPU code would make using GPUs far more accessible to R users.  This is the core idea behind the <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/gpuR/index.html">gpuR</a> package.  There are three novel aspects of <code>gpuR</code>:</p>
<ol>
<li> Applicable on ‘ALL’ GPUs</li>
<li> Abstracts away CUDA/OpenCL code to easily incorporate in to existing R algorithms</li>
<li> Separates copy/compute functions to allow objects to persist on GPU</li>
</ol>
<h3 id="Broad-application"><a href="#Broad-application" class="headerlink" title="Broad application:"></a><strong>Broad application</strong>:</h3><p>The ‘gpuR’ package was created to bring the power of GPU computing to any R user with a GPU device.  Although there are a handful of packages that provide some GPU capability (e.g. <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/gputools/index.html">gputools</a>, <a target="_blank" rel="noopener" href="http://cran.fhcrc.org/web/packages/cudaBayesreg/index.html">cudaBayesreg</a>, <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/HiPLARM/index.html">HiPLARM</a>, <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/HiPLARb/index.html">HiPLARb</a>, and <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/gmatrix/index.html">gmatrix</a>) all are strictly limited to NVIDIA GPUs.  As such, a backend that is based upon OpenCL would allow all users to benefit from GPU hardware.  The ‘gpuR’ package therefore utilizes the <a target="_blank" rel="noopener" href="http://viennacl.sourceforge.net/">ViennaCL</a> linear algebra library which contains auto-tuned OpenCL kernels (among others) that can be leveraged for GPUs.  The headers have been conveniently repackaged in the <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/RViennaCL/index.html">RViennaCL </a>package.  It also allows for a CUDA backend for those with NVIDIA GPUs that may see further improved performance (contained within the companion <a target="_blank" rel="noopener" href="https://github.com/cdeterman/gpuRcuda">gpuRcuda</a> package not yet formally released).</p>
<h3 id="Abstract-away-GPU-code"><a href="#Abstract-away-GPU-code" class="headerlink" title="Abstract away GPU code:"></a><strong>Abstract away GPU code:</strong></h3><p>The <code>gpuR</code> package uses the S4 object oriented system to have explicit classes and methods that all the user to simply cast their <code>matrix</code> or <code>vector</code> and continue programming in R as normal.  For example: [code language=”r”] ORDER = 1024 A = matrix(rnorm(ORDER^2), nrow=ORDER) B = matrix(rnorm(ORDER^2), nrow=ORDER) gpuA = gpuMatrix(A, type=”double”) gpuB = gpuMatrix(B, type=”double”) C = A %<em>% B gpuC = gpuA %</em>% gpuB all.equal(C == gpuC[]) [1] TRUE [/code] The <code>gpuMatrix</code> object points to a matrix in RAM which is then computed by the GPU when a desired function is called.  This avoids R’s habit of copying the memory of objects.  For example: [code language=”r”] library(pryr) # Initially points to same object x = matrix(rnorm(16), 4) y = x address(x) [1] “0x16177f28” address(y) [1] “0x16177f28” # But once modify the second object it creates a copy y[1,1] = 0 address(x) [1] “0x16177f28” address(y) [1] “0x15fbb1d8 [/code] In contrast, the same syntax for a <code>gpuMatrix</code> will modify the original object in-place without any copy. [code language=”r”] library(pryr) library(gpuR) # Initially points to same object x = gpuMatrix(rnorm(16), 4, 4) y = x x@address [1] &lt;pointer: 0x6baa040&gt; y@address [1] &lt;pointer: 0x6baa040&gt; # Modification affects both objects without copy y[1,1] = 0 x@address [1] &lt;pointer: 0x6baa040&gt; y@address [1] &lt;pointer: 0x6baa040&gt; [/code]   Each new variable assigned to this object will only copy the pointer thereby making the program more memory efficient.  However, the <code>gpuMatrix&gt;</code> class does still require allocating GPU memory and copying data to device for each function call. The most commonly used methods have been overloaded such as  %*%, +, -, *, /, crossprod, tcrossprod, and trig functions among others.  In this way, an R user can create these objects and leverage GPU resources without the need to know a bunch more functions that would break existing algorithms.</p>
<h3 id="Distinct-Copy-Compute-Functionality"><a href="#Distinct-Copy-Compute-Functionality" class="headerlink" title="Distinct Copy/Compute Functionality:"></a><strong>Distinct Copy/Compute Functionality:</strong></h3><p>For the <code>gpuMatix</code> and <code>gpuVector</code> classes there are companion <code>vclMatrix</code> and <code>vclVector</code> class that point to objects that persist in the GPU RAM.  In this way, the user explicitly decides when data needs to be moved back to the host.  By avoiding unnecessary data transfer between host and device performance can significantly improve.  For example: [code language=”r”] vclA = vclMatrix(rnorm(10000), nrow = 100) vclB = vclMatrix(rnorm(10000), nrow = 100) vclC = vclMatrix(rnorm(10000), nrow = 100) # GEMM vclD = vclA %*% vclB # Element-wise addition vclD = vclD + vclC [/code] In this code, the three initial matrices already exist in the GPU memory so no data transfer takes place in the GEMM call.  Furthermore, the returned matrix remains in the GPU memory.  In this case, the ‘vclD’ object is still in GPU RAM. As such, the element-wise addition call also happens directly on the GPU with no data transfers. It is worth also noting that the user can still modify elements, rows, or columns with the exact same syntax as a normal R matrix. [code language=”r”] vclD[1,1] = 42 vclD[,2] = rep(12, 100) vclD[3,] = rep(23, 100) [/code] These operations simply copy the <em>new</em> elements to the GPU and modify the object in-place within the GPU memory. The ‘vclD’ object is never copied to the host.</p>
<h3 id="Benchmarks"><a href="#Benchmarks" class="headerlink" title="Benchmarks:"></a>Benchmarks:</h3><p>With all that in mind, how does gpuR perform?  Here are some general benchmarks of the popular GEMM operation.  I currently only have access to a single NVIDIA GeForce GTX 970 for these simulations.  Users should expect to see differences with high performance GPUs (e.g. AMD FirePro, NVIDIA Tesla, etc.). Speedup relative to CPU will also vary depending upon user hardware.</p>
<h4 id="1-Default-dGEMM-vs-Base-R"><a href="#1-Default-dGEMM-vs-Base-R" class="headerlink" title="(1) Default dGEMM vs Base R"></a>(1) Default dGEMM vs Base R</h4><p>R is known to only support two numeric types (integer and double).  As such, Figure 1 shows the fold speedup achieved by using the <code>gpuMatrix</code> and <code>vclMatrix</code> classes.  Since R is already known to not be the fastest language, an implementation with the OpenBLAS backend is included as well for reference using a 4 core Intel i5-2500 CPU @ 3.30GHz.  As can be seen there is a dramatic speedup from just using OpenBLAS or the gpuMatrix class (essentially equivalent).  Of interest is the impact of the transfer time from host-device-host that is typical in many GPU implementations.  This cost is eliminated by using the <code>vclMatrix</code> class which continues to scale with matrix size. [caption id=”attachment_768” align=”aligncenter” width=”640”]<img src="http://www.parallelr.com/wp-content/uploads/2016/05/dgemm-1024x588.png" alt="dgemm"> Figure 1 - Fold speedup achieved using openblas (CPU) as well as the gpuMatrix/vclMatrix (GPU) classes provided in gpuR.[/caption]  </p>
<h4 id="2-sGEMM-vs-Base-R"><a href="#2-sGEMM-vs-Base-R" class="headerlink" title="(2) sGEMM vs Base R"></a>(2) sGEMM vs Base R</h4><p>In many GPU benchmarks there is often float operations measured as well.  As noted above, R does not provide this by default.  One way to go around this is to use the <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/RcppArmadillo/index.html">RcppArmadillo</a> or RcppEigen packages and explicitly casting R objects as float types.  The armadillo library will also default to using the BLAS backend provided (i.e. OpenBLAS).  Float types are implemented <code>gpuR</code> by setting <code>type = &quot;float&quot;</code> in the matrix calls (e.g. <code>vclMatrix(mat, type = &quot;float&quot;)</code>) in Figure 2 shows the impact of using float data types.  OpenBLAS continues to provide a noticeable speedup but <code>gpuMatrix</code> begins to outperform once matrix order exceeds 1500.  The <code>vclMatrix</code> continues to demonstrate the value in retaining objects in GPU memory and avoiding memory transfers.   [caption id=”attachment_769” align=”aligncenter” width=”640”]<img src="http://www.parallelr.com/wp-content/uploads/2016/05/sgemm-1024x588.png" alt="sgemm"> Figure 2 - Float type GEMM comparisons. Fold speedup achieved using openblas (via RcppArmadillo) as well as the gpuMatrix/vclMatrix (GPU) classes provided in gpuR.[/caption]   To give an additional view on the performance achieved by <code>gpuMatrix</code> and <code>vclMatrix</code> is comparing directly against the OpenBLAS performance.  The <code>gpuMatrix</code> reaches ~2-3 fold speedup over OpenBLAS whereas <code>vclMatrix</code> scales to over 100 fold speedup!  It is curious as to why the performance with vcl<code>Matrix</code> is so much faster (only differing in host-device-host transfers).  Further optimization with <code>gpuMatrix</code> will need to be explored (fresh eyes are welcome) accepting limitations in the BUS transfer speed.  Performance will certainly improve with improved hardware capabilities such as NVIDIA’s NVLink. [caption id=”attachment_831” align=”aligncenter” width=”737”]<img src="http://www.parallelr.com/wp-content/uploads/2016/05/sgemm_openblas-1024x528.png" alt="sgemm_openblas"> Figure 3 - Fold speedup achieved over openblas (via RcppArmadillo) float type GEMM comparisons vs the gpuMatrix/vclMatrix (GPU) classes provided in gpuR.[/caption]</p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>The <code>gpuR</code> package has been created to bring GPU computing to as many R users as possible.  It is the intention to use <code>gpuR</code> to more easily supplement current and future algorithms that could benefit from GPU acceleration.  The <code>gpuR</code> package is currently available on <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/gpuR/index.html">CRAN</a>.  The development version can be found on my <a target="_blank" rel="noopener" href="https://github.com/cdeterman/gpuR">github</a> in addition to existing issues and wiki pages (assisting primarily in installation).  Future developments include solvers (e.g. QR, SVD, cholesky, etc.), scaling across multiple GPUs,  ‘sparse’ class objects, and custom OpenCL kernels. As noted above, this package is intended to be used with a multitude of hardware and operating systems (it has been tested on Windows, Mac, and multiple Linux flavors).  I only have access to a limited set of hardware (I can’t access every GPU, let along the most expensive).  As such, the development of <code>gpuR</code> depends upon the R user community.  Volunteers who possess different hardware are always welcomed and encouraged to submit issues regarding any discovered bugs.  I have begun a gitter account for users to report on successful usage with alternate hardware.  Suggestions and general conversation about gpuR is welcome.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://jitmatrix.github.io/oneXPU/2016/05/04/r-gpu-programming-for-all-with-gpur/" data-id="ckivbjgoi000q6lolc8kmek5e" data-title="R - GPU Programming for All with &#39;gpuR&#39;" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GEMM/" rel="tag">GEMM</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GPU/" rel="tag">GPU</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HPC/" rel="tag">HPC</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/R/" rel="tag">R</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/openblas/" rel="tag">openblas</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-r-hpac-benchmark-analysis" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2016/04/15/r-hpac-benchmark-analysis/" class="article-date">
  <time class="dt-published" datetime="2016-04-15T05:10:18.000Z" itemprop="datePublished">2016-04-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Accelerators/">Accelerators</a>►<a class="article-category-link" href="/categories/Accelerators/GPGPU/">GPGPU</a>►<a class="article-category-link" href="/categories/Accelerators/GPGPU/MultiCores/">MultiCores</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2016/04/15/r-hpac-benchmark-analysis/">R benchmark for High-Performance Analytics and Computing (I):Accelerators</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="Objectives-of-Experiments"><a href="#Objectives-of-Experiments" class="headerlink" title="Objectives of Experiments"></a>Objectives of Experiments</h2><p>R is more and more popular in various fields, including the high-performance analytics and computing (HPAC) fields. Nowadays, the architecture of HPC system can be classified as pure CPU system, CPU + Accelerators (GPGPU/FPGA) heterogeneous system, CPU + Coprocessors system. In software side, high performance scientific libraries, such as basic linear algebra subprograms (BLAS), will significantly influence the performance of R for HPAC applications. So, in the first post of R benchmark series, the experiments mainly contain two aspects: <em>(1)  Performance on different architectures of HPC system,</em> <em>(2)  Performance on different BLAS libraries.</em> </p>
<h2 id="Benchmark-and-Testing-Goals"><a href="#Benchmark-and-Testing-Goals" class="headerlink" title="Benchmark and Testing Goals"></a>Benchmark and Testing Goals</h2><p>In this post, we choose R-25 benchmark (available in <a target="_blank" rel="noopener" href="http://r.research.att.com/benchmarks/">here</a> ) which includes the most popular, widely acknowledged functions in the high performance analytic field. The testing script includes fifteen common computational intensive tasks (in Table-1) grouped into three categories: <em>(1) Matrix Calculation (1-5)</em> <em>(2) Matrix function (6-10)</em> <em>(3) Programmation (11-15)</em></p>
<p>Table-1 R-25 Benchmark Description</p>
<p><strong>Task Number</strong></p>
<p><strong>R-25 Benchmark Description</strong></p>
<p>1</p>
<p>Creation,transposition,deformation of a 2500*2500 matrix</p>
<p>2</p>
<p>2400*2400 normal distributed random matrix</p>
<p>3</p>
<p>Sorting of 7,000,000 random values</p>
<p>4</p>
<p>2800*2800 cross-product matrix</p>
<p>5</p>
<p>Linear regression over a 3000*3000 matrix</p>
<p>6</p>
<p>FFT over 2,400,000 random values</p>
<p>7</p>
<p>Eigenvalues of a 640*640 random values</p>
<p>8</p>
<p>Determinant of a 2500*2500 random matrix</p>
<p>9</p>
<p>Cholesky decomposition of a 3000*3000 matrix</p>
<p>10</p>
<p>Inverse of a 1600*1600 random matrix</p>
<p>11</p>
<p>3,500,000 Fibonacci numbers calculation(vector calculation)</p>
<p>12</p>
<p>Creation of a 3000*3000 Hilbert matrix(matrix calculation)</p>
<p>13</p>
<p>Grand common divisors of 400,000 pairs(recursion)</p>
<p>14</p>
<p>Creation of a 500*500 Toeplitz matrix(loops)</p>
<p>15</p>
<p>Escoufier’s method on a 45*45 matrix(mixed)</p>
<p>  In our benchmark, we measured the performance of R-25 benchmark on various hardware platforms, including Intel Xeon CPU processors, NVIDIA GPGPU cards and Intel Xeon Phi coprocessors. Meanwhile, R built with different BLAS libraries results in different performance, so we tested R with self-contained BLAS, OpenBLAS, Intel MKL and CUDA BLAS. <strong>Because the performance of self-contained BLAS is</strong> <strong>hugely**</strong> lower than the other BLAS library and in practice HPAC users of R always built R with high performance BLAS, the testing results running with self-contained BLAS is negligible. ** Moreover, in order to investigate the performance of functions or algorithms such as GEMM that HPC users mostly used, we explore the speed-up when varying the size of the matrices and number of elements as known as <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Scalability">scalability</a>.  </p>
<h2 id="System-Descriptions"><a href="#System-Descriptions" class="headerlink" title="System Descriptions"></a>System Descriptions</h2><p>To evaluate the applicability of different methods for improving R performance in a HPC environment, the hardware and software of platform we used listed in the Table-2 and Table-3. <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/04/table2.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/04/table2.png" alt="hardware configuration"></a> <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/04/table3.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/04/table3.png" alt="software configuration"></a>  </p>
<h2 id="Results-and-Discussions"><a href="#Results-and-Discussions" class="headerlink" title="Results and Discussions"></a>Results and Discussions</h2><h4 id="1-General-Comparisons"><a href="#1-General-Comparisons" class="headerlink" title="(1) General Comparisons"></a><em>(1) General Comparisons</em></h4><p>Fig. 1 shows the speedup of R using different BLAS libraries and different hosts. The default R running with OpenBLAS is shown in red as <strong>our baseline</strong> for comparison so that its speedup is constantly equal to one. Intel Xeon E5-2670 has eight physical cores in one chipset, so there are 16 physical cores in one server node.<a target="_blank" rel="noopener" href="https://software.intel.com/en-us/articles/parallelism-in-the-intel-math-kernel-library">Intel MKL library</a> supports the single thread mode (Sequential) or OpenMP threading mode. MKL with OpenMP threading mode defaultly uses all physical cores in one node(here is 16).Fig.1 shows the results of using Intel MKL for 1 thread and 16 threads with automatic parallel execution are shown in blue. There are five subtasks showing a significant benefit from either optimized sequential math library or the automatic parallelization with MKL including <strong>crossprod</strong> (matrix size 2800*2800), <strong>linear regression</strong>, <strong>matrix decomposition</strong>, <strong>computing inverse</strong> and <strong>determinant of a matrix</strong>. Other non-computational intensive tasks received very little performance gains from parallel execution with MKL. <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/04/image004.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/04/image004.png" alt="Speedup compared with OpenBLAS"></a></p>
<p>Fig.1 Performance comparison among  Intel MKL and NVIDIA BLAS against R+OpenBLAS</p>
<p>We also exploited parallelism with CUDA BLAS (libnvblas.so) on NVIDIA GPU platform. Since drop-in library (nvblas) only accelerated the level 3 BLAS functions and overhead of preloading, the result (green column) in Fig.2 showed little benefit and even worse performance for some computing tasks against Intel MKL accelerations.</p>
<p><a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/04/image005-3.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/04/image005-3-1024x674.png" alt="Speedup against Xeon"></a></p>
<p>Fig.2 Performance comparison for CPU and GPU with NVIDIA BLAS and Intel MKL</p>
<h4 id="2-Scalability-on-NVIDIA-GPU"><a href="#2-Scalability-on-NVIDIA-GPU" class="headerlink" title="(2) Scalability on NVIDIA GPU"></a><em>(2) Scalability on NVIDIA GPU</em></h4><p>The performance using two GPU devices (green column) is not superior to using one GPU device (blue column) , even the results of some subtasks on one GPU device gains more. Taking the function <strong>crossproduct</strong> with computing-intensive as an example is to explain the difference between one GPU device and two GPU device, as followed the Fig. 3. The advantage of the performance of the two card is gradually displayed as the size of the matrix increases. The sub-vertical axis shows the ratio of the elapsed time on two devices to one device. A ratio greater than 1 indicates that the two card performance is better than 1 cards,and the greater the ratio of the two cards, the better the performance of the card.  </p>
<p><a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/04/image007.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/04/image007.png" alt="Scalability on GPU with R"></a>Fig.3 Scalability for 1X and 2X NVIDIA K40m GPU for ‘crossprod’ function</p>
<h4 id="3-Heterogeneous-Parallel-Models-on-Intel-Xeon-Phi-MIC"><a href="#3-Heterogeneous-Parallel-Models-on-Intel-Xeon-Phi-MIC" class="headerlink" title="(3) Heterogeneous Parallel Models on Intel Xeon Phi (MIC)"></a><em>(3) Heterogeneous Parallel Models on Intel Xeon Phi (MIC)</em></h4><p>To compare the parallelism supported by pure CPU (Intel Xeon processor) and Intel Xeon Phi  coprocessor, we conducted <strong>batch runs</strong> (  10 times for the average elapsed time) with the different matrix size of matrix production. MKL supports <a target="_blank" rel="noopener" href="https://software.intel.com/sites/default/files/11MIC42_How_to_Use_MKL_Automatic_Offload_0.pdf"><strong>automatic offload</strong></a> computation to Intel Xeon Phi card, but before using you must know , Automatic offload functions in MKL</p>
<ul>
<li>  Level-3 BLAS: GEMM, TRSM, TRMM, SYMM</li>
<li>  LAPACK 3 amigos : LU, QR, Cholesky</li>
</ul>
<p>Matrix size for offloading</p>
<ul>
<li>  GEMM: M, N &gt;2048, K&gt;256</li>
<li>  SYMM: M, N &gt;2048</li>
<li>  TRSM/TRMM: M, N &gt;3072</li>
<li>  LU: M, N&gt;8192</li>
</ul>
<p>Here, we use <code>**a%*%a</code>** substituted for the function `crossprod` used in R-benchmark-25.R because <code>_crossprod_</code> can not be auto-offloaded to Intel Xeon Phi.  We compared the elapsed time running on CPU+Xeon Phi with running on pure CPU. In Fig.4, the vertical axis is the ratio of running elapsed time with CPU+Xeon Phi running mode to elapsed time with pure CPU running mode. The results showed the greater size of the matrix, the better performance CPU+Xeon Phi gains. The matrix size less than 4000 could get the best performance on pure CPU.  </p>
<p><a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/04/image009-2.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/04/image009-2-1024x674.png" alt="Heterogeneous Computing with Xeon and Xeon Phi for R"></a></p>
<p>Fig.4 Heterogeneous Computing with Intel Xeon and Intel Xeon Phi</p>
<p>Fig.5  shows the 80% computation on Xeon Phi could get the best performance as the matrix size is growing, 70% computation on Xeon Phi could get the steadily better performance when the matrix size larger than 2000. <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/04/image008.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/04/image008-1024x674.png" alt="Scalability for Xeon and Xeon Phi for R"></a></p>
<p>Fig.5 Different computation ratio on Intel Xeon Phi result in different performance</p>
<h4 id="4-Comparison-NVIDIA-GPU-with-Intel-Xeon-Phi"><a href="#4-Comparison-NVIDIA-GPU-with-Intel-Xeon-Phi" class="headerlink" title="(4) Comparison NVIDIA GPU with Intel Xeon Phi"></a><em>(4) Comparison NVIDIA GPU with Intel Xeon Phi</em></h4><p>Here, we plotted the results of NVIDIA GPU and Intel Xeon Phi compared to Intel Xeon in Fig.6. In general, 80% running on Xeon Phi(2X 7110P)+Xeon CPU(2X E5-2670)  gets similar performance to 1X K40m+2X E5-2670(2X 7110P ~ 1X K40m). When the matrix size is less than 12000, GPU gets better performance than Xeon Phi. And after that, Intel Xeon Phi shows the similar performance with NVIDIA K40m. For this benchmark, it can clearly seen that NVIDIA’s Tesla GPU(2X K40m) outperforms significantly.At 16000 of matrix size, nearly 3.9x faster than the 8-core dual E5-2670(Sandy-Bridge CPU) and 2.3x faster than the 80% running on Xeon Phi. The Xeon Phi is 2.8x faster than the Sandy-Bridge.  </p>
<p><a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/04/111.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/04/111.png" alt="Intel Xeon Phi .vs. NVIDIA GPU"></a></p>
<p>Fig.6 Comparison NVIDIA GPU with Intel Xeon Phi</p>
<h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a><strong>Conclusions</strong></h2><p>In this article, we tested the R-benchmark-25.R script on the different hardware platform with different BLAS libraries. From our analysis, we concluded (1) R built with  Intel MKL (either sequential or threaded) can accelerate lots of computationally intensive algorithms of HPAC and get  the best performance, such as linear regression, PCA, SVD (2) R is performed faster on GPU for matrix production (GEMM) since it’s really computational intensive algorithm and GPU has more computing cores than Intel Xeon or Xeon Phi (3) R executed in the heterogeneous platforms (CPU+GPU or CPU+MIC) can gain more performance improvements (4) R can get more benefits from multiple GPUs, especially for large GEMM operations.   In the next post, we will further investigate the benchmark performance with different R parallel packages and commercial productions of R .  </p>
<hr>
<h2 id="Appendix-How-to-build-R-with-different-BLAS-library"><a href="#Appendix-How-to-build-R-with-different-BLAS-library" class="headerlink" title="Appendix : How to build R with different BLAS library"></a><strong>Appendix : How to build R with different BLAS library</strong></h2><h4 id="Stock-R"><a href="#Stock-R" class="headerlink" title="Stock R"></a>Stock R</h4><p>(1) Stock R build Download base R package from the R project <a target="_blank" rel="noopener" href="http://www.r-project.org/">website</a>, the current package is R-3.2.3. Enter into the R root directory, and execute _  &gt; $./configure –with-readline=no –with-x=no –prefix=$HOME/R-3.2.3-ori_ _  &gt; $make -j4_ _  &gt; $make install_ (2) Add R bin directory and library directory to the environment variables PATH and LD_LIBRARY_PATH seperately, just like as: _  &gt; export PATH=$HOME/R-3.2.3-ori/bin:$PATH_ _  &gt; export LD_LIBRARY_PATH=$HOME/R-3.2.3-ori/lib64/R/lib:$LD_LIBRARY_PATH_</p>
<h4 id="R-with-OpenBLAS"><a href="#R-with-OpenBLAS" class="headerlink" title="R with OpenBLAS"></a>R with OpenBLAS</h4><p>(1) OpenBLAS build Download OpenBlas-0.2.15.tar.gz from <a target="_blank" rel="noopener" href="http://www.openblas.net/">http://www.openblas.net/</a> Change directory to OpenBLAS Home directory, and execute _  &gt; $make_ _  &gt; $make PREFIX=$OPENBLAS_INSTALL_DIRECTORY install_ (2) Set the OpenBLAS library environment (3) Run benchmark _  &gt; $LD_PRELOAD=$OPENBLAS_HOME/lib/libopenblas.so R_</p>
<h4 id="R-with-Intel-MKL"><a href="#R-with-Intel-MKL" class="headerlink" title="R with Intel MKL"></a>R with Intel MKL</h4><p>(1）Obtain Intel parallel studio software from <a target="_blank" rel="noopener" href="https://software.intel.com/en-us/intel-parallel-studio-xe">Intel website</a> (2) Install the parallel studio (3) Set the Intel compiler and MKL library environment (4) Build R with MKL Link MKL libraries configuration file mkl.conf as follows: a. Sequencial MKL or MKL single thread</p>
<p><em>#make sure intel compiler is installed and loaded which can be set in .bashrc</em> <em>## as e.g.</em> <em>source /opt/intel/bin/compilervars.sh intel64</em> <em>MKL_LIB_PATH=/opt/intel/mkl/lib/intel64## Use intel compiler</em> <em>CC=’icc -std=c99’</em> <em>CFLAGS=’-g -O3 -wd188 -ip ‘F77=’ifort’</em> <em>FFLAGS=’-g -O3 ‘CXX=’icpc’</em> <em>CXXFLAGS=’-g -O3 ‘FC=’ifort’</em> <em>FCFLAGS=’-g -O3 ‘## MKL sequential, ICC</em> <em>MKL=” -L${MKL_LIB_PATH} \</em> <em>-Wl,–start-group \</em> <em>-lmkl_intel_lp64 \</em> <em>-lmkl_sequential _ _-lmkl_core \</em> <em>-Wl,–end-group”</em></p>
<p>b.  OpenMP Threading MKL</p>
<p><em>#make sure intel compiler is installed and loaded which can be set in .bashrc</em> <em>## as e.g.</em> <em>source /opt/intel/bin/compilervars.sh intel64</em> <em>MKL_LIB_PATH=/opt/intel/mkl/lib/intel64## Use intel compiler</em> <em>CC=’icc -std=c99’</em> <em>CFLAGS=’-g -O3 -wd188 -ip ‘F77=’ifort’</em> <em>FFLAGS=’-g -O3 ‘CXX=’icpc’</em> <em>CXXFLAGS=’-g -O3 ‘FC=’ifort’</em> <em>FCFLAGS=’-g -O3 ‘## MKL With Intel MP threaded , ICC</em> <em>MKL=” -L${MKL_LIB_PATH} \</em> <em>-Wl,–start-group \</em> <em>-lmkl_intel_lp64 \</em> <em>-lmkl_intel_thread \</em> <em>-lmkl_core _ _-Wl,–end-group \</em> <em>-liomp5 -lpthread”</em></p>
<p>build R with following command, _  &gt; $./configure –prefix=$HOME/R-3.2.3-mkl-icc –with-readline=no –with-x=no –with-blas=”$MKL” –with-lapack CC=’icc -std=c99’ CFLAGS=’-g -O3 -wd188 -ip ‘ F77=’ifort’ FFLAGS=’-g -O3 ‘ CXX=’icpc’ CXXFLAGS=’-g -O3 ‘ FC=’ifort’ FCFLAGS=’-g -O3 ‘_ _  &gt; $make -j 4; make install_ (5) Set $HOME/R-3.2.3-mkl-icc environment</p>
<h4 id="R-with-CUDA-BLAS"><a href="#R-with-CUDA-BLAS" class="headerlink" title="R with CUDA BLAS"></a>R with CUDA BLAS</h4><p>(1) Install the driver and CUDA tools with version  up to 6.5 for NVIDIA Tesla Cards (2)Set the CUDA environment (3)Edit the nvblas.conf file</p>
<p><em># This is the configuration file to use NVBLAS Library</em> <em># Setup the environment variable NVBLAS_CONFIG_FILE to specify your own config file.</em> <em># By default, if NVBLAS_CONFIG_FILE is not defined,</em> <em># NVBLAS Library will try to open the file “nvblas.conf” in its current directory</em> <em># Example : NVBLAS_CONFIG_FILE /home/cuda_user/my_nvblas.conf</em> <em># The config file should have restricted write permissions accesses# Specify which output log file (default is stderr)</em> <em>NVBLAS_LOGFILE nvblas.log#Put here the CPU BLAS fallback Library of your choice_ <em>#It is strongly advised to use full path to describe the location of the CPU Library</em> _NVBLAS_CPU_BLAS_LIB /opt/R-3.2.3-ori/lib64/R/lib/libRblas.so</em> <em>#NVBLAS_CPU_BLAS_LIB &lt;mkl_path_installtion&gt;/libmkl_rt.so# List of GPU devices Id to participate to the computation</em> <em># Use ALL if you want all your GPUs to contribute</em> <em># Use ALL0, if you want all your GPUs of the same type as device 0 to contribute</em> <em># However, NVBLAS consider that all GPU have the same performance and PCI bandwidth</em> <em># By default if no GPU are listed, only device 0 will be used#NVBLAS_GPU_LIST 0 2 4</em> <em>#NVBLAS_GPU_LIST ALL</em> <em>NVBLAS_GPU_LIST ALL# Tile Dimension</em> <em>NVBLAS_TILE_DIM 2048# Autopin Memory</em> <em>NVBLAS_AUTOPIN_MEM_ENABLED#List of BLAS routines that are prevented from running on GPU (use for debugging purpose</em> <em># The current list of BLAS routines supported by NVBLAS are</em> <em># GEMM, SYRK, HERK, TRSM, TRMM, SYMM, HEMM, SYR2K, HER2K#NVBLAS_GPU_DISABLED_SGEMM</em> <em>#NVBLAS_GPU_DISABLED_DGEMM</em> <em>#NVBLAS_GPU_DISABLED_CGEMM</em> <em>#NVBLAS_GPU_DISABLED_ZGEMM# Computation can be optionally hybridized between CPU and GPU</em> <em># By default, GPU-supported BLAS routines are ran fully on GPU</em> <em># The option NVBLAS_CPU_RATIO</em>&lt;BLAS_ROUTINE&gt; give the ratio [0,1]_ <em># of the amount of computation that should be done on CPU</em> <em># CAUTION : this option should be used wisely because it can actually</em> <em># significantly reduced the overall performance if too much work is given to CPU#NVBLAS_CPU_RATIO_CGEMM 0.07</em></p>
<p>Set NVBLAS_CONFIG_FILE to the nvblas.conf location (4) Run the benchmark _  &gt; LD_PRELOAD=/opt/cuda-7.5/lib64/libnvblas.so R_</p>
<h4 id="R-with-MKL-on-Intel-Xeon-Phi"><a href="#R-with-MKL-on-Intel-Xeon-Phi" class="headerlink" title="R with MKL on Intel Xeon Phi"></a>R with MKL on Intel Xeon Phi</h4><p>(1) Build R with MKL Build R with MKL is same to Threaded MKL at 6 (2) Enable MKL  MIC Automatic Offload Mode _  &gt; export MKL_MIC_ENABLE=1_ _  &gt; export MIC_KMP_AFFINITY=compact_ Otherwise , you can set the workload division between host CPU and MIC card. If one host has two MIC cards, you could set: _  &gt; export MKL_HOST_WORKDIVISION=0.2_ _  &gt; export MKL_MIC_0_WORKDIVISION=0.4_ _  &gt; export MKL_MIC_1_WORKDIVISION=0.4_</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://jitmatrix.github.io/oneXPU/2016/04/15/r-hpac-benchmark-analysis/" data-id="ckivbjgr0006a6lolajk56ysl" data-title="R benchmark for High-Performance Analytics and Computing (I):Accelerators" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GEMM/" rel="tag">GEMM</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HPAC/" rel="tag">HPAC</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HPC/" rel="tag">HPC</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MIC/" rel="tag">MIC</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MKL/" rel="tag">MKL</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/R/" rel="tag">R</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Xeon/" rel="tag">Xeon</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Xeon-Phi/" rel="tag">Xeon Phi</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/benchmark/" rel="tag">benchmark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cuBLAS/" rel="tag">cuBLAS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/data-analytics/" rel="tag">data analytics</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/mutlithreading/" rel="tag">mutlithreading</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/performance-optimization/" rel="tag">performance optimization</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/rstats/" rel="tag">rstats</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-r-dnn-parallel-acceleration" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2016/03/08/r-dnn-parallel-acceleration/" class="article-date">
  <time class="dt-published" datetime="2016-03-08T14:18:34.000Z" itemprop="datePublished">2016-03-08</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/GPGPU/">GPGPU</a>►<a class="article-category-link" href="/categories/GPGPU/MultiCores/">MultiCores</a>►<a class="article-category-link" href="/categories/GPGPU/MultiCores/Performance-Optimizaiton/">Performance Optimizaiton</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2016/03/08/r-dnn-parallel-acceleration/">R for Deep Learning (II): Achieve High-Performance DNN with Parallel Acceleration</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <hr>
<p><em>I would like to thank  <a target="_blank" rel="noopener" href="http://junma5.weebly.com/">Jun Ma</a>  and all other technical reviewers and readers for their informative comments and suggestions in this post.</em></p>
<hr>
<blockquote>
<p><em><strong><a href="courses.csail.mit.edu/18.337/2015/docs/50YearsDataScience.pdf">50 years of Data Science</a></strong>, David Donoho, 2015</em></p>
<p><em>**Computing with Data.**Every data scientist should know and use several languages for data analysis and data processing. These can include popular languages like R and Python …</em> <em>Beyond basic knowledge of languages, data scientists need to keep current on new idioms for efficiently using those languages and need to understand the deeper issues associated with computational efficiency.</em></p>
</blockquote>
<p>  In the previous post, <a target="_blank" rel="noopener" href="http://www.parallelr.com/r-deep-neural-network-from-scratch/">R for deep learning (I)</a>, I introduced the core components of neural networks and illustrated how to implement it from scratch by R. Now, I will focus on computational performance and efficiency of R’s implementation, especially for the parallel algorithm on multicores CPU and <a target="_blank" rel="noopener" href="http://www.nvidia.com/object/tesla-supercomputing-solutions.html">NVIDIA GPU</a> architectures.  </p>
<h2 id="Performance-Profiling"><a href="#Performance-Profiling" class="headerlink" title="Performance Profiling"></a>Performance Profiling</h2><p>In this post, we are going to a little big dataset, <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a>, for performance analysis. <a target="_blank" rel="noopener" href="http://yann.lecun.com/exdb/mnist/">MNIST</a> widely used to measure the accuracy of classification by handwritten digits in machine learning field, and also used for the competition in <a target="_blank" rel="noopener" href="https://www.kaggle.com/c/digit-recognizer">Kaggle</a> (data in download page or <a target="_blank" rel="noopener" href="http://www.parallelr.com/materials/3_ParDNN/">here</a>). <a target="_blank" rel="noopener" href="http://yann.lecun.com/">Yann</a> has provided the classification results based on various machine learning algorithms on his <a target="_blank" rel="noopener" href="http://yann.lecun.com/exdb/mnist/">page</a>. <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/02/mnist.jpg"><img src="http://www.parallelr.com/wp-content/uploads/2016/02/mnist.jpg" alt="mnist"></a></p>
<p>Picture.1 Handwritten Digits in MNIST dataset</p>
<p>The MNIST database contains 60,000 training images and 10,000 testing images. Each of image is represented by 28*28 points so totally  784 points. In this post, I will train the neural network with 784 points as input features and the number of 0-9 as output classes,  then compare the runtime of our R DNN code with <a target="_blank" rel="noopener" href="http://www.h2o.ai/verticals/algos/deep-learning/">H2O deep learning</a> implementations for 2-layers networks of the various number of hidden units (HU).</p>
<p># h2o<br><a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/library">library</a>(h2o)<br># single thread<br>h2o.init()<br> <br> <br>train_file &lt;- “<a target="_blank" rel="noopener" href="https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/train.csv.gz&quot;">https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/train.csv.gz&quot;</a><br>test_file &lt;- “<a target="_blank" rel="noopener" href="https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/test.csv.gz&quot;">https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/test.csv.gz&quot;</a><br> <br>train &lt;- h2o.importFile(train_file)<br>test  &lt;- h2o.importFile(test_file)<br> <br># To see a brief summary of the data, run the following command<br><a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/summary">summary</a>(train)<br><a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/summary">summary</a>(test)<br> <br>y &lt;- “C785”<br>x &lt;- <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/setdiff">setdiff</a>(<a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/names">names</a>(train), y)<br> <br># We encode the response column as categorical for multinomial<br>#classification<br>train[,y] &lt;- <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/as.factor">as.factor</a>(train[,y])<br>test[,y]  &lt;- <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/as.factor">as.factor</a>(test[,y])<br> <br># Train a Deep Learning model and valid<br><a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/system.time">system.time</a>(<br>  model_cv &lt;- h2o.deeplearning(x = x,<br>                               y = y,<br>                               training_frame = train,<br>                               distribution = “multinomial”,<br>                               activation = “Rectifier”,<br>                               hidden = <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/c">c</a>(32),<br>                               l1 = 1e-5,<br>                               epochs = 200)<br>)</p>
<p>As I know, H2O is  the fast and most popular deep learning package in R platform implemented by Java in the backend. Thus, it will be valuable to know how much performance gap between native R code and  the mature package. As below barplot shown, the hidden units of 32, 64 and 128 are tested in 200 steps. <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/02/Runtime.RDNN_.H2O.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/02/Runtime.RDNN_.H2O.png" alt="Runtime.RDNN.H2O"></a> Obviously,  the R DNN is significantly slow than H2O, and the runtime increases with the number of hidden units quickly. For more details, we break down the R DNN runtime into each function call  by Rprof() and summaryRprof() which report out the final results including 4 parts: <em>total.time</em>, <em>total.pct</em>, <em>self.time</em> and <em>self.pct</em>. The <em>self.time</em> and <em>self.pct</em> columns represent the elapsed time for each function, excluding the time from its inner called functions. The <em>total.time</em> and <em>total.pct</em> columns mean the total elapsed time for each function including the time spent on function calls [<a target="_blank" rel="noopener" href="https://www.packtpub.com/application-development/r-high-performance-programming">Aloysius Lim</a>]. From the profiling results, we can see the top 1 time-consuming function is *<em>“%</em>%”** which represents matrix multiplications and usually people call it <strong>GEMM</strong> (GEneral Matrix Multiplication).  </p>
<p>&gt; <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/utils/Rprof">Rprof</a>()<br>&gt; mnist.model &lt;- train.dnn(x=1:784, y=785, traindata=train, hidden=64, maxit=200, display=50)<br>&gt; <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/utils/Rprof">Rprof</a>(NULL)<br>&gt; <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/utils/summaryRprof">summaryRprof</a>()<br>$by.self<br>                   self.time self.pct total.time total.pct<br>“%<em>%”                1250.08    90.19    1250.08     90.19<br>“t.default”            61.62     4.45      61.62      4.45<br>“pmax”                 24.40     1.76      28.42      2.05<br>“aperm.default”        11.60     0.84      11.60      0.84<br>“array”                10.36     0.75      10.36      0.75<br>“train.dnn”             9.74     0.70    1386.00    100.00<br>“&lt;=”                    5.72     0.41       5.72      0.41<br>“mostattributes&lt;-“      4.02     0.29       4.02      0.29<br>“exp”                   3.60     0.26       3.60      0.26<br>“sweep”                 1.58     0.11     676.32     48.80<br>“is.data.frame”         1.28     0.09       1.28      0.09<br>“colSums”               0.86     0.06       0.86      0.06<br>“/“                     0.52     0.04       0.52      0.04<br>“rowSums”               0.36     0.03       0.36      0.03<br>“unname”                0.18     0.01       1.46      0.11<br>“-“                     0.04     0.00       0.04      0.00<br>“t”                     0.02     0.00      61.64      4.45<br>“sum”                   0.02     0.00       0.02      0.00<br> <br>$by.total<br>                   total.time total.pct self.time self.pct<br>“train.dnn”           1386.00    100.00      9.74     0.70<br>“%</em>%”                 1250.08     90.19   1250.08    90.19<br>“sweep”                676.32     48.80      1.58     0.11<br>“t”                     61.64      4.45      0.02     0.00<br>“t.default”             61.62      4.45     61.62     4.45<br>“pmax”                  28.42      2.05     24.40     1.76<br>“aperm”                 21.96      1.58      0.00     0.00<br>“aperm.default”         11.60      0.84     11.60     0.84<br>“array”                 10.36      0.75     10.36     0.75<br>“&lt;=”                     5.72      0.41      5.72     0.41<br>“mostattributes&lt;-“       4.02      0.29      4.02     0.29<br>“exp”                    3.60      0.26      3.60     0.26<br>“unname”                 1.46      0.11      0.18     0.01<br>“is.data.frame”          1.28      0.09      1.28     0.09<br>“data.matrix”            1.28      0.09      0.00     0.00<br>“colSums”                0.86      0.06      0.86     0.06<br>“/“                      0.52      0.04      0.52     0.04</p>
<h2 id="Parallel-Acceleration"><a href="#Parallel-Acceleration" class="headerlink" title="Parallel Acceleration"></a>Parallel Acceleration</h2><p>From above analysis,  the matrix multiplication (“%*%”) accounts for about 90% computation time in the training stage of the neural network. Thus, the key of DNN acceleration is to speed up matrix multiplication.  Fortunately, there are already several parallel libraries for matrix multiplication and we can deploy it to R easily.  In this post, I will introduce three basic linear algebra subprograms (<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms">BLAS</a>) libraries, <a target="_blank" rel="noopener" href="http://www.openblas.net/">openBLAS</a>, <a target="_blank" rel="noopener" href="https://software.intel.com/en-us/intel-mkl">Intel MKL</a>, and <a target="_blank" rel="noopener" href="https://developer.nvidia.com/cublas">cuBLAS</a>. (In another <a target="_blank" rel="noopener" href="http://www.parallelr.com/r-hpac-benchmark-analysis/">blog</a>, moreover, we show their performance on modern hardware architectures and instructions of installation). The former two are multithread accelerated libraries which need to be built with R together (instructions in <a target="_blank" rel="noopener" href="http://www.r-bloggers.com/compile-r-and-openblas-from-source-guide/">here</a> and <a target="_blank" rel="noopener" href="https://software.intel.com/en-us/articles/using-intel-mkl-with-r">here</a>). On the other hand, it is indeed easy to apply NVIDIA cuBLAS library in R on Linux system by  the drop-on wrap, <a target="_blank" rel="noopener" href="http://docs.nvidia.com/cuda/nvblas/">nvBLAS</a>, which can be preloaded into R in order to hijack original Rblas.so. By this way, we can leverage NVIDIA GPU’s power into R with zero programming efforts :) The below picture shows the architecture of R with BLAS libraries. Typically, R will call their own BLAS, Rblas.so,on Linux system which handles all kinds of linear algebra functions (<a target="_blank" rel="noopener" href="http://www.netlib.org/blas/#_blas_routines">here</a>), but it is a single thread implementation. The trick to speed up the linear algebra calculations is to update the R standard BLAS to modern multithread libraries. For R developers, there is almost a ‘free lunch’  without rewriting their codes for parallel accelerations. <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/02/R_BLAS.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/02/R_BLAS-300x144.png" alt="R_BLAS"></a> As below code shown, we tested prebuilt R  with openBLAS,  Intel MKL and  nvBLAS  for 2-layers neural network of 32, 64 and 128 neurons.From the below bar chart, the runtime of R DNN are dramatically decreased and it is nearly <strong>2X</strong> faster than H2O and <strong>9X</strong> speedup (from 2816 to 365 for 128 hidden neurons network) than original R code!</p>
<blockquote>
<p># you must create a configuration file nvBLAS.conf in the current directory, example in <a target="_blank" rel="noopener" href="http://docs.nvidia.com/cuda/nvblas/#configuration_example">here</a>. &gt;LD_PRELOAD=libnvblas.so /home/patricz/tools/R-3.2.0/bin/bin/R CMD <a target="_blank" rel="noopener" href="http://inside-r.org/packages/cran/batch">BATCH</a> MNIST_DNN.R</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/02/rdnn_gemm.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/02/rdnn_gemm.png" alt="rdnn_mnist_blas_gemm"></a></p>
<blockquote>
<p>Note: Testing hardware: Ivy Bridge E5-2690 v2 @ 3.00GHz, dual socket 10-core (total 20 cores), 128G RAM;   NVIDIA GPU K40m;  Software:  CUDA 7.5,  OpenBLAS 0.2.8,  Intel MKL 11.1</p>
</blockquote>
<h2 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h2><p>Till now, it seems everything is great and we gain lots of performance increments from BLAS libraries under multicores CPU and NVIDIA GPU system. <strong>But can we do a little more for performance optimizations?</strong> Let’s look into the profiling again and probe the top performance limiters. The below table shows the breakdown of R DNN code under acceleration by nvBLAS where the GEMM performance is 10X faster ( from original 1250 to 114 seconds ).  But the next two top time-consuming functions, “sweep()” and “t()”, account for 27% (<em>self.pct</em>) runtime.Therefore, it makes sense to do further optimization for them.</p>
<blockquote>
<p><strong>A question for readers:</strong> <strong>The total time of ‘sweep’ is 87.28 but the self-time is only 1.8, so what are the real computation parts and is it a reasonable choose to optimize it?</strong></p>
</blockquote>
<p><a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/02/runtime.breakdown.nvblas.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/02/runtime.breakdown.nvblas.png" alt="DNN.runtime.breakdown.nvblas"></a> From the source code, there are several function calls of t()  to transfer matrix before multiplication; however, R has already provided the inner function `crossprod` and `tcrossprod` for this kind of operations.</p>
<pre><code>\# original:  t() with matrix multiplication
dW2     &lt;- [t](http://inside-r.org/r-doc/base/t)(hidden.layer) %*% dscores 
dhidden &lt;- dscores %*% [t](http://inside-r.org/r-doc/base/t)(W2)</code></pre>
<p> <br>   # Opt1: use builtin function<br>   dW2     &lt;- <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/crossprod">crossprod</a>(hidden.layer, dscores)<br>   dhidden &lt;- <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/tcrossprod">tcrossprod</a>(dscores, W2)</p>
<p>Secondly, the `sweep()` is performed to add the matrix with bias. Alternatively, we can combine weight and bias together and then use matrix multiplications, as below code shown. But the backside is that we have to create the new matrix for combinations of matrix and bias which increases memory pressure.</p>
<p> # Opt2: combine data and add 1 column for bias<br> #  extra matrix for combinations<br> X1   &lt;- <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/cbind">cbind</a>(X, <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/rep">rep</a>(1, <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/nrow">nrow</a>(X)))<br> W1b1 &lt;- <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/rbind">rbind</a>(W1, b1)<br> W2b2 &lt;- <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/rbind">rbind</a>(W2, b2)<br> <br> <br> # Opt2: remove `sweep`<br> #hidden.layer &lt;- sweep(X %<em>% W1 ,2, b1, ‘+’)<br> hidden.layer &lt;- X1 %</em>% W1b1<br> <br> #score &lt;- sweep(hidden.layer %<em>% W2, 2, b2, ‘+’)<br> hidden.layer1 &lt;- <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/cbind">cbind</a>(hidden.layer, <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/rep">rep</a>(1,<a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/nrow">nrow</a>(hidden.layer)))<br> score &lt;- hidden.layer1 %</em>% W2b2</p>
<p><a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/03/opt2.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/03/opt2.png" alt="bias optimization"></a> Now, we profile and compare the results again with below table. We save the computation time of ‘t.default’ and ‘aperm.default’ after removing ‘t()’ and ‘sweep()’ functions. Totally, the performance is <strong>DOUBLE</strong> again! <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/02/DNN.Optimization.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/02/DNN.Optimization.png" alt="DNN.Optimization"></a></p>
<blockquote>
<p><strong>Another question:</strong> <strong>What is your opinion about the next step of optimizations? Any good idea and tell me your numbers  :)</strong></p>
</blockquote>
<p> </p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>In this post, we have introduced the coarse granularity parallelism skill to accelerate R code by BLAS libraries on multicores CPU and NVIDIA GPU architectures. Till now, we have got <strong>+10X</strong> speedup under NVIDIA GPU and <strong>2X</strong> faster than 20-threads H2O packages for a relatively small network; however, there are still lots of space to improve, take a try. <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/02/RDNN_nvBLAS_runtime.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/02/RDNN_nvBLAS_runtime.png" alt="RDNN_nvBLAS_runtime"></a> And finally we train MNIST dataset with a 2 layer neural network of 300 hidden unit on Tesla K40m GPU, and we reach 94.7% accuracy (5.3% error rate)  in an hour while <a target="_blank" rel="noopener" href="http://yann.lecun.com/exdb/mnist/">Yann</a> got 4.7% error rate with smaller learning rate ( lr=0.001)  which will cost longer training time but more accurate.   <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/03/MNIST.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/03/MNIST.png" alt="neural network training results"></a> <strong>In the next <a target="_blank" rel="noopener" href="http://www.parallelr.com/r-dnn-cuda-multigpu/">blog post</a>, I will continue deep learning topic and focus on CUDA integration and multiple GPUs accelerations.</strong></p>
<hr>
<p>Notes: 1. The entire source code of this post in <a target="_blank" rel="noopener" href="https://github.com/PatricZhao/ParallelR">here</a> 2. The PDF version of this post in <a target="_blank" rel="noopener" href="http://www.parallelr.com/materials/3_ParDNN/3_.DNN_Parallel_Acceleration.pdf">here</a> 3. Pretty R syntax in this blog is <a target="_blank" rel="noopener" href="http://www.inside-r.org/pretty-r" title="Created by Pretty R at inside-R.org">Created by inside-R .org</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://jitmatrix.github.io/oneXPU/2016/03/08/r-dnn-parallel-acceleration/" data-id="ckivbjgof000l6lol2q6cbqv9" data-title="R for Deep Learning (II): Achieve High-Performance DNN with Parallel Acceleration" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GPU/" rel="tag">GPU</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/H2O/" rel="tag">H2O</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MKL/" rel="tag">MKL</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MNIST/" rel="tag">MNIST</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/R/" rel="tag">R</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/classification/" rel="tag">classification</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cuBLAS/" rel="tag">cuBLAS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deep-learning/" rel="tag">deep learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/dnn/" rel="tag">dnn</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/high-performance/" rel="tag">high performance</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/multicores/" rel="tag">multicores</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/multithreads/" rel="tag">multithreads</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/openblas/" rel="tag">openblas</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/parallel-computing/" rel="tag">parallel computing</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/performance-optimization/" rel="tag">performance optimization</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/profiling/" rel="tag">profiling</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/rstats/" rel="tag">rstats</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-test" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2016/02/19/test/" class="article-date">
  <time class="dt-published" datetime="2016-02-19T05:07:04.000Z" itemprop="datePublished">2016-02-19</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Uncategorized/">Uncategorized</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2016/02/19/test/">test</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p><strong>Recurrent-Neural-Network(RNN) gives state-of-the-art results in automatic speech recognition, stock market prediction, handwriting recognition and other sequence learning problems. In RNN layer, weight matrix is shared among all timesteps, but general cuda-based RNN implementations don’t utilize this characteristic and weight matrix has to be loaded every recurrent step. Especially in inference stage, when batch size is small, the performance of RNN is limited by memory bandwidth. In this talk, I will introduce a persistent thread method to accelerate RNN on GPU. In this method, whole weight matrix is stored into GPU registers to avoid repeated memory loads, and a CTA spinlock is implemented to achieve global synchronization. The result shows significant speedup on Maxwell architecture.</strong> Most researchers uses cuBLAS to implement RNN on GPU. But unlike DNN, data dependent behavior of RNN makes it hard to achieve peak utilization, which make RNN become major bottleneck in practical applications. The basic equations representing one forward stage of a RNN layer from timestep t-1 to timestep t is: h_t = g(W_r * h_t-1 + b_t) (1) b_t is the result at time t in pervious layer; h_t-1 is the result at timestep t-1 in RNN layer; weight matrix W_r is shared among all timesteps in RNN layer. In RNN layer, the result of h_t depends on the result of h_t-1. If total timesteps size is N, the general RNN implementations need call GEMV/GEMM(call GEMV, if batch size is 1; or call GEMM, if batch size &gt;1) N times. General implementations doesn’t utilize the characteristic that weight matrix W_r is shared among all timesteps, so weight matrix has to be loaded N times(usually N &gt; 100). When batch size is small, GEMM performance memory is limited by memory bandwidth. If we can keep weight matrix into SM registers, we can reduce hundreds of times weight matrix loads and change performance limitation from memory bandwidth to compute throughput. In this talk, I will present a persistent thread method to speedup RNN layer. My implementation can keep 1152*1152*4B=5.0625MB size weight matrix into registers, which takes 84.4% of total register size of GeForce GTX TITAN X(6MB). A CTA spinlock is used to achieve global synchronization. To avoid deadlock, all warps must be active. So occupancy is limited to 8 warps per SM, and each thread can use up to 256 registers (Finally, each thread uses 253 registers, which 216 registers are used to store weights and 37 registers are used to control). The baseline implementation is executing 1152*1152 size cublasSgemv and 1152 size vectorAdd 256 times on GeForce GTX TITAN X. (1152*1152 size cublasSgemv can achieve 52% memory bandwidth). The result shows persistent thread method can achieve 15X speedup on GeForce GTX TITAN X. If using fp16, we can keep 1152*sqrt(2) weights. If GPU had more registers, we could support RNN with larger weight matrix. Future works: 1) make persistent thread method support batch size &gt;1 and 2) spread this method to GRU and general LSTM network.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://jitmatrix.github.io/oneXPU/2016/02/19/test/" data-id="ckivbjgom000u6lol9x24fvng" data-title="test" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Accelerators/">Accelerators</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Accelerators/GPGPU/">GPGPU</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Accelerators/GPGPU/MultiCores/">MultiCores</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Accelerators/GPGPU/Performance-Optimizaiton/">Performance Optimizaiton</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Accelerators/General/">General</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Accelerators/General/GPGPU/">GPGPU</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Accelerators/General/GPGPU/MultiCores/">MultiCores</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Accelerators/General/GPGPU/MultiCores/Performance-Optimizaiton/">Performance Optimizaiton</a></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Accelerators/General/MPI/">MPI</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Accelerators/General/MPI/MultiCores/">MultiCores</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Accelerators/General/MPI/MultiCores/Performance-Optimizaiton/">Performance Optimizaiton</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Accelerators/General/MPI/MultiCores/Performance-Optimizaiton/Vectorization/">Vectorization</a></li></ul></li></ul></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Accelerators/MultiCores/">MultiCores</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Accelerators/MultiCores/Performance-Optimizaiton/">Performance Optimizaiton</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Accelerators/MultiCores/Vectorization/">Vectorization</a></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/GPGPU/">GPGPU</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/GPGPU/Intel-Xeon-Phi/">Intel Xeon Phi</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GPGPU/MultiCores/">MultiCores</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/GPGPU/MultiCores/Performance-Optimizaiton/">Performance Optimizaiton</a></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/General/">General</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/General/MultiCores/">MultiCores</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Uncategorized/">Uncategorized</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/BLAS/" rel="tag">BLAS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CRAN/" rel="tag">CRAN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CUDA/" rel="tag">CUDA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GEMM/" rel="tag">GEMM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GPU/" rel="tag">GPU</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/H2O/" rel="tag">H2O</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HPAC/" rel="tag">HPAC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HPC/" rel="tag">HPC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MIC/" rel="tag">MIC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MKL/" rel="tag">MKL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MNIST/" rel="tag">MNIST</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Maximum-Likelihood/" rel="tag">Maximum Likelihood</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/R/" rel="tag">R</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Rcpp/" rel="tag">Rcpp</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SSE/" rel="tag">SSE</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Xeon/" rel="tag">Xeon</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Xeon-Phi/" rel="tag">Xeon Phi</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/benchmark/" rel="tag">benchmark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/big-data/" rel="tag">big data</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/boost/" rel="tag">boost</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/classification/" rel="tag">classification</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cuBLAS/" rel="tag">cuBLAS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/data-analytics/" rel="tag">data analytics</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/deep-learning/" rel="tag">deep learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/dnn/" rel="tag">dnn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/doMC/" rel="tag">doMC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/doParallel/" rel="tag">doParallel</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/foreach/" rel="tag">foreach</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/gmatrix/" rel="tag">gmatrix</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/gpuR/" rel="tag">gpuR</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/gputools/" rel="tag">gputools</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/high-performance/" rel="tag">high performance</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/iris/" rel="tag">iris</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/knn/" rel="tag">knn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/lightboost/" rel="tag">lightboost</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/machine-learing/" rel="tag">machine learing</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/machine-learning/" rel="tag">machine learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mcapply/" rel="tag">mcapply</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/memory-usage/" rel="tag">memory usage</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/multicores/" rel="tag">multicores</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/multit/" rel="tag">multit</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/multithreads/" rel="tag">multithreads</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mutlithreading/" rel="tag">mutlithreading</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mutltiGPU/" rel="tag">mutltiGPU</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/neural-network/" rel="tag">neural network</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nvblas/" rel="tag">nvblas</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/openMP/" rel="tag">openMP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/openblas/" rel="tag">openblas</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/parallel/" rel="tag">parallel</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/parallel-computing/" rel="tag">parallel computing</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/parlapply/" rel="tag">parlapply</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/performance-optimization/" rel="tag">performance optimization</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/profiling/" rel="tag">profiling</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/rstats/" rel="tag">rstats</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sklearn/" rel="tag">sklearn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/snow/" rel="tag">snow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/xgboost/" rel="tag">xgboost</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/BLAS/" style="font-size: 10px;">BLAS</a> <a href="/tags/CRAN/" style="font-size: 10px;">CRAN</a> <a href="/tags/CUDA/" style="font-size: 12px;">CUDA</a> <a href="/tags/GEMM/" style="font-size: 12px;">GEMM</a> <a href="/tags/GPU/" style="font-size: 14px;">GPU</a> <a href="/tags/H2O/" style="font-size: 12px;">H2O</a> <a href="/tags/HPAC/" style="font-size: 10px;">HPAC</a> <a href="/tags/HPC/" style="font-size: 12px;">HPC</a> <a href="/tags/MIC/" style="font-size: 10px;">MIC</a> <a href="/tags/MKL/" style="font-size: 14px;">MKL</a> <a href="/tags/MNIST/" style="font-size: 10px;">MNIST</a> <a href="/tags/Maximum-Likelihood/" style="font-size: 10px;">Maximum Likelihood</a> <a href="/tags/R/" style="font-size: 20px;">R</a> <a href="/tags/Rcpp/" style="font-size: 10px;">Rcpp</a> <a href="/tags/SSE/" style="font-size: 10px;">SSE</a> <a href="/tags/Xeon/" style="font-size: 10px;">Xeon</a> <a href="/tags/Xeon-Phi/" style="font-size: 10px;">Xeon Phi</a> <a href="/tags/benchmark/" style="font-size: 10px;">benchmark</a> <a href="/tags/big-data/" style="font-size: 10px;">big data</a> <a href="/tags/boost/" style="font-size: 10px;">boost</a> <a href="/tags/classification/" style="font-size: 10px;">classification</a> <a href="/tags/cuBLAS/" style="font-size: 16px;">cuBLAS</a> <a href="/tags/data-analytics/" style="font-size: 12px;">data analytics</a> <a href="/tags/deep-learning/" style="font-size: 14px;">deep learning</a> <a href="/tags/dnn/" style="font-size: 12px;">dnn</a> <a href="/tags/doMC/" style="font-size: 10px;">doMC</a> <a href="/tags/doParallel/" style="font-size: 10px;">doParallel</a> <a href="/tags/foreach/" style="font-size: 10px;">foreach</a> <a href="/tags/gmatrix/" style="font-size: 10px;">gmatrix</a> <a href="/tags/gpuR/" style="font-size: 10px;">gpuR</a> <a href="/tags/gputools/" style="font-size: 10px;">gputools</a> <a href="/tags/high-performance/" style="font-size: 10px;">high performance</a> <a href="/tags/iris/" style="font-size: 10px;">iris</a> <a href="/tags/knn/" style="font-size: 10px;">knn</a> <a href="/tags/lightboost/" style="font-size: 10px;">lightboost</a> <a href="/tags/machine-learing/" style="font-size: 10px;">machine learing</a> <a href="/tags/machine-learning/" style="font-size: 12px;">machine learning</a> <a href="/tags/mcapply/" style="font-size: 10px;">mcapply</a> <a href="/tags/memory-usage/" style="font-size: 10px;">memory usage</a> <a href="/tags/multicores/" style="font-size: 18px;">multicores</a> <a href="/tags/multit/" style="font-size: 10px;">multit</a> <a href="/tags/multithreads/" style="font-size: 10px;">multithreads</a> <a href="/tags/mutlithreading/" style="font-size: 10px;">mutlithreading</a> <a href="/tags/mutltiGPU/" style="font-size: 10px;">mutltiGPU</a> <a href="/tags/neural-network/" style="font-size: 10px;">neural network</a> <a href="/tags/nvblas/" style="font-size: 10px;">nvblas</a> <a href="/tags/openMP/" style="font-size: 12px;">openMP</a> <a href="/tags/openblas/" style="font-size: 12px;">openblas</a> <a href="/tags/parallel/" style="font-size: 10px;">parallel</a> <a href="/tags/parallel-computing/" style="font-size: 18px;">parallel computing</a> <a href="/tags/parlapply/" style="font-size: 10px;">parlapply</a> <a href="/tags/performance-optimization/" style="font-size: 16px;">performance optimization</a> <a href="/tags/profiling/" style="font-size: 12px;">profiling</a> <a href="/tags/python/" style="font-size: 10px;">python</a> <a href="/tags/rstats/" style="font-size: 20px;">rstats</a> <a href="/tags/sklearn/" style="font-size: 10px;">sklearn</a> <a href="/tags/snow/" style="font-size: 10px;">snow</a> <a href="/tags/xgboost/" style="font-size: 10px;">xgboost</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">August 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">July 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">May 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/04/">April 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/03/">March 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/04/07/r-hpac-benchmark-analysis-gpu/">R benchmark for High-Performance Analytics and Computing (II): GPU Packages</a>
          </li>
        
          <li>
            <a href="/2017/01/24/parallel-computation-with-r-and-xgboost/">Parallel Computation with R and XGBoost</a>
          </li>
        
          <li>
            <a href="/2016/09/10/r-with-parallel-computing/">R with Parallel Computing from User Perspectives</a>
          </li>
        
          <li>
            <a href="/2016/08/15/r-cran-package-modernization-openmp/">R and OpenMP:  CRAN Package Modernization</a>
          </li>
        
          <li>
            <a href="/2016/07/26/r-and-openmp-boosting-compiled-code-on-multi-core-cpu-s/">R and openMP: boosting compiled code on multi-core cpu-s</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2020 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>
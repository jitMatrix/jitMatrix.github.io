<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>R - GPU Programming for All with &#39;gpuR&#39; | oneXPU</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="IntroductionGPUs (Graphic Processing Units) have become much more popular in recent years for computationally intensive calculations.  Despite these gains, the use of this hardware has been very limit">
<meta property="og:type" content="article">
<meta property="og:title" content="R - GPU Programming for All with &#39;gpuR&#39;">
<meta property="og:url" content="https://jitmatrix.github.io/oneXPU/2016/05/04/r-gpu-programming-for-all-with-gpur/index.html">
<meta property="og:site_name" content="oneXPU">
<meta property="og:description" content="IntroductionGPUs (Graphic Processing Units) have become much more popular in recent years for computationally intensive calculations.  Despite these gains, the use of this hardware has been very limit">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://www.parallelr.com/wp-content/uploads/2016/05/dgemm-1024x588.png">
<meta property="og:image" content="http://www.parallelr.com/wp-content/uploads/2016/05/sgemm-1024x588.png">
<meta property="og:image" content="http://www.parallelr.com/wp-content/uploads/2016/05/sgemm_openblas-1024x528.png">
<meta property="article:published_time" content="2016-05-04T13:52:08.000Z">
<meta property="article:modified_time" content="2020-12-19T06:37:18.634Z">
<meta property="article:author" content="Patric Zhao">
<meta property="article:tag" content="R">
<meta property="article:tag" content="GPU">
<meta property="article:tag" content="openblas">
<meta property="article:tag" content="GEMM">
<meta property="article:tag" content="HPC">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://www.parallelr.com/wp-content/uploads/2016/05/dgemm-1024x588.png">
  
    <link rel="alternate" href="/oneXPU/atom.xml" title="oneXPU" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/oneXPU/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/oneXPU/css/style.css">

  
    
<link rel="stylesheet" href="/oneXPU/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/oneXPU/" id="logo">oneXPU</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/oneXPU/" id="subtitle">Diving into Parallel Technology</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/oneXPU/">Home</a>
        
          <a class="main-nav-link" href="/oneXPU/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/oneXPU/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://jitmatrix.github.io/oneXPU"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-r-gpu-programming-for-all-with-gpur" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/oneXPU/2016/05/04/r-gpu-programming-for-all-with-gpur/" class="article-date">
  <time class="dt-published" datetime="2016-05-04T13:52:08.000Z" itemprop="datePublished">2016-05-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/oneXPU/categories/GPGPU/">GPGPU</a>►<a class="article-category-link" href="/oneXPU/categories/GPGPU/MultiCores/">MultiCores</a>►<a class="article-category-link" href="/oneXPU/categories/GPGPU/MultiCores/Performance-Optimizaiton/">Performance Optimizaiton</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      R - GPU Programming for All with &#39;gpuR&#39;
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>GPUs (Graphic Processing Units) have become much more popular in recent years for computationally intensive calculations.  Despite these gains, the use of this hardware has been very limited in the R programming language.  Although possible, the prospect of programming in either OpenCL or CUDA is difficult for many programmers unaccustomed to working with such a low-level interface.  Creating bindings for R’s high-level programming that abstracts away the complex GPU code would make using GPUs far more accessible to R users.  This is the core idea behind the <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/gpuR/index.html">gpuR</a> package.  There are three novel aspects of <code>gpuR</code>:</p>
<ol>
<li> Applicable on ‘ALL’ GPUs</li>
<li> Abstracts away CUDA/OpenCL code to easily incorporate in to existing R algorithms</li>
<li> Separates copy/compute functions to allow objects to persist on GPU</li>
</ol>
<h3 id="Broad-application"><a href="#Broad-application" class="headerlink" title="Broad application:"></a><strong>Broad application</strong>:</h3><p>The ‘gpuR’ package was created to bring the power of GPU computing to any R user with a GPU device.  Although there are a handful of packages that provide some GPU capability (e.g. <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/gputools/index.html">gputools</a>, <a target="_blank" rel="noopener" href="http://cran.fhcrc.org/web/packages/cudaBayesreg/index.html">cudaBayesreg</a>, <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/HiPLARM/index.html">HiPLARM</a>, <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/HiPLARb/index.html">HiPLARb</a>, and <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/gmatrix/index.html">gmatrix</a>) all are strictly limited to NVIDIA GPUs.  As such, a backend that is based upon OpenCL would allow all users to benefit from GPU hardware.  The ‘gpuR’ package therefore utilizes the <a target="_blank" rel="noopener" href="http://viennacl.sourceforge.net/">ViennaCL</a> linear algebra library which contains auto-tuned OpenCL kernels (among others) that can be leveraged for GPUs.  The headers have been conveniently repackaged in the <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/RViennaCL/index.html">RViennaCL </a>package.  It also allows for a CUDA backend for those with NVIDIA GPUs that may see further improved performance (contained within the companion <a target="_blank" rel="noopener" href="https://github.com/cdeterman/gpuRcuda">gpuRcuda</a> package not yet formally released).</p>
<h3 id="Abstract-away-GPU-code"><a href="#Abstract-away-GPU-code" class="headerlink" title="Abstract away GPU code:"></a><strong>Abstract away GPU code:</strong></h3><p>The <code>gpuR</code> package uses the S4 object oriented system to have explicit classes and methods that all the user to simply cast their <code>matrix</code> or <code>vector</code> and continue programming in R as normal.  For example: [code language=”r”] ORDER = 1024 A = matrix(rnorm(ORDER^2), nrow=ORDER) B = matrix(rnorm(ORDER^2), nrow=ORDER) gpuA = gpuMatrix(A, type=”double”) gpuB = gpuMatrix(B, type=”double”) C = A %<em>% B gpuC = gpuA %</em>% gpuB all.equal(C == gpuC[]) [1] TRUE [/code] The <code>gpuMatrix</code> object points to a matrix in RAM which is then computed by the GPU when a desired function is called.  This avoids R’s habit of copying the memory of objects.  For example: [code language=”r”] library(pryr) # Initially points to same object x = matrix(rnorm(16), 4) y = x address(x) [1] “0x16177f28” address(y) [1] “0x16177f28” # But once modify the second object it creates a copy y[1,1] = 0 address(x) [1] “0x16177f28” address(y) [1] “0x15fbb1d8 [/code] In contrast, the same syntax for a <code>gpuMatrix</code> will modify the original object in-place without any copy. [code language=”r”] library(pryr) library(gpuR) # Initially points to same object x = gpuMatrix(rnorm(16), 4, 4) y = x x@address [1] &lt;pointer: 0x6baa040&gt; y@address [1] &lt;pointer: 0x6baa040&gt; # Modification affects both objects without copy y[1,1] = 0 x@address [1] &lt;pointer: 0x6baa040&gt; y@address [1] &lt;pointer: 0x6baa040&gt; [/code]   Each new variable assigned to this object will only copy the pointer thereby making the program more memory efficient.  However, the <code>gpuMatrix&gt;</code> class does still require allocating GPU memory and copying data to device for each function call. The most commonly used methods have been overloaded such as  %*%, +, -, *, /, crossprod, tcrossprod, and trig functions among others.  In this way, an R user can create these objects and leverage GPU resources without the need to know a bunch more functions that would break existing algorithms.</p>
<h3 id="Distinct-Copy-Compute-Functionality"><a href="#Distinct-Copy-Compute-Functionality" class="headerlink" title="Distinct Copy/Compute Functionality:"></a><strong>Distinct Copy/Compute Functionality:</strong></h3><p>For the <code>gpuMatix</code> and <code>gpuVector</code> classes there are companion <code>vclMatrix</code> and <code>vclVector</code> class that point to objects that persist in the GPU RAM.  In this way, the user explicitly decides when data needs to be moved back to the host.  By avoiding unnecessary data transfer between host and device performance can significantly improve.  For example: [code language=”r”] vclA = vclMatrix(rnorm(10000), nrow = 100) vclB = vclMatrix(rnorm(10000), nrow = 100) vclC = vclMatrix(rnorm(10000), nrow = 100) # GEMM vclD = vclA %*% vclB # Element-wise addition vclD = vclD + vclC [/code] In this code, the three initial matrices already exist in the GPU memory so no data transfer takes place in the GEMM call.  Furthermore, the returned matrix remains in the GPU memory.  In this case, the ‘vclD’ object is still in GPU RAM. As such, the element-wise addition call also happens directly on the GPU with no data transfers. It is worth also noting that the user can still modify elements, rows, or columns with the exact same syntax as a normal R matrix. [code language=”r”] vclD[1,1] = 42 vclD[,2] = rep(12, 100) vclD[3,] = rep(23, 100) [/code] These operations simply copy the <em>new</em> elements to the GPU and modify the object in-place within the GPU memory. The ‘vclD’ object is never copied to the host.</p>
<h3 id="Benchmarks"><a href="#Benchmarks" class="headerlink" title="Benchmarks:"></a>Benchmarks:</h3><p>With all that in mind, how does gpuR perform?  Here are some general benchmarks of the popular GEMM operation.  I currently only have access to a single NVIDIA GeForce GTX 970 for these simulations.  Users should expect to see differences with high performance GPUs (e.g. AMD FirePro, NVIDIA Tesla, etc.). Speedup relative to CPU will also vary depending upon user hardware.</p>
<h4 id="1-Default-dGEMM-vs-Base-R"><a href="#1-Default-dGEMM-vs-Base-R" class="headerlink" title="(1) Default dGEMM vs Base R"></a>(1) Default dGEMM vs Base R</h4><p>R is known to only support two numeric types (integer and double).  As such, Figure 1 shows the fold speedup achieved by using the <code>gpuMatrix</code> and <code>vclMatrix</code> classes.  Since R is already known to not be the fastest language, an implementation with the OpenBLAS backend is included as well for reference using a 4 core Intel i5-2500 CPU @ 3.30GHz.  As can be seen there is a dramatic speedup from just using OpenBLAS or the gpuMatrix class (essentially equivalent).  Of interest is the impact of the transfer time from host-device-host that is typical in many GPU implementations.  This cost is eliminated by using the <code>vclMatrix</code> class which continues to scale with matrix size. [caption id=”attachment_768” align=”aligncenter” width=”640”]<img src="http://www.parallelr.com/wp-content/uploads/2016/05/dgemm-1024x588.png" alt="dgemm"> Figure 1 - Fold speedup achieved using openblas (CPU) as well as the gpuMatrix/vclMatrix (GPU) classes provided in gpuR.[/caption]  </p>
<h4 id="2-sGEMM-vs-Base-R"><a href="#2-sGEMM-vs-Base-R" class="headerlink" title="(2) sGEMM vs Base R"></a>(2) sGEMM vs Base R</h4><p>In many GPU benchmarks there is often float operations measured as well.  As noted above, R does not provide this by default.  One way to go around this is to use the <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/RcppArmadillo/index.html">RcppArmadillo</a> or RcppEigen packages and explicitly casting R objects as float types.  The armadillo library will also default to using the BLAS backend provided (i.e. OpenBLAS).  Float types are implemented <code>gpuR</code> by setting <code>type = &quot;float&quot;</code> in the matrix calls (e.g. <code>vclMatrix(mat, type = &quot;float&quot;)</code>) in Figure 2 shows the impact of using float data types.  OpenBLAS continues to provide a noticeable speedup but <code>gpuMatrix</code> begins to outperform once matrix order exceeds 1500.  The <code>vclMatrix</code> continues to demonstrate the value in retaining objects in GPU memory and avoiding memory transfers.   [caption id=”attachment_769” align=”aligncenter” width=”640”]<img src="http://www.parallelr.com/wp-content/uploads/2016/05/sgemm-1024x588.png" alt="sgemm"> Figure 2 - Float type GEMM comparisons. Fold speedup achieved using openblas (via RcppArmadillo) as well as the gpuMatrix/vclMatrix (GPU) classes provided in gpuR.[/caption]   To give an additional view on the performance achieved by <code>gpuMatrix</code> and <code>vclMatrix</code> is comparing directly against the OpenBLAS performance.  The <code>gpuMatrix</code> reaches ~2-3 fold speedup over OpenBLAS whereas <code>vclMatrix</code> scales to over 100 fold speedup!  It is curious as to why the performance with vcl<code>Matrix</code> is so much faster (only differing in host-device-host transfers).  Further optimization with <code>gpuMatrix</code> will need to be explored (fresh eyes are welcome) accepting limitations in the BUS transfer speed.  Performance will certainly improve with improved hardware capabilities such as NVIDIA’s NVLink. [caption id=”attachment_831” align=”aligncenter” width=”737”]<img src="http://www.parallelr.com/wp-content/uploads/2016/05/sgemm_openblas-1024x528.png" alt="sgemm_openblas"> Figure 3 - Fold speedup achieved over openblas (via RcppArmadillo) float type GEMM comparisons vs the gpuMatrix/vclMatrix (GPU) classes provided in gpuR.[/caption]</p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>The <code>gpuR</code> package has been created to bring GPU computing to as many R users as possible.  It is the intention to use <code>gpuR</code> to more easily supplement current and future algorithms that could benefit from GPU acceleration.  The <code>gpuR</code> package is currently available on <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/gpuR/index.html">CRAN</a>.  The development version can be found on my <a target="_blank" rel="noopener" href="https://github.com/cdeterman/gpuR">github</a> in addition to existing issues and wiki pages (assisting primarily in installation).  Future developments include solvers (e.g. QR, SVD, cholesky, etc.), scaling across multiple GPUs,  ‘sparse’ class objects, and custom OpenCL kernels. As noted above, this package is intended to be used with a multitude of hardware and operating systems (it has been tested on Windows, Mac, and multiple Linux flavors).  I only have access to a limited set of hardware (I can’t access every GPU, let along the most expensive).  As such, the development of <code>gpuR</code> depends upon the R user community.  Volunteers who possess different hardware are always welcomed and encouraged to submit issues regarding any discovered bugs.  I have begun a gitter account for users to report on successful usage with alternate hardware.  Suggestions and general conversation about gpuR is welcome.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://jitmatrix.github.io/oneXPU/2016/05/04/r-gpu-programming-for-all-with-gpur/" data-id="ckivbxbh2000qzroe94s60rki" data-title="R - GPU Programming for All with &#39;gpuR&#39;" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/GEMM/" rel="tag">GEMM</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/GPU/" rel="tag">GPU</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/HPC/" rel="tag">HPC</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/R/" rel="tag">R</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/openblas/" rel="tag">openblas</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/oneXPU/2016/05/09/r-dnn-cuda-multigpu/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          R for Deep Learning (III): CUDA and MultiGPUs Acceleration
        
      </div>
    </a>
  
  
    <a href="/oneXPU/2016/04/15/r-hpac-benchmark-analysis/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">R benchmark for High-Performance Analytics and Computing (I):Accelerators</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/">Accelerators</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/GPGPU/">GPGPU</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/GPGPU/MultiCores/">MultiCores</a></li><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/GPGPU/Performance-Optimizaiton/">Performance Optimizaiton</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/General/">General</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/General/GPGPU/">GPGPU</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/General/GPGPU/MultiCores/">MultiCores</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/General/GPGPU/MultiCores/Performance-Optimizaiton/">Performance Optimizaiton</a></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/General/MPI/">MPI</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/General/MPI/MultiCores/">MultiCores</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/General/MPI/MultiCores/Performance-Optimizaiton/">Performance Optimizaiton</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/General/MPI/MultiCores/Performance-Optimizaiton/Vectorization/">Vectorization</a></li></ul></li></ul></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/MultiCores/">MultiCores</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/MultiCores/Performance-Optimizaiton/">Performance Optimizaiton</a></li><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/MultiCores/Vectorization/">Vectorization</a></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/GPGPU/">GPGPU</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/GPGPU/Intel-Xeon-Phi/">Intel Xeon Phi</a></li><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/GPGPU/MultiCores/">MultiCores</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/GPGPU/MultiCores/Performance-Optimizaiton/">Performance Optimizaiton</a></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/General/">General</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/General/MultiCores/">MultiCores</a></li></ul></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/BLAS/" rel="tag">BLAS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/CRAN/" rel="tag">CRAN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/CUDA/" rel="tag">CUDA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/GEMM/" rel="tag">GEMM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/GPU/" rel="tag">GPU</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/H2O/" rel="tag">H2O</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/HPAC/" rel="tag">HPAC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/HPC/" rel="tag">HPC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/MIC/" rel="tag">MIC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/MKL/" rel="tag">MKL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/MNIST/" rel="tag">MNIST</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/Maximum-Likelihood/" rel="tag">Maximum Likelihood</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/R/" rel="tag">R</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/Rcpp/" rel="tag">Rcpp</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/SSE/" rel="tag">SSE</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/Xeon/" rel="tag">Xeon</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/Xeon-Phi/" rel="tag">Xeon Phi</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/benchmark/" rel="tag">benchmark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/big-data/" rel="tag">big data</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/boost/" rel="tag">boost</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/classification/" rel="tag">classification</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/cuBLAS/" rel="tag">cuBLAS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/data-analytics/" rel="tag">data analytics</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/deep-learning/" rel="tag">deep learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/dnn/" rel="tag">dnn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/doMC/" rel="tag">doMC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/doParallel/" rel="tag">doParallel</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/foreach/" rel="tag">foreach</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/gmatrix/" rel="tag">gmatrix</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/gpuR/" rel="tag">gpuR</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/gputools/" rel="tag">gputools</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/high-performance/" rel="tag">high performance</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/iris/" rel="tag">iris</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/knn/" rel="tag">knn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/lightboost/" rel="tag">lightboost</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/machine-learing/" rel="tag">machine learing</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/machine-learning/" rel="tag">machine learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/mcapply/" rel="tag">mcapply</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/memory-usage/" rel="tag">memory usage</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/multicores/" rel="tag">multicores</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/multit/" rel="tag">multit</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/multithreads/" rel="tag">multithreads</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/mutlithreading/" rel="tag">mutlithreading</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/mutltiGPU/" rel="tag">mutltiGPU</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/neural-network/" rel="tag">neural network</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/nvblas/" rel="tag">nvblas</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/openMP/" rel="tag">openMP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/openblas/" rel="tag">openblas</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/parallel/" rel="tag">parallel</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/parallel-computing/" rel="tag">parallel computing</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/parlapply/" rel="tag">parlapply</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/performance-optimization/" rel="tag">performance optimization</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/profiling/" rel="tag">profiling</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/rstats/" rel="tag">rstats</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/sklearn/" rel="tag">sklearn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/snow/" rel="tag">snow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/xgboost/" rel="tag">xgboost</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/oneXPU/tags/BLAS/" style="font-size: 10px;">BLAS</a> <a href="/oneXPU/tags/CRAN/" style="font-size: 10px;">CRAN</a> <a href="/oneXPU/tags/CUDA/" style="font-size: 12px;">CUDA</a> <a href="/oneXPU/tags/GEMM/" style="font-size: 12px;">GEMM</a> <a href="/oneXPU/tags/GPU/" style="font-size: 14px;">GPU</a> <a href="/oneXPU/tags/H2O/" style="font-size: 12px;">H2O</a> <a href="/oneXPU/tags/HPAC/" style="font-size: 10px;">HPAC</a> <a href="/oneXPU/tags/HPC/" style="font-size: 12px;">HPC</a> <a href="/oneXPU/tags/MIC/" style="font-size: 10px;">MIC</a> <a href="/oneXPU/tags/MKL/" style="font-size: 14px;">MKL</a> <a href="/oneXPU/tags/MNIST/" style="font-size: 10px;">MNIST</a> <a href="/oneXPU/tags/Maximum-Likelihood/" style="font-size: 10px;">Maximum Likelihood</a> <a href="/oneXPU/tags/R/" style="font-size: 20px;">R</a> <a href="/oneXPU/tags/Rcpp/" style="font-size: 10px;">Rcpp</a> <a href="/oneXPU/tags/SSE/" style="font-size: 10px;">SSE</a> <a href="/oneXPU/tags/Xeon/" style="font-size: 10px;">Xeon</a> <a href="/oneXPU/tags/Xeon-Phi/" style="font-size: 10px;">Xeon Phi</a> <a href="/oneXPU/tags/benchmark/" style="font-size: 10px;">benchmark</a> <a href="/oneXPU/tags/big-data/" style="font-size: 10px;">big data</a> <a href="/oneXPU/tags/boost/" style="font-size: 10px;">boost</a> <a href="/oneXPU/tags/classification/" style="font-size: 10px;">classification</a> <a href="/oneXPU/tags/cuBLAS/" style="font-size: 16px;">cuBLAS</a> <a href="/oneXPU/tags/data-analytics/" style="font-size: 12px;">data analytics</a> <a href="/oneXPU/tags/deep-learning/" style="font-size: 14px;">deep learning</a> <a href="/oneXPU/tags/dnn/" style="font-size: 12px;">dnn</a> <a href="/oneXPU/tags/doMC/" style="font-size: 10px;">doMC</a> <a href="/oneXPU/tags/doParallel/" style="font-size: 10px;">doParallel</a> <a href="/oneXPU/tags/foreach/" style="font-size: 10px;">foreach</a> <a href="/oneXPU/tags/gmatrix/" style="font-size: 10px;">gmatrix</a> <a href="/oneXPU/tags/gpuR/" style="font-size: 10px;">gpuR</a> <a href="/oneXPU/tags/gputools/" style="font-size: 10px;">gputools</a> <a href="/oneXPU/tags/high-performance/" style="font-size: 10px;">high performance</a> <a href="/oneXPU/tags/iris/" style="font-size: 10px;">iris</a> <a href="/oneXPU/tags/knn/" style="font-size: 10px;">knn</a> <a href="/oneXPU/tags/lightboost/" style="font-size: 10px;">lightboost</a> <a href="/oneXPU/tags/machine-learing/" style="font-size: 10px;">machine learing</a> <a href="/oneXPU/tags/machine-learning/" style="font-size: 12px;">machine learning</a> <a href="/oneXPU/tags/mcapply/" style="font-size: 10px;">mcapply</a> <a href="/oneXPU/tags/memory-usage/" style="font-size: 10px;">memory usage</a> <a href="/oneXPU/tags/multicores/" style="font-size: 18px;">multicores</a> <a href="/oneXPU/tags/multit/" style="font-size: 10px;">multit</a> <a href="/oneXPU/tags/multithreads/" style="font-size: 10px;">multithreads</a> <a href="/oneXPU/tags/mutlithreading/" style="font-size: 10px;">mutlithreading</a> <a href="/oneXPU/tags/mutltiGPU/" style="font-size: 10px;">mutltiGPU</a> <a href="/oneXPU/tags/neural-network/" style="font-size: 10px;">neural network</a> <a href="/oneXPU/tags/nvblas/" style="font-size: 10px;">nvblas</a> <a href="/oneXPU/tags/openMP/" style="font-size: 12px;">openMP</a> <a href="/oneXPU/tags/openblas/" style="font-size: 12px;">openblas</a> <a href="/oneXPU/tags/parallel/" style="font-size: 10px;">parallel</a> <a href="/oneXPU/tags/parallel-computing/" style="font-size: 18px;">parallel computing</a> <a href="/oneXPU/tags/parlapply/" style="font-size: 10px;">parlapply</a> <a href="/oneXPU/tags/performance-optimization/" style="font-size: 16px;">performance optimization</a> <a href="/oneXPU/tags/profiling/" style="font-size: 12px;">profiling</a> <a href="/oneXPU/tags/python/" style="font-size: 10px;">python</a> <a href="/oneXPU/tags/rstats/" style="font-size: 20px;">rstats</a> <a href="/oneXPU/tags/sklearn/" style="font-size: 10px;">sklearn</a> <a href="/oneXPU/tags/snow/" style="font-size: 10px;">snow</a> <a href="/oneXPU/tags/xgboost/" style="font-size: 10px;">xgboost</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/oneXPU/archives/2017/04/">April 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/oneXPU/archives/2017/01/">January 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/oneXPU/archives/2016/09/">September 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/oneXPU/archives/2016/08/">August 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/oneXPU/archives/2016/07/">July 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/oneXPU/archives/2016/05/">May 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/oneXPU/archives/2016/04/">April 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/oneXPU/archives/2016/03/">March 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/oneXPU/archives/2016/02/">February 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/oneXPU/2017/04/07/r-hpac-benchmark-analysis-gpu/">R benchmark for High-Performance Analytics and Computing (II): GPU Packages</a>
          </li>
        
          <li>
            <a href="/oneXPU/2017/01/24/parallel-computation-with-r-and-xgboost/">Parallel Computation with R and XGBoost</a>
          </li>
        
          <li>
            <a href="/oneXPU/2016/09/10/r-with-parallel-computing/">R with Parallel Computing from User Perspectives</a>
          </li>
        
          <li>
            <a href="/oneXPU/2016/08/15/r-cran-package-modernization-openmp/">R and OpenMP:  CRAN Package Modernization</a>
          </li>
        
          <li>
            <a href="/oneXPU/2016/07/26/r-and-openmp-boosting-compiled-code-on-multi-core-cpu-s/">R and openMP: boosting compiled code on multi-core cpu-s</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2020 Patric Zhao<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/oneXPU/" class="mobile-nav-link">Home</a>
  
    <a href="/oneXPU/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/oneXPU/js/jquery-3.4.1.min.js"></script>



  
<script src="/oneXPU/fancybox/jquery.fancybox.min.js"></script>




<script src="/oneXPU/js/script.js"></script>





  </div>
</body>
</html>
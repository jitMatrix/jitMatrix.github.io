<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>R for Deep Learning (II): Achieve High-Performance DNN with Parallel Acceleration | oneXPU</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="I would like to thank  Jun Ma  and all other technical reviewers and readers for their informative comments and suggestions in this post.   50 years of Data Science, David Donoho, 2015 **Computing wi">
<meta property="og:type" content="article">
<meta property="og:title" content="R for Deep Learning (II): Achieve High-Performance DNN with Parallel Acceleration">
<meta property="og:url" content="https://jitmatrix.github.io/oneXPU/2016/03/08/r-dnn-parallel-acceleration/index.html">
<meta property="og:site_name" content="oneXPU">
<meta property="og:description" content="I would like to thank  Jun Ma  and all other technical reviewers and readers for their informative comments and suggestions in this post.   50 years of Data Science, David Donoho, 2015 **Computing wi">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://www.parallelr.com/wp-content/uploads/2016/02/mnist.jpg">
<meta property="og:image" content="http://www.parallelr.com/wp-content/uploads/2016/02/Runtime.RDNN_.H2O.png">
<meta property="og:image" content="http://www.parallelr.com/wp-content/uploads/2016/02/R_BLAS-300x144.png">
<meta property="og:image" content="http://www.parallelr.com/wp-content/uploads/2016/02/rdnn_gemm.png">
<meta property="og:image" content="http://www.parallelr.com/wp-content/uploads/2016/02/runtime.breakdown.nvblas.png">
<meta property="og:image" content="http://www.parallelr.com/wp-content/uploads/2016/03/opt2.png">
<meta property="og:image" content="http://www.parallelr.com/wp-content/uploads/2016/02/DNN.Optimization.png">
<meta property="og:image" content="http://www.parallelr.com/wp-content/uploads/2016/02/RDNN_nvBLAS_runtime.png">
<meta property="og:image" content="http://www.parallelr.com/wp-content/uploads/2016/03/MNIST.png">
<meta property="article:published_time" content="2016-03-08T14:18:34.000Z">
<meta property="article:modified_time" content="2020-12-19T06:37:18.634Z">
<meta property="article:author" content="Patric Zhao">
<meta property="article:tag" content="multicores">
<meta property="article:tag" content="rstats">
<meta property="article:tag" content="parallel computing">
<meta property="article:tag" content="performance optimization">
<meta property="article:tag" content="R">
<meta property="article:tag" content="deep learning">
<meta property="article:tag" content="H2O">
<meta property="article:tag" content="profiling">
<meta property="article:tag" content="classification">
<meta property="article:tag" content="cuBLAS">
<meta property="article:tag" content="dnn">
<meta property="article:tag" content="GPU">
<meta property="article:tag" content="high performance">
<meta property="article:tag" content="MKL">
<meta property="article:tag" content="MNIST">
<meta property="article:tag" content="multithreads">
<meta property="article:tag" content="openblas">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://www.parallelr.com/wp-content/uploads/2016/02/mnist.jpg">
  
    <link rel="alternate" href="/oneXPU/atom.xml" title="oneXPU" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/oneXPU/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/oneXPU/css/style.css">

  
    
<link rel="stylesheet" href="/oneXPU/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/oneXPU/" id="logo">oneXPU</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/oneXPU/" id="subtitle">Diving into Parallel Technology</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/oneXPU/">Home</a>
        
          <a class="main-nav-link" href="/oneXPU/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/oneXPU/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://jitmatrix.github.io/oneXPU"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-r-dnn-parallel-acceleration" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/oneXPU/2016/03/08/r-dnn-parallel-acceleration/" class="article-date">
  <time class="dt-published" datetime="2016-03-08T14:18:34.000Z" itemprop="datePublished">2016-03-08</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/oneXPU/categories/GPGPU/">GPGPU</a>►<a class="article-category-link" href="/oneXPU/categories/GPGPU/MultiCores/">MultiCores</a>►<a class="article-category-link" href="/oneXPU/categories/GPGPU/MultiCores/Performance-Optimizaiton/">Performance Optimizaiton</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      R for Deep Learning (II): Achieve High-Performance DNN with Parallel Acceleration
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <hr>
<p><em>I would like to thank  <a target="_blank" rel="noopener" href="http://junma5.weebly.com/">Jun Ma</a>  and all other technical reviewers and readers for their informative comments and suggestions in this post.</em></p>
<hr>
<blockquote>
<p><em><strong><a href="courses.csail.mit.edu/18.337/2015/docs/50YearsDataScience.pdf">50 years of Data Science</a></strong>, David Donoho, 2015</em></p>
<p><em>**Computing with Data.**Every data scientist should know and use several languages for data analysis and data processing. These can include popular languages like R and Python …</em> <em>Beyond basic knowledge of languages, data scientists need to keep current on new idioms for efficiently using those languages and need to understand the deeper issues associated with computational efficiency.</em></p>
</blockquote>
<p>  In the previous post, <a target="_blank" rel="noopener" href="http://www.parallelr.com/r-deep-neural-network-from-scratch/">R for deep learning (I)</a>, I introduced the core components of neural networks and illustrated how to implement it from scratch by R. Now, I will focus on computational performance and efficiency of R’s implementation, especially for the parallel algorithm on multicores CPU and <a target="_blank" rel="noopener" href="http://www.nvidia.com/object/tesla-supercomputing-solutions.html">NVIDIA GPU</a> architectures.  </p>
<h2 id="Performance-Profiling"><a href="#Performance-Profiling" class="headerlink" title="Performance Profiling"></a>Performance Profiling</h2><p>In this post, we are going to a little big dataset, <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a>, for performance analysis. <a target="_blank" rel="noopener" href="http://yann.lecun.com/exdb/mnist/">MNIST</a> widely used to measure the accuracy of classification by handwritten digits in machine learning field, and also used for the competition in <a target="_blank" rel="noopener" href="https://www.kaggle.com/c/digit-recognizer">Kaggle</a> (data in download page or <a target="_blank" rel="noopener" href="http://www.parallelr.com/materials/3_ParDNN/">here</a>). <a target="_blank" rel="noopener" href="http://yann.lecun.com/">Yann</a> has provided the classification results based on various machine learning algorithms on his <a target="_blank" rel="noopener" href="http://yann.lecun.com/exdb/mnist/">page</a>. <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/02/mnist.jpg"><img src="http://www.parallelr.com/wp-content/uploads/2016/02/mnist.jpg" alt="mnist"></a></p>
<p>Picture.1 Handwritten Digits in MNIST dataset</p>
<p>The MNIST database contains 60,000 training images and 10,000 testing images. Each of image is represented by 28*28 points so totally  784 points. In this post, I will train the neural network with 784 points as input features and the number of 0-9 as output classes,  then compare the runtime of our R DNN code with <a target="_blank" rel="noopener" href="http://www.h2o.ai/verticals/algos/deep-learning/">H2O deep learning</a> implementations for 2-layers networks of the various number of hidden units (HU).</p>
<p># h2o<br><a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/library">library</a>(h2o)<br># single thread<br>h2o.init()<br> <br> <br>train_file &lt;- “<a target="_blank" rel="noopener" href="https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/train.csv.gz&quot;">https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/train.csv.gz&quot;</a><br>test_file &lt;- “<a target="_blank" rel="noopener" href="https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/test.csv.gz&quot;">https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/test.csv.gz&quot;</a><br> <br>train &lt;- h2o.importFile(train_file)<br>test  &lt;- h2o.importFile(test_file)<br> <br># To see a brief summary of the data, run the following command<br><a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/summary">summary</a>(train)<br><a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/summary">summary</a>(test)<br> <br>y &lt;- “C785”<br>x &lt;- <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/setdiff">setdiff</a>(<a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/names">names</a>(train), y)<br> <br># We encode the response column as categorical for multinomial<br>#classification<br>train[,y] &lt;- <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/as.factor">as.factor</a>(train[,y])<br>test[,y]  &lt;- <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/as.factor">as.factor</a>(test[,y])<br> <br># Train a Deep Learning model and valid<br><a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/system.time">system.time</a>(<br>  model_cv &lt;- h2o.deeplearning(x = x,<br>                               y = y,<br>                               training_frame = train,<br>                               distribution = “multinomial”,<br>                               activation = “Rectifier”,<br>                               hidden = <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/c">c</a>(32),<br>                               l1 = 1e-5,<br>                               epochs = 200)<br>)</p>
<p>As I know, H2O is  the fast and most popular deep learning package in R platform implemented by Java in the backend. Thus, it will be valuable to know how much performance gap between native R code and  the mature package. As below barplot shown, the hidden units of 32, 64 and 128 are tested in 200 steps. <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/02/Runtime.RDNN_.H2O.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/02/Runtime.RDNN_.H2O.png" alt="Runtime.RDNN.H2O"></a> Obviously,  the R DNN is significantly slow than H2O, and the runtime increases with the number of hidden units quickly. For more details, we break down the R DNN runtime into each function call  by Rprof() and summaryRprof() which report out the final results including 4 parts: <em>total.time</em>, <em>total.pct</em>, <em>self.time</em> and <em>self.pct</em>. The <em>self.time</em> and <em>self.pct</em> columns represent the elapsed time for each function, excluding the time from its inner called functions. The <em>total.time</em> and <em>total.pct</em> columns mean the total elapsed time for each function including the time spent on function calls [<a target="_blank" rel="noopener" href="https://www.packtpub.com/application-development/r-high-performance-programming">Aloysius Lim</a>]. From the profiling results, we can see the top 1 time-consuming function is *<em>“%</em>%”** which represents matrix multiplications and usually people call it <strong>GEMM</strong> (GEneral Matrix Multiplication).  </p>
<p>&gt; <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/utils/Rprof">Rprof</a>()<br>&gt; mnist.model &lt;- train.dnn(x=1:784, y=785, traindata=train, hidden=64, maxit=200, display=50)<br>&gt; <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/utils/Rprof">Rprof</a>(NULL)<br>&gt; <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/utils/summaryRprof">summaryRprof</a>()<br>$by.self<br>                   self.time self.pct total.time total.pct<br>“%<em>%”                1250.08    90.19    1250.08     90.19<br>“t.default”            61.62     4.45      61.62      4.45<br>“pmax”                 24.40     1.76      28.42      2.05<br>“aperm.default”        11.60     0.84      11.60      0.84<br>“array”                10.36     0.75      10.36      0.75<br>“train.dnn”             9.74     0.70    1386.00    100.00<br>“&lt;=”                    5.72     0.41       5.72      0.41<br>“mostattributes&lt;-“      4.02     0.29       4.02      0.29<br>“exp”                   3.60     0.26       3.60      0.26<br>“sweep”                 1.58     0.11     676.32     48.80<br>“is.data.frame”         1.28     0.09       1.28      0.09<br>“colSums”               0.86     0.06       0.86      0.06<br>“/“                     0.52     0.04       0.52      0.04<br>“rowSums”               0.36     0.03       0.36      0.03<br>“unname”                0.18     0.01       1.46      0.11<br>“-“                     0.04     0.00       0.04      0.00<br>“t”                     0.02     0.00      61.64      4.45<br>“sum”                   0.02     0.00       0.02      0.00<br> <br>$by.total<br>                   total.time total.pct self.time self.pct<br>“train.dnn”           1386.00    100.00      9.74     0.70<br>“%</em>%”                 1250.08     90.19   1250.08    90.19<br>“sweep”                676.32     48.80      1.58     0.11<br>“t”                     61.64      4.45      0.02     0.00<br>“t.default”             61.62      4.45     61.62     4.45<br>“pmax”                  28.42      2.05     24.40     1.76<br>“aperm”                 21.96      1.58      0.00     0.00<br>“aperm.default”         11.60      0.84     11.60     0.84<br>“array”                 10.36      0.75     10.36     0.75<br>“&lt;=”                     5.72      0.41      5.72     0.41<br>“mostattributes&lt;-“       4.02      0.29      4.02     0.29<br>“exp”                    3.60      0.26      3.60     0.26<br>“unname”                 1.46      0.11      0.18     0.01<br>“is.data.frame”          1.28      0.09      1.28     0.09<br>“data.matrix”            1.28      0.09      0.00     0.00<br>“colSums”                0.86      0.06      0.86     0.06<br>“/“                      0.52      0.04      0.52     0.04</p>
<h2 id="Parallel-Acceleration"><a href="#Parallel-Acceleration" class="headerlink" title="Parallel Acceleration"></a>Parallel Acceleration</h2><p>From above analysis,  the matrix multiplication (“%*%”) accounts for about 90% computation time in the training stage of the neural network. Thus, the key of DNN acceleration is to speed up matrix multiplication.  Fortunately, there are already several parallel libraries for matrix multiplication and we can deploy it to R easily.  In this post, I will introduce three basic linear algebra subprograms (<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms">BLAS</a>) libraries, <a target="_blank" rel="noopener" href="http://www.openblas.net/">openBLAS</a>, <a target="_blank" rel="noopener" href="https://software.intel.com/en-us/intel-mkl">Intel MKL</a>, and <a target="_blank" rel="noopener" href="https://developer.nvidia.com/cublas">cuBLAS</a>. (In another <a target="_blank" rel="noopener" href="http://www.parallelr.com/r-hpac-benchmark-analysis/">blog</a>, moreover, we show their performance on modern hardware architectures and instructions of installation). The former two are multithread accelerated libraries which need to be built with R together (instructions in <a target="_blank" rel="noopener" href="http://www.r-bloggers.com/compile-r-and-openblas-from-source-guide/">here</a> and <a target="_blank" rel="noopener" href="https://software.intel.com/en-us/articles/using-intel-mkl-with-r">here</a>). On the other hand, it is indeed easy to apply NVIDIA cuBLAS library in R on Linux system by  the drop-on wrap, <a target="_blank" rel="noopener" href="http://docs.nvidia.com/cuda/nvblas/">nvBLAS</a>, which can be preloaded into R in order to hijack original Rblas.so. By this way, we can leverage NVIDIA GPU’s power into R with zero programming efforts :) The below picture shows the architecture of R with BLAS libraries. Typically, R will call their own BLAS, Rblas.so,on Linux system which handles all kinds of linear algebra functions (<a target="_blank" rel="noopener" href="http://www.netlib.org/blas/#_blas_routines">here</a>), but it is a single thread implementation. The trick to speed up the linear algebra calculations is to update the R standard BLAS to modern multithread libraries. For R developers, there is almost a ‘free lunch’  without rewriting their codes for parallel accelerations. <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/02/R_BLAS.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/02/R_BLAS-300x144.png" alt="R_BLAS"></a> As below code shown, we tested prebuilt R  with openBLAS,  Intel MKL and  nvBLAS  for 2-layers neural network of 32, 64 and 128 neurons.From the below bar chart, the runtime of R DNN are dramatically decreased and it is nearly <strong>2X</strong> faster than H2O and <strong>9X</strong> speedup (from 2816 to 365 for 128 hidden neurons network) than original R code!</p>
<blockquote>
<p># you must create a configuration file nvBLAS.conf in the current directory, example in <a target="_blank" rel="noopener" href="http://docs.nvidia.com/cuda/nvblas/#configuration_example">here</a>. &gt;LD_PRELOAD=libnvblas.so /home/patricz/tools/R-3.2.0/bin/bin/R CMD <a target="_blank" rel="noopener" href="http://inside-r.org/packages/cran/batch">BATCH</a> MNIST_DNN.R</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/02/rdnn_gemm.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/02/rdnn_gemm.png" alt="rdnn_mnist_blas_gemm"></a></p>
<blockquote>
<p>Note: Testing hardware: Ivy Bridge E5-2690 v2 @ 3.00GHz, dual socket 10-core (total 20 cores), 128G RAM;   NVIDIA GPU K40m;  Software:  CUDA 7.5,  OpenBLAS 0.2.8,  Intel MKL 11.1</p>
</blockquote>
<h2 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h2><p>Till now, it seems everything is great and we gain lots of performance increments from BLAS libraries under multicores CPU and NVIDIA GPU system. <strong>But can we do a little more for performance optimizations?</strong> Let’s look into the profiling again and probe the top performance limiters. The below table shows the breakdown of R DNN code under acceleration by nvBLAS where the GEMM performance is 10X faster ( from original 1250 to 114 seconds ).  But the next two top time-consuming functions, “sweep()” and “t()”, account for 27% (<em>self.pct</em>) runtime.Therefore, it makes sense to do further optimization for them.</p>
<blockquote>
<p><strong>A question for readers:</strong> <strong>The total time of ‘sweep’ is 87.28 but the self-time is only 1.8, so what are the real computation parts and is it a reasonable choose to optimize it?</strong></p>
</blockquote>
<p><a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/02/runtime.breakdown.nvblas.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/02/runtime.breakdown.nvblas.png" alt="DNN.runtime.breakdown.nvblas"></a> From the source code, there are several function calls of t()  to transfer matrix before multiplication; however, R has already provided the inner function `crossprod` and `tcrossprod` for this kind of operations.</p>
<pre><code>\# original:  t() with matrix multiplication
dW2     &lt;- [t](http://inside-r.org/r-doc/base/t)(hidden.layer) %*% dscores 
dhidden &lt;- dscores %*% [t](http://inside-r.org/r-doc/base/t)(W2)</code></pre>
<p> <br>   # Opt1: use builtin function<br>   dW2     &lt;- <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/crossprod">crossprod</a>(hidden.layer, dscores)<br>   dhidden &lt;- <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/tcrossprod">tcrossprod</a>(dscores, W2)</p>
<p>Secondly, the `sweep()` is performed to add the matrix with bias. Alternatively, we can combine weight and bias together and then use matrix multiplications, as below code shown. But the backside is that we have to create the new matrix for combinations of matrix and bias which increases memory pressure.</p>
<p> # Opt2: combine data and add 1 column for bias<br> #  extra matrix for combinations<br> X1   &lt;- <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/cbind">cbind</a>(X, <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/rep">rep</a>(1, <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/nrow">nrow</a>(X)))<br> W1b1 &lt;- <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/rbind">rbind</a>(W1, b1)<br> W2b2 &lt;- <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/rbind">rbind</a>(W2, b2)<br> <br> <br> # Opt2: remove `sweep`<br> #hidden.layer &lt;- sweep(X %<em>% W1 ,2, b1, ‘+’)<br> hidden.layer &lt;- X1 %</em>% W1b1<br> <br> #score &lt;- sweep(hidden.layer %<em>% W2, 2, b2, ‘+’)<br> hidden.layer1 &lt;- <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/cbind">cbind</a>(hidden.layer, <a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/rep">rep</a>(1,<a target="_blank" rel="noopener" href="http://inside-r.org/r-doc/base/nrow">nrow</a>(hidden.layer)))<br> score &lt;- hidden.layer1 %</em>% W2b2</p>
<p><a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/03/opt2.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/03/opt2.png" alt="bias optimization"></a> Now, we profile and compare the results again with below table. We save the computation time of ‘t.default’ and ‘aperm.default’ after removing ‘t()’ and ‘sweep()’ functions. Totally, the performance is <strong>DOUBLE</strong> again! <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/02/DNN.Optimization.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/02/DNN.Optimization.png" alt="DNN.Optimization"></a></p>
<blockquote>
<p><strong>Another question:</strong> <strong>What is your opinion about the next step of optimizations? Any good idea and tell me your numbers  :)</strong></p>
</blockquote>
<p> </p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>In this post, we have introduced the coarse granularity parallelism skill to accelerate R code by BLAS libraries on multicores CPU and NVIDIA GPU architectures. Till now, we have got <strong>+10X</strong> speedup under NVIDIA GPU and <strong>2X</strong> faster than 20-threads H2O packages for a relatively small network; however, there are still lots of space to improve, take a try. <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/02/RDNN_nvBLAS_runtime.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/02/RDNN_nvBLAS_runtime.png" alt="RDNN_nvBLAS_runtime"></a> And finally we train MNIST dataset with a 2 layer neural network of 300 hidden unit on Tesla K40m GPU, and we reach 94.7% accuracy (5.3% error rate)  in an hour while <a target="_blank" rel="noopener" href="http://yann.lecun.com/exdb/mnist/">Yann</a> got 4.7% error rate with smaller learning rate ( lr=0.001)  which will cost longer training time but more accurate.   <a target="_blank" rel="noopener" href="http://www.parallelr.com/wp-content/uploads/2016/03/MNIST.png"><img src="http://www.parallelr.com/wp-content/uploads/2016/03/MNIST.png" alt="neural network training results"></a> <strong>In the next <a target="_blank" rel="noopener" href="http://www.parallelr.com/r-dnn-cuda-multigpu/">blog post</a>, I will continue deep learning topic and focus on CUDA integration and multiple GPUs accelerations.</strong></p>
<hr>
<p>Notes: 1. The entire source code of this post in <a target="_blank" rel="noopener" href="https://github.com/PatricZhao/ParallelR">here</a> 2. The PDF version of this post in <a target="_blank" rel="noopener" href="http://www.parallelr.com/materials/3_ParDNN/3_.DNN_Parallel_Acceleration.pdf">here</a> 3. Pretty R syntax in this blog is <a target="_blank" rel="noopener" href="http://www.inside-r.org/pretty-r" title="Created by Pretty R at inside-R.org">Created by inside-R .org</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://jitmatrix.github.io/oneXPU/2016/03/08/r-dnn-parallel-acceleration/" data-id="ckivbxbgz000lzroe4raq7tkz" data-title="R for Deep Learning (II): Achieve High-Performance DNN with Parallel Acceleration" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/GPU/" rel="tag">GPU</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/H2O/" rel="tag">H2O</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/MKL/" rel="tag">MKL</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/MNIST/" rel="tag">MNIST</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/R/" rel="tag">R</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/classification/" rel="tag">classification</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/cuBLAS/" rel="tag">cuBLAS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/deep-learning/" rel="tag">deep learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/dnn/" rel="tag">dnn</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/high-performance/" rel="tag">high performance</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/multicores/" rel="tag">multicores</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/multithreads/" rel="tag">multithreads</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/openblas/" rel="tag">openblas</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/parallel-computing/" rel="tag">parallel computing</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/performance-optimization/" rel="tag">performance optimization</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/profiling/" rel="tag">profiling</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/oneXPU/tags/rstats/" rel="tag">rstats</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/oneXPU/2016/04/15/r-hpac-benchmark-analysis/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          R benchmark for High-Performance Analytics and Computing (I):Accelerators
        
      </div>
    </a>
  
  
    <a href="/oneXPU/2016/02/13/r-deep-neural-network-from-scratch/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">R for Deep Learning (I):  Build Fully Connected Neural Network from Scratch</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/">Accelerators</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/GPGPU/">GPGPU</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/GPGPU/MultiCores/">MultiCores</a></li><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/GPGPU/Performance-Optimizaiton/">Performance Optimizaiton</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/General/">General</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/General/GPGPU/">GPGPU</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/General/GPGPU/MultiCores/">MultiCores</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/General/GPGPU/MultiCores/Performance-Optimizaiton/">Performance Optimizaiton</a></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/General/MPI/">MPI</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/General/MPI/MultiCores/">MultiCores</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/General/MPI/MultiCores/Performance-Optimizaiton/">Performance Optimizaiton</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/General/MPI/MultiCores/Performance-Optimizaiton/Vectorization/">Vectorization</a></li></ul></li></ul></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/MultiCores/">MultiCores</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/MultiCores/Performance-Optimizaiton/">Performance Optimizaiton</a></li><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/Accelerators/MultiCores/Vectorization/">Vectorization</a></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/GPGPU/">GPGPU</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/GPGPU/Intel-Xeon-Phi/">Intel Xeon Phi</a></li><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/GPGPU/MultiCores/">MultiCores</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/GPGPU/MultiCores/Performance-Optimizaiton/">Performance Optimizaiton</a></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/General/">General</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/oneXPU/categories/General/MultiCores/">MultiCores</a></li></ul></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/BLAS/" rel="tag">BLAS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/CRAN/" rel="tag">CRAN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/CUDA/" rel="tag">CUDA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/GEMM/" rel="tag">GEMM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/GPU/" rel="tag">GPU</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/H2O/" rel="tag">H2O</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/HPAC/" rel="tag">HPAC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/HPC/" rel="tag">HPC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/MIC/" rel="tag">MIC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/MKL/" rel="tag">MKL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/MNIST/" rel="tag">MNIST</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/Maximum-Likelihood/" rel="tag">Maximum Likelihood</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/R/" rel="tag">R</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/Rcpp/" rel="tag">Rcpp</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/SSE/" rel="tag">SSE</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/Xeon/" rel="tag">Xeon</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/Xeon-Phi/" rel="tag">Xeon Phi</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/benchmark/" rel="tag">benchmark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/big-data/" rel="tag">big data</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/boost/" rel="tag">boost</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/classification/" rel="tag">classification</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/cuBLAS/" rel="tag">cuBLAS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/data-analytics/" rel="tag">data analytics</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/deep-learning/" rel="tag">deep learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/dnn/" rel="tag">dnn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/doMC/" rel="tag">doMC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/doParallel/" rel="tag">doParallel</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/foreach/" rel="tag">foreach</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/gmatrix/" rel="tag">gmatrix</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/gpuR/" rel="tag">gpuR</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/gputools/" rel="tag">gputools</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/high-performance/" rel="tag">high performance</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/iris/" rel="tag">iris</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/knn/" rel="tag">knn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/lightboost/" rel="tag">lightboost</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/machine-learing/" rel="tag">machine learing</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/machine-learning/" rel="tag">machine learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/mcapply/" rel="tag">mcapply</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/memory-usage/" rel="tag">memory usage</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/multicores/" rel="tag">multicores</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/multit/" rel="tag">multit</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/multithreads/" rel="tag">multithreads</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/mutlithreading/" rel="tag">mutlithreading</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/mutltiGPU/" rel="tag">mutltiGPU</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/neural-network/" rel="tag">neural network</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/nvblas/" rel="tag">nvblas</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/openMP/" rel="tag">openMP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/openblas/" rel="tag">openblas</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/parallel/" rel="tag">parallel</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/parallel-computing/" rel="tag">parallel computing</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/parlapply/" rel="tag">parlapply</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/performance-optimization/" rel="tag">performance optimization</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/profiling/" rel="tag">profiling</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/rstats/" rel="tag">rstats</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/sklearn/" rel="tag">sklearn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/snow/" rel="tag">snow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/oneXPU/tags/xgboost/" rel="tag">xgboost</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/oneXPU/tags/BLAS/" style="font-size: 10px;">BLAS</a> <a href="/oneXPU/tags/CRAN/" style="font-size: 10px;">CRAN</a> <a href="/oneXPU/tags/CUDA/" style="font-size: 12px;">CUDA</a> <a href="/oneXPU/tags/GEMM/" style="font-size: 12px;">GEMM</a> <a href="/oneXPU/tags/GPU/" style="font-size: 14px;">GPU</a> <a href="/oneXPU/tags/H2O/" style="font-size: 12px;">H2O</a> <a href="/oneXPU/tags/HPAC/" style="font-size: 10px;">HPAC</a> <a href="/oneXPU/tags/HPC/" style="font-size: 12px;">HPC</a> <a href="/oneXPU/tags/MIC/" style="font-size: 10px;">MIC</a> <a href="/oneXPU/tags/MKL/" style="font-size: 14px;">MKL</a> <a href="/oneXPU/tags/MNIST/" style="font-size: 10px;">MNIST</a> <a href="/oneXPU/tags/Maximum-Likelihood/" style="font-size: 10px;">Maximum Likelihood</a> <a href="/oneXPU/tags/R/" style="font-size: 20px;">R</a> <a href="/oneXPU/tags/Rcpp/" style="font-size: 10px;">Rcpp</a> <a href="/oneXPU/tags/SSE/" style="font-size: 10px;">SSE</a> <a href="/oneXPU/tags/Xeon/" style="font-size: 10px;">Xeon</a> <a href="/oneXPU/tags/Xeon-Phi/" style="font-size: 10px;">Xeon Phi</a> <a href="/oneXPU/tags/benchmark/" style="font-size: 10px;">benchmark</a> <a href="/oneXPU/tags/big-data/" style="font-size: 10px;">big data</a> <a href="/oneXPU/tags/boost/" style="font-size: 10px;">boost</a> <a href="/oneXPU/tags/classification/" style="font-size: 10px;">classification</a> <a href="/oneXPU/tags/cuBLAS/" style="font-size: 16px;">cuBLAS</a> <a href="/oneXPU/tags/data-analytics/" style="font-size: 12px;">data analytics</a> <a href="/oneXPU/tags/deep-learning/" style="font-size: 14px;">deep learning</a> <a href="/oneXPU/tags/dnn/" style="font-size: 12px;">dnn</a> <a href="/oneXPU/tags/doMC/" style="font-size: 10px;">doMC</a> <a href="/oneXPU/tags/doParallel/" style="font-size: 10px;">doParallel</a> <a href="/oneXPU/tags/foreach/" style="font-size: 10px;">foreach</a> <a href="/oneXPU/tags/gmatrix/" style="font-size: 10px;">gmatrix</a> <a href="/oneXPU/tags/gpuR/" style="font-size: 10px;">gpuR</a> <a href="/oneXPU/tags/gputools/" style="font-size: 10px;">gputools</a> <a href="/oneXPU/tags/high-performance/" style="font-size: 10px;">high performance</a> <a href="/oneXPU/tags/iris/" style="font-size: 10px;">iris</a> <a href="/oneXPU/tags/knn/" style="font-size: 10px;">knn</a> <a href="/oneXPU/tags/lightboost/" style="font-size: 10px;">lightboost</a> <a href="/oneXPU/tags/machine-learing/" style="font-size: 10px;">machine learing</a> <a href="/oneXPU/tags/machine-learning/" style="font-size: 12px;">machine learning</a> <a href="/oneXPU/tags/mcapply/" style="font-size: 10px;">mcapply</a> <a href="/oneXPU/tags/memory-usage/" style="font-size: 10px;">memory usage</a> <a href="/oneXPU/tags/multicores/" style="font-size: 18px;">multicores</a> <a href="/oneXPU/tags/multit/" style="font-size: 10px;">multit</a> <a href="/oneXPU/tags/multithreads/" style="font-size: 10px;">multithreads</a> <a href="/oneXPU/tags/mutlithreading/" style="font-size: 10px;">mutlithreading</a> <a href="/oneXPU/tags/mutltiGPU/" style="font-size: 10px;">mutltiGPU</a> <a href="/oneXPU/tags/neural-network/" style="font-size: 10px;">neural network</a> <a href="/oneXPU/tags/nvblas/" style="font-size: 10px;">nvblas</a> <a href="/oneXPU/tags/openMP/" style="font-size: 12px;">openMP</a> <a href="/oneXPU/tags/openblas/" style="font-size: 12px;">openblas</a> <a href="/oneXPU/tags/parallel/" style="font-size: 10px;">parallel</a> <a href="/oneXPU/tags/parallel-computing/" style="font-size: 18px;">parallel computing</a> <a href="/oneXPU/tags/parlapply/" style="font-size: 10px;">parlapply</a> <a href="/oneXPU/tags/performance-optimization/" style="font-size: 16px;">performance optimization</a> <a href="/oneXPU/tags/profiling/" style="font-size: 12px;">profiling</a> <a href="/oneXPU/tags/python/" style="font-size: 10px;">python</a> <a href="/oneXPU/tags/rstats/" style="font-size: 20px;">rstats</a> <a href="/oneXPU/tags/sklearn/" style="font-size: 10px;">sklearn</a> <a href="/oneXPU/tags/snow/" style="font-size: 10px;">snow</a> <a href="/oneXPU/tags/xgboost/" style="font-size: 10px;">xgboost</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/oneXPU/archives/2017/04/">April 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/oneXPU/archives/2017/01/">January 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/oneXPU/archives/2016/09/">September 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/oneXPU/archives/2016/08/">August 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/oneXPU/archives/2016/07/">July 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/oneXPU/archives/2016/05/">May 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/oneXPU/archives/2016/04/">April 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/oneXPU/archives/2016/03/">March 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/oneXPU/archives/2016/02/">February 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/oneXPU/2017/04/07/r-hpac-benchmark-analysis-gpu/">R benchmark for High-Performance Analytics and Computing (II): GPU Packages</a>
          </li>
        
          <li>
            <a href="/oneXPU/2017/01/24/parallel-computation-with-r-and-xgboost/">Parallel Computation with R and XGBoost</a>
          </li>
        
          <li>
            <a href="/oneXPU/2016/09/10/r-with-parallel-computing/">R with Parallel Computing from User Perspectives</a>
          </li>
        
          <li>
            <a href="/oneXPU/2016/08/15/r-cran-package-modernization-openmp/">R and OpenMP:  CRAN Package Modernization</a>
          </li>
        
          <li>
            <a href="/oneXPU/2016/07/26/r-and-openmp-boosting-compiled-code-on-multi-core-cpu-s/">R and openMP: boosting compiled code on multi-core cpu-s</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2020 Patric Zhao<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/oneXPU/" class="mobile-nav-link">Home</a>
  
    <a href="/oneXPU/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/oneXPU/js/jquery-3.4.1.min.js"></script>



  
<script src="/oneXPU/fancybox/jquery.fancybox.min.js"></script>




<script src="/oneXPU/js/script.js"></script>





  </div>
</body>
</html>
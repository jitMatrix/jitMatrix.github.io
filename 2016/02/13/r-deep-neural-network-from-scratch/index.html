<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/oneXPU/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/oneXPU/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/oneXPU/oneXPU/uploads/favicon/favicon.ico">
  <link rel="mask-icon" href="/oneXPU/images/logo.svg" color="#222">

<link rel="stylesheet" href="/oneXPU/css/main.css">


<link rel="stylesheet" href="/oneXPU/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jitmatrix.github.io","root":"/oneXPU/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="BackgroundsDeep Neural Network (DNN) has made a great progress in recent years in image recognition, natural language processing and automatic driving fields, such as Picture.1 shown from 2012  to 201">
<meta property="og:type" content="article">
<meta property="og:title" content="R for Deep Learning (I):  Build Fully Connected Neural Network from Scratch">
<meta property="og:url" content="https://jitmatrix.github.io/oneXPU/2016/02/13/r-deep-neural-network-from-scratch/index.html">
<meta property="og:site_name" content="ParallelR">
<meta property="og:description" content="BackgroundsDeep Neural Network (DNN) has made a great progress in recent years in image recognition, natural language processing and automatic driving fields, such as Picture.1 shown from 2012  to 201">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://jitmatrix.github.io/oneXPU/uploads/2016/02/ces2016.png">
<meta property="og:image" content="https://jitmatrix.github.io/oneXPU/uploads/2016/02/matrureDNNPackages-2.png">
<meta property="og:image" content="https://jitmatrix.github.io/oneXPU/uploads/2016/02/dnn_architecture.png">
<meta property="og:image" content="https://jitmatrix.github.io/oneXPU/uploads/2016/02/neuron.png">
<meta property="og:image" content="https://jitmatrix.github.io/oneXPU/uploads/2016/02/iris_network.png">
<meta property="og:image" content="https://jitmatrix.github.io/oneXPU/uploads/2016/02/class.relu_.png">
<meta property="og:image" content="https://jitmatrix.github.io/oneXPU/uploads/2016/02/iris_loss_accuracy-1.png">
<meta property="og:image" content="https://jitmatrix.github.io/oneXPU/uploads/2016/02/tensorflow.png">
<meta property="article:published_time" content="2016-02-13T14:08:47.000Z">
<meta property="article:modified_time" content="2020-12-19T10:16:53.419Z">
<meta property="article:author" content="Patric Zhao">
<meta property="article:tag" content="rstats">
<meta property="article:tag" content="R">
<meta property="article:tag" content="deep learning">
<meta property="article:tag" content="machine learning">
<meta property="article:tag" content="dnn">
<meta property="article:tag" content="data analytics">
<meta property="article:tag" content="big data">
<meta property="article:tag" content="iris">
<meta property="article:tag" content="neural network">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jitmatrix.github.io/oneXPU/uploads/2016/02/ces2016.png">

<link rel="canonical" href="https://jitmatrix.github.io/oneXPU/2016/02/13/r-deep-neural-network-from-scratch/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>R for Deep Learning (I):  Build Fully Connected Neural Network from Scratch | ParallelR</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/oneXPU/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">ParallelR</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Diving into Parallel Technology</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/oneXPU/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/oneXPU/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-opensource">

    <a href="/oneXPU/opensource/" rel="section"><i class="fa fa-archive fa-fw"></i>Opensource</a>

  </li>
        <li class="menu-item menu-item-presentation">

    <a href="/oneXPU/presentation/" rel="section"><i class="fa fa-archive fa-fw"></i>Presentation</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/oneXPU/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/oneXPU/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/oneXPU/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/oneXPU/404/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>Commonweal 404</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jitmatrix.github.io/oneXPU/2016/02/13/r-deep-neural-network-from-scratch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/oneXPU/images/avatar.gif">
      <meta itemprop="name" content="Patric Zhao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ParallelR">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          R for Deep Learning (I):  Build Fully Connected Neural Network from Scratch
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-02-13 14:08:47" itemprop="dateCreated datePublished" datetime="2016-02-13T14:08:47+00:00">2016-02-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-19 10:16:53" itemprop="dateModified" datetime="2020-12-19T10:16:53+00:00">2020-12-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/General/" itemprop="url" rel="index"><span itemprop="name">General</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="Backgrounds"><a href="#Backgrounds" class="headerlink" title="Backgrounds"></a>Backgrounds</h2><p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Deep_learning">Deep Neural Network (DNN)</a> has made a great progress in recent years in image recognition, natural language processing and automatic driving fields, such as Picture.1 shown from 2012  to 2015 DNN improved <a target="_blank" rel="noopener" href="http://image-net.org/challenges/LSVRC/2015/">IMAGNET</a>’s accuracy from ~80% to ~95%, which really beats traditional computer vision (CV) methods. <a href="/oneXPU/uploads/2016/02/ces2016.png"><img src="/oneXPU/uploads/2016/02/ces2016.png" alt="Jensen&#39;s CES2016 talk" title="From NVIDIA CEO Jensen&#39;s talk in CES16"></a></p>
<p>Picture.1 - <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLZHnYvH1qtObAdk3waYMalhA8bdbhyTm2">From NVIDIA CEO Jensen’s talk in CES16</a></p>
<p>In this post, we will focus on fully connected neural networks which are commonly called DNN in data science. The biggest advantage of DNN is to extract and learn features automatically by deep layers architecture, especially for these complex and high-dimensional data that feature engineers can’t capture easily, examples in <a target="_blank" rel="noopener" href="http://blog.kaggle.com/2014/08/01/learning-from-the-best/">Kaggle</a>. Therefore, DNN is also very attractive to data scientists and there are lots of successful cases as well in classification, time series, and recommendation system, such as <a target="_blank" rel="noopener" href="http://blog.dominodatalab.com/using-r-h2o-and-domino-for-a-kaggle-competition/">Nick’s post</a> and <a target="_blank" rel="noopener" href="http://www.r-bloggers.com/using-neural-networks-for-credit-scoring-a-simple-example/">credit scoring</a> by DNN. In CRAN and R’s community, there are several popular and mature DNN packages including <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/nnet/index.html">nnet</a>, <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/neuralnet/">nerualnet</a>, <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/h2o/index.html">H2O</a>, <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/darch/index.html">DARCH</a>, <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/deepnet/index.html">deepnet </a>and <a target="_blank" rel="noopener" href="https://github.com/dmlc/mxnet">mxnet</a>,  and I strong recommend <a target="_blank" rel="noopener" href="http://www.h2o.ai/verticals/algos/deep-learning/">H2O DNN algorithm and R interface</a>. <a href="/oneXPU/uploads/2016/02/matrureDNNPackages-2.png"><img src="/oneXPU/uploads/2016/02/matrureDNNPackages-2.png"></a>   So, <strong>why we need to build DNN from scratch at all?</strong> - Understand how neural network works Using existing DNN package, you only need one line R code for your DNN model in most of the time and there is <a target="_blank" rel="noopener" href="http://www.r-bloggers.com/fitting-a-neural-network-in-r-neuralnet-package/">an example</a> by neuralnet. For the inexperienced user, however, the processing and results may be difficult to understand.  Therefore, it will be a valuable practice to implement your own network in order to understand more details from mechanism and computation views. - Build specified network with your new ideas DNN is one of rapidly developing area. Lots of novel works and research results are published in the top journals and Internet every week, and the users also have their specified neural network configuration to meet their problems such as different activation functions, loss functions, regularization, and connected graph. On the other hand, the existing packages are definitely behind the latest researches, and almost all existing packages are written in C/C++, Java so it’s not flexible to apply latest changes and your ideas into the packages. - Debug and visualize network and data As we mentioned, the existing DNN package is highly assembled and written by low-level languages so that it’s a nightmare to debug the network layer by layer or node by node. Even it’s not easy to visualize the results in each layer, monitor the data or weights changes during training, and show the discovered patterns in the network.  </p>
<h2 id="Fundamental-Concepts-and-Components"><a href="#Fundamental-Concepts-and-Components" class="headerlink" title="Fundamental Concepts and Components"></a>Fundamental Concepts and Components</h2><p><a target="_blank" rel="noopener" href="http://lo.epfl.ch/files/content/sites/lo/files/shared/1990/IEEE_78_1637_Oct1990.pdf">Fully connected neural network</a>, called DNN in data science, is that adjacent network layers are fully connected to each other. Every neuron in the network is connected to every neuron in adjacent layers. A very simple and typical neural network is shown below with 1 input layer, 2 hidden layers, and 1 output layer. Mostly, when researchers talk about network’s architecture, it refers to the configuration of DNN, such as how many layers in the network, how many neurons in each layer, what kind of activation, loss function, and regularization are used. <a href="/oneXPU/uploads/2016/02/dnn_architecture.png"><img src="/oneXPU/uploads/2016/02/dnn_architecture.png" alt="arch"></a> Now, we will go through the basic components of DNN and show you how it is implemented in R. <strong>Weights and Bias</strong> Take above DNN architecture, for example, there are 3 groups of weights from the input layer to first hidden layer, first to second hidden layer and second hidden layer to output layer. Bias unit links to every hidden node and which affects the output scores, but without interacting with the actual data. In our R implementation, we represent weights and bias by the matrix. Weight size is defined by,</p>
<blockquote>
<p>  (number of neurons layer M) X (number of neurons in layer M+1)</p>
</blockquote>
<p>and weights are initialized by random number from rnorm. Bias is just a one dimension matrix with the same size of  neurons and set to zero. Other initialization approaches, such as calibrating the variances with 1/sqrt(n) and sparse initialization, are introduced in <a target="_blank" rel="noopener" href="http://cs231n.github.io/neural-networks-2/#init">weight initialization</a> part of Stanford CS231n. Pseudo R code:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">weight.i &lt;- 0.01*matrix(rnorm(layer size of (i) * layer size of (i+<span class="number">1</span>)),</span><br><span class="line">                        nrow=layer size of (i),</span><br><span class="line">                        ncol=layer size of (i+<span class="number">1</span>))</span><br><span class="line">bias.i &lt;- matrix(<span class="number">0</span>, nrow=<span class="number">1</span>, ncol = layer size of (i+<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>Another common implementation approach combines weights and bias together so that the dimension of input is N+1 which indicates N input features with 1 bias, as below code:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">weight &lt;- 0.01*matrix(rnorm((layer size of (i) +<span class="number">1</span>) * layer size of (i+<span class="number">1</span>)),</span><br><span class="line">                            nrow=layer size of (i) +<span class="number">1</span>,</span><br><span class="line">                            ncol=layer size of (i+<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p><strong>Neuron</strong> A neuron is a basic unit in the DNN which is biologically inspired model of the human neuron. A single neuron performs weight and input multiplication and addition (FMA), which is as same as the linear regression in data science, and then FMA’s result is passed to the activation function. The commonly used activation functions include <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid</a>, <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLu</a>, <a target="_blank" rel="noopener" href="https://reference.wolfram.com/language/ref/Tanh.html">Tanh</a> and Maxout. In this post, I will take the rectified linear unit (ReLU)  as activation function,  <code>f(x) = max(0, x)</code>. For other types of activation function, you can refer <a target="_blank" rel="noopener" href="http://cs231n.github.io/neural-networks-1/#actfun">here</a>. <a href="/oneXPU/uploads/2016/02/neuron.png"><img src="/oneXPU/uploads/2016/02/neuron.png" alt="neuron"></a> In R, we can implement neuron by various methods, such as <code>sum(xi*wi)</code>. But, more efficient representation is by matrix multiplication. R code:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">neuron.ij &lt;- <span class="built_in">max</span>(<span class="number">0</span>, input %*% weight + bias)</span><br></pre></td></tr></table></figure>
<p><strong><em>Implementation Tips</em></strong> _In practice, we always update all neurons in a layer with a batch of examples for performance consideration. Thus, the above code will not work correctly.</p>
<ol>
<li>Matrix Multiplication and Addition</li>
</ol>
<p>As below code shown,  <code>input %*% weights</code> and <code>bias</code> with different dimensions and  it can’t  be added directly. Two solutions are provided. The first one repeats bias_ ncol _times, however, it will waste lots of memory in big data input. Therefore, the second approach is better.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dimension: 2X2</span></span><br><span class="line">input &lt;- matrix(<span class="number">1</span>:<span class="number">4</span>, nrow=<span class="number">2</span>, ncol=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># dimension: 2x3</span></span><br><span class="line">weights &lt;- matrix(<span class="number">1</span>:<span class="number">6</span>, nrow=<span class="number">2</span>, ncol=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># dimension: 1*3</span></span><br><span class="line">bias &lt;- matrix(<span class="number">1</span>:<span class="number">3</span>, nrow=<span class="number">1</span>, ncol=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># doesn&#x27;t work since unmatched dimension</span></span><br><span class="line">input %*% weights + bias</span><br><span class="line">Error input %*% weights + bias : non-conformable arrays </span><br><span class="line"> </span><br><span class="line"><span class="comment"># solution 1: repeat bias aligned to 2X3 </span></span><br><span class="line">s1 &lt;- input %*% weights + matrix(<span class="built_in">rep</span>(bias, each=<span class="number">2</span>), ncol=<span class="number">3</span>) </span><br><span class="line"> </span><br><span class="line"><span class="comment"># solution 2: sweep addition</span></span><br><span class="line">s2 &lt;- sweep(input %*% weights ,<span class="number">2</span>, bias, <span class="string">&#x27;+&#x27;</span>)</span><br><span class="line"> </span><br><span class="line">all.equal(s1, s2)</span><br><span class="line"><span class="comment"># [1] TRUE</span></span><br></pre></td></tr></table></figure>
<ol start="2">
<li>Element-wise max value for a matrix</li>
</ol>
<p>Another trick in here is to replace <code>max</code> by <code>pmax</code> to get element-wise maximum value instead of a global one, and be careful of the order in <code>pmax</code> :)</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the original matrix</span></span><br><span class="line">&gt; s1</span><br><span class="line">     [,<span class="number">1</span>] [,<span class="number">2</span>] [,<span class="number">3</span>]</span><br><span class="line">[<span class="number">1</span>,]    <span class="number">8</span>   <span class="number">17</span>   <span class="number">26</span></span><br><span class="line">[<span class="number">2</span>,]   <span class="number">11</span>   <span class="number">24</span>   <span class="number">37</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># max returns global maximum </span></span><br><span class="line">&gt; <span class="built_in">max</span>(<span class="number">0</span>, s1)</span><br><span class="line">[<span class="number">1</span>] <span class="number">37</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># s1 is aligned with a scalar, so the matrix structure is lost</span></span><br><span class="line">&gt; pmax(<span class="number">0</span>, s1)</span><br><span class="line">[<span class="number">1</span>]  <span class="number">8</span> <span class="number">11</span> <span class="number">17</span> <span class="number">24</span> <span class="number">26</span> <span class="number">37</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># correct </span></span><br><span class="line"><span class="comment"># put matrix in the first, the scalar will be recycled to match matrix structure</span></span><br><span class="line">&gt; pmax(s1, <span class="number">0</span>)</span><br><span class="line">     [,<span class="number">1</span>] [,<span class="number">2</span>] [,<span class="number">3</span>]</span><br><span class="line">[<span class="number">1</span>,]    <span class="number">8</span>   <span class="number">17</span>   <span class="number">26</span></span><br><span class="line">[<span class="number">2</span>,]   <span class="number">11</span>   <span class="number">24</span>   <span class="number">37</span></span><br></pre></td></tr></table></figure>
<p><strong>Layer</strong></p>
<ul>
<li>Input Layer</li>
</ul>
<p>the input layer is relatively fixed with only 1 layer and the unit number is equivalent to the number of features in the input data.</p>
<ul>
<li>Hidden layers</li>
</ul>
<p>Hidden layers are very various and it’s the core component in DNN. But in general,  more hidden layers are needed to capture desired patterns in case the problem is more complex (non-linear).</p>
<ul>
<li>Output Layer</li>
</ul>
<p>The unit in output layer most commonly does not have an activation because it is usually taken to represent the class scores in classification and arbitrary real-valued numbers in regression. For classification, the number of output units matches the number of categories of prediction while there is only one output node for regression.  </p>
<h2 id="Build-Neural-Network-Architecture-Prediction-and-Training"><a href="#Build-Neural-Network-Architecture-Prediction-and-Training" class="headerlink" title="Build Neural Network: Architecture, Prediction, and Training"></a>Build Neural Network: Architecture, Prediction, and Training</h2><p>Till now, we have covered the basic concepts of deep neural network and we are going to build a neural network now, which includes determining the network architecture, training network and then predict new data with the learned network. To make things simple, we use a small data set, Edgar Anderson’s Iris Data (<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Iris_flower_data_set">iris</a>) to do classification by DNN.</p>
<p><strong>Network Architecture</strong></p>
<p>IRIS is well-known built-in dataset in stock R for machine learning. So you can take a look at this dataset by the <code>summary</code> at the console directly as below.</p>
<p>R code:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">summary(iris)</span><br><span class="line">  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width          Species  </span><br><span class="line"> Min.   :<span class="number">4.300</span>   Min.   :<span class="number">2.000</span>   Min.   :<span class="number">1.000</span>   Min.   :<span class="number">0.100</span>   setosa    :<span class="number">50</span>  </span><br><span class="line"> <span class="number">1</span>st Qu.:<span class="number">5.100</span>   <span class="number">1</span>st Qu.:<span class="number">2.800</span>   <span class="number">1</span>st Qu.:<span class="number">1.600</span>   <span class="number">1</span>st Qu.:<span class="number">0.300</span>   versicolor:<span class="number">50</span>  </span><br><span class="line"> Median :<span class="number">5.800</span>   Median :<span class="number">3.000</span>   Median :<span class="number">4.350</span>   Median :<span class="number">1.300</span>   virginica :<span class="number">50</span>  </span><br><span class="line"> Mean   :<span class="number">5.843</span>   Mean   :<span class="number">3.057</span>   Mean   :<span class="number">3.758</span>   Mean   :<span class="number">1.199</span>                  </span><br><span class="line"> <span class="number">3</span>rd Qu.:<span class="number">6.400</span>   <span class="number">3</span>rd Qu.:<span class="number">3.300</span>   <span class="number">3</span>rd Qu.:<span class="number">5.100</span>   <span class="number">3</span>rd Qu.:<span class="number">1.800</span>                  </span><br><span class="line"> Max.   :<span class="number">7.900</span>   Max.   :<span class="number">4.400</span>   Max.   :<span class="number">6.900</span>   Max.   :<span class="number">2.500</span></span><br></pre></td></tr></table></figure>
<p>From the summary, there are four features and three categories of Species. So we can design a DNN architecture as below. <a target="_blank" rel="noopener" href="http://www.parallelr.com/r-deep-neural-network-from-scratch/iris_network/"><img src="/oneXPU/uploads/2016/02/iris_network.png" alt="iris_network"></a>   And then we will keep our DNN model in a list, which can be used for retrain or prediction, as below. Actually, we can keep more interesting parameters in the model with great flexibility.</p>
<p>R code:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">List of <span class="number">7</span></span><br><span class="line"> $ D : int <span class="number">4</span></span><br><span class="line"> $ H : num <span class="number">6</span></span><br><span class="line"> $ K : int <span class="number">3</span></span><br><span class="line"> $ W1: num [<span class="number">1</span>:<span class="number">4</span>, <span class="number">1</span>:<span class="number">6</span>] <span class="number">1.34994</span> <span class="number">1.11369</span> -<span class="number">0.57346</span> -<span class="number">1.12123</span> -<span class="number">0.00107</span> ...</span><br><span class="line"> $ b1: num [<span class="number">1</span>, <span class="number">1</span>:<span class="number">6</span>] <span class="number">1.336621</span> -<span class="number">0.509689</span> -<span class="number">0.000277</span> -<span class="number">0.473194</span> <span class="number">0</span> ...</span><br><span class="line"> $ W2: num [<span class="number">1</span>:<span class="number">6</span>, <span class="number">1</span>:<span class="number">3</span>] <span class="number">1.31464</span> -<span class="number">0.92211</span> -<span class="number">0.00574</span> -<span class="number">0.82909</span> <span class="number">0.00312</span> ...</span><br><span class="line"> $ b2: num [<span class="number">1</span>, <span class="number">1</span>:<span class="number">3</span>] <span class="number">0.581</span> <span class="number">0.506</span> -<span class="number">1.088</span></span><br></pre></td></tr></table></figure>
<p><strong>Prediction</strong> Prediction, also called classification or inference in machine learning field, is concise compared with training, which walks through the network layer by layer from input to output by matrix multiplication. In output layer, the activation function doesn’t need. And for classification, the probabilities will be calculated by <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a>  while for regression the output represents the real value of predicted.   This process is called feed forward or feed propagation.</p>
<p>R code:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Prediction</span></span><br><span class="line">predict.dnn &lt;- <span class="keyword">function</span>(model, data = X.test) &#123;</span><br><span class="line">  <span class="comment"># new data, transfer to matrix</span></span><br><span class="line">  new.data &lt;- data.matrix(data)</span><br><span class="line"> </span><br><span class="line">  <span class="comment"># Feed Forwad</span></span><br><span class="line">  hidden.layer &lt;- sweep(new.data %*% model$W1 ,<span class="number">2</span>, model$b1, <span class="string">&#x27;+&#x27;</span>)</span><br><span class="line">  <span class="comment"># neurons : Rectified Linear</span></span><br><span class="line">  hidden.layer &lt;- pmax(hidden.layer, <span class="number">0</span>)</span><br><span class="line">  score &lt;- sweep(hidden.layer %*% model$W2, <span class="number">2</span>, model$b2, <span class="string">&#x27;+&#x27;</span>)</span><br><span class="line"> </span><br><span class="line">  <span class="comment"># Loss Function: softmax</span></span><br><span class="line">  score.exp &lt;- <span class="built_in">exp</span>(score)</span><br><span class="line">  probs &lt;-sweep(score.exp, <span class="number">1</span>, rowSums(score.exp), <span class="string">&#x27;/&#x27;</span>) </span><br><span class="line"> </span><br><span class="line">  <span class="comment"># select max possiblity</span></span><br><span class="line">  labels.predicted &lt;- max.col(probs)</span><br><span class="line">  <span class="built_in">return</span>(labels.predicted)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Training</strong></p>
<p>Training is to search the optimization parameters (weights and bias) under the given network architecture and minimize the classification error or residuals.  This process includes two parts: feed forward and back propagation. Feed forward is going through the network with input data (as prediction parts) and then compute data loss in the output layer by <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Loss_function">loss function</a> (cost function). “<em>Data loss</em> measures the compatibility between a prediction (e.g. the class scores in classification) and the ground truth label.” In our example code, we selected cross-entropy function to evaluate data loss, see detail in <a target="_blank" rel="noopener" href="http://cs231n.github.io/neural-networks-2/#losses">here</a>. After getting data loss, we need to minimize the data loss by changing the weights and bias. The very popular method is to back-propagate the loss into every layers and neuron by <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a> or  <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">stochastic gradient descent</a> which requires derivatives of data loss for each parameter (W1, W2, b1, b2). And <a target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-08-Backprop/">back propagation</a> will be different for different activation functions and see <a target="_blank" rel="noopener" href="http://mochajl.readthedocs.org/en/latest/user-guide/neuron.html">here</a> and <a target="_blank" rel="noopener" href="http://like.silk.to/studymemo/ChainRuleNeuralNetwork.pdf">here</a> for their derivatives formula and method, and <a target="_blank" rel="noopener" href="http://cs231n.github.io/neural-networks-3/">Stanford CS231n</a> for more training tips. In our example, the point-wise derivative for ReLu is: <a target="_blank" rel="noopener" href="http://www.parallelr.com/r-deep-neural-network-from-scratch/class-relu/"><img src="/oneXPU/uploads/2016/02/class.relu_.png" alt="class.relu"></a></p>
<p>R code:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train: build and train a 2-layers neural network </span></span><br><span class="line">train.dnn &lt;- <span class="keyword">function</span>(x, y, traindata=data, testdata=<span class="literal">NULL</span>,</span><br><span class="line">                  <span class="comment"># set hidden layers and neurons</span></span><br><span class="line">                  <span class="comment"># currently, only support 1 hidden layer</span></span><br><span class="line">                  hidden=<span class="built_in">c</span>(<span class="number">6</span>), </span><br><span class="line">                  <span class="comment"># max iteration steps</span></span><br><span class="line">                  maxit=<span class="number">2000</span>,</span><br><span class="line">                  <span class="comment"># delta loss </span></span><br><span class="line">                  abstol=<span class="number">1e-2</span>,</span><br><span class="line">                  <span class="comment"># learning rate</span></span><br><span class="line">                  lr = <span class="number">1e-2</span>,</span><br><span class="line">                  <span class="comment"># regularization rate</span></span><br><span class="line">                  reg = <span class="number">1e-3</span>,</span><br><span class="line">                  <span class="comment"># show results every &#x27;display&#x27; step</span></span><br><span class="line">                  display = <span class="number">100</span>,</span><br><span class="line">                  random.seed = <span class="number">1</span>)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment"># to make the case reproducible.</span></span><br><span class="line">  set.seed(random.seed)</span><br><span class="line"> </span><br><span class="line">  <span class="comment"># total number of training set</span></span><br><span class="line">  N &lt;- nrow(traindata)</span><br><span class="line"> </span><br><span class="line">  <span class="comment"># extract the data and label</span></span><br><span class="line">  <span class="comment"># don&#x27;t need atribute </span></span><br><span class="line">  X &lt;- unname(data.matrix(traindata[,x]))</span><br><span class="line">  Y &lt;- traindata[,y]</span><br><span class="line">  <span class="keyword">if</span>(is.factor(Y)) &#123; Y &lt;- <span class="built_in">as.integer</span>(Y) &#125;</span><br><span class="line">  <span class="comment"># updated: 10.March.2016: create index for both row and col</span></span><br><span class="line">  Y.len   &lt;- <span class="built_in">length</span>(unique(Y))</span><br><span class="line">  Y.set   &lt;- sort(unique(Y))</span><br><span class="line">  Y.index &lt;- cbind(<span class="number">1</span>:N, match(Y, Y.set))</span><br><span class="line"> </span><br><span class="line">  <span class="comment"># number of input features</span></span><br><span class="line">  D &lt;- ncol(X)</span><br><span class="line">  <span class="comment"># number of categories for classification</span></span><br><span class="line">  K &lt;- <span class="built_in">length</span>(unique(Y))</span><br><span class="line">  H &lt;-  hidden</span><br><span class="line"> </span><br><span class="line">  <span class="comment"># create and init weights and bias </span></span><br><span class="line">  W1 &lt;- 0.01*matrix(rnorm(D*H), nrow=D, ncol=H)</span><br><span class="line">  b1 &lt;- matrix(<span class="number">0</span>, nrow=<span class="number">1</span>, ncol=H)</span><br><span class="line"> </span><br><span class="line">  W2 &lt;- 0.01*matrix(rnorm(H*K), nrow=H, ncol=K)</span><br><span class="line">  b2 &lt;- matrix(<span class="number">0</span>, nrow=<span class="number">1</span>, ncol=K)</span><br><span class="line"> </span><br><span class="line">  <span class="comment"># use all train data to update weights since it&#x27;s a small dataset</span></span><br><span class="line">  batchsize &lt;- N</span><br><span class="line">  <span class="comment"># updated: March 17. 2016</span></span><br><span class="line">  <span class="comment"># init loss to a very big value</span></span><br><span class="line">  loss &lt;- 100000</span><br><span class="line"> </span><br><span class="line">  <span class="comment"># Training the network</span></span><br><span class="line">  i &lt;- 0</span><br><span class="line">  <span class="keyword">while</span>(i &lt; maxit &amp;&amp; loss &gt; abstol ) &#123;</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># iteration index</span></span><br><span class="line">    i &lt;- i +<span class="number">1</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># forward ....</span></span><br><span class="line">    <span class="comment"># 1 indicate row, 2 indicate col</span></span><br><span class="line">    hidden.layer &lt;- sweep(X %*% W1 ,<span class="number">2</span>, b1, <span class="string">&#x27;+&#x27;</span>)</span><br><span class="line">    <span class="comment"># neurons : ReLU</span></span><br><span class="line">    hidden.layer &lt;- pmax(hidden.layer, <span class="number">0</span>)</span><br><span class="line">    score &lt;- sweep(hidden.layer %*% W2, <span class="number">2</span>, b2, <span class="string">&#x27;+&#x27;</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># softmax</span></span><br><span class="line">    score.exp &lt;- <span class="built_in">exp</span>(score)</span><br><span class="line">    probs &lt;-sweep(score.exp, <span class="number">1</span>, rowSums(score.exp), <span class="string">&#x27;/&#x27;</span>) </span><br><span class="line"> </span><br><span class="line">    <span class="comment"># compute the loss</span></span><br><span class="line">    corect.logprobs &lt;- -<span class="built_in">log</span>(probs[Y.index])</span><br><span class="line">    data.loss  &lt;- <span class="built_in">sum</span>(corect.logprobs)/batchsize</span><br><span class="line">    reg.loss   &lt;- 0.5*reg* (<span class="built_in">sum</span>(W1*W1) + <span class="built_in">sum</span>(W2*W2))</span><br><span class="line">    loss &lt;- data.loss + reg.loss</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># display results and update model</span></span><br><span class="line">    <span class="keyword">if</span>( i %% display == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span>(!<span class="built_in">is.null</span>(testdata)) &#123;</span><br><span class="line">            model &lt;- <span class="built_in">list</span>( D = D,</span><br><span class="line">                           H = H,</span><br><span class="line">                           K = K,</span><br><span class="line">                           <span class="comment"># weights and bias</span></span><br><span class="line">                           W1 = W1, </span><br><span class="line">                           b1 = b1, </span><br><span class="line">                           W2 = W2, </span><br><span class="line">                           b2 = b2)</span><br><span class="line">            labs &lt;- predict.dnn(model, testdata[,-y])      </span><br><span class="line">            <span class="comment"># updated: 10.March.2016</span></span><br><span class="line">            accuracy &lt;- mean(<span class="built_in">as.integer</span>(testdata[,y]) == Y.set[labs])</span><br><span class="line">            cat(i, loss, accuracy, <span class="string">&quot;\n&quot;</span>)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            cat(i, loss, <span class="string">&quot;\n&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># backward ....</span></span><br><span class="line">    dscores &lt;- probs</span><br><span class="line">    dscores[Y.index] &lt;- dscores[Y.index] -<span class="number">1</span></span><br><span class="line">    dscores &lt;- dscores / batchsize</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    dW2 &lt;- t(hidden.layer) %*% dscores </span><br><span class="line">    db2 &lt;- colSums(dscores)</span><br><span class="line"> </span><br><span class="line">    dhidden &lt;- dscores %*% t(W2)</span><br><span class="line">    dhidden[hidden.layer &lt;= <span class="number">0</span>] &lt;- <span class="number">0</span></span><br><span class="line"> </span><br><span class="line">    dW1 &lt;- t(X) %*% dhidden</span><br><span class="line">    db1 &lt;- colSums(dhidden) </span><br><span class="line"> </span><br><span class="line">    <span class="comment"># update ....</span></span><br><span class="line">    dW2 &lt;- dW2 + reg*W2</span><br><span class="line">    dW1 &lt;- dW1  + reg*W1</span><br><span class="line"> </span><br><span class="line">    W1 &lt;- W1 - lr * dW1</span><br><span class="line">    b1 &lt;- b1 - lr * db1</span><br><span class="line"> </span><br><span class="line">    W2 &lt;- W2 - lr * dW2</span><br><span class="line">    b2 &lt;- b2 - lr * db2</span><br><span class="line"> </span><br><span class="line">  &#125;</span><br><span class="line"> </span><br><span class="line">  <span class="comment"># final results</span></span><br><span class="line">  <span class="comment"># creat list to store learned parameters</span></span><br><span class="line">  <span class="comment"># you can add more parameters for debug and visualization</span></span><br><span class="line">  <span class="comment"># such as residuals, fitted.values ...</span></span><br><span class="line">  model &lt;- <span class="built_in">list</span>( D = D,</span><br><span class="line">                 H = H,</span><br><span class="line">                 K = K,</span><br><span class="line">                 <span class="comment"># weights and bias</span></span><br><span class="line">                 W1= W1, </span><br><span class="line">                 b1= b1, </span><br><span class="line">                 W2= W2, </span><br><span class="line">                 b2= b2)</span><br><span class="line"> </span><br><span class="line">  <span class="built_in">return</span>(model)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Testing-and-Visualization"><a href="#Testing-and-Visualization" class="headerlink" title="Testing and Visualization"></a>Testing and Visualization</h2><p>We have built the simple 2-layers DNN model and now we can test our model. First, the dataset is split into two parts for training and testing, and then use the training set to train model while testing set to measure the generalization ability of our model.</p>
<p>R code</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">########################################################################</span></span><br><span class="line"><span class="comment"># testing</span></span><br><span class="line"><span class="comment">#######################################################################</span></span><br><span class="line">set.seed(<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 0. EDA</span></span><br><span class="line">summary(iris)</span><br><span class="line">plot(iris)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 1. split data into test/train</span></span><br><span class="line">samp &lt;- <span class="built_in">c</span>(sample(<span class="number">1</span>:<span class="number">50</span>,<span class="number">25</span>), sample(<span class="number">51</span>:<span class="number">100</span>,<span class="number">25</span>), sample(<span class="number">101</span>:<span class="number">150</span>,<span class="number">25</span>))</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 2. train model</span></span><br><span class="line">ir.model &lt;- train.dnn(x=<span class="number">1</span>:<span class="number">4</span>, y=<span class="number">5</span>, traindata=iris[samp,], testdata=iris[-samp,], hidden=<span class="number">6</span>, maxit=<span class="number">2000</span>, display=<span class="number">50</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 3. prediction</span></span><br><span class="line">labels.dnn &lt;- predict.dnn(ir.model, iris[-samp, -<span class="number">5</span>])</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 4. verify the results</span></span><br><span class="line">table(iris[-samp,<span class="number">5</span>], labels.dnn)</span><br><span class="line"><span class="comment">#          labels.dnn</span></span><br><span class="line"><span class="comment">#            1  2  3</span></span><br><span class="line"><span class="comment">#setosa     25  0  0</span></span><br><span class="line"><span class="comment">#versicolor  0 24  1</span></span><br><span class="line"><span class="comment">#virginica   0  0 25</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#accuracy</span></span><br><span class="line">mean(<span class="built_in">as.integer</span>(iris[-samp, <span class="number">5</span>]) == labels.dnn)</span><br><span class="line"><span class="comment"># 0.98</span></span><br></pre></td></tr></table></figure>
<p>The data loss in train set and the accuracy in test as below: <a target="_blank" rel="noopener" href="http://www.parallelr.com/r-deep-neural-network-from-scratch/iris_loss_accuracy-2/"><img src="/oneXPU/uploads/2016/02/iris_loss_accuracy-1.png"></a> Then we compare our DNN model with ‘nnet’ package as below codes.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">library(nnet)</span><br><span class="line">ird &lt;- data.frame(rbind(iris3[,,<span class="number">1</span>], iris3[,,<span class="number">2</span>], iris3[,,<span class="number">3</span>]),</span><br><span class="line">                  species = factor(<span class="built_in">c</span>(<span class="built_in">rep</span>(<span class="string">&quot;s&quot;</span>,<span class="number">50</span>), <span class="built_in">rep</span>(<span class="string">&quot;c&quot;</span>, <span class="number">50</span>), <span class="built_in">rep</span>(<span class="string">&quot;v&quot;</span>, <span class="number">50</span>))))</span><br><span class="line">ir.nn2 &lt;- nnet(species ~ ., data = ird, subset = samp, size = <span class="number">6</span>, rang = <span class="number">0.1</span>,</span><br><span class="line">               decay = <span class="number">1e-2</span>, maxit = <span class="number">2000</span>)</span><br><span class="line"> </span><br><span class="line">labels.nnet &lt;- predict(ir.nn2, ird[-samp,], type=<span class="string">&quot;class&quot;</span>)</span><br><span class="line">table(ird$species[-samp], labels.nnet)</span><br><span class="line"><span class="comment">#  labels.nnet</span></span><br><span class="line"><span class="comment">#   c  s  v</span></span><br><span class="line"><span class="comment">#c 22  0  3</span></span><br><span class="line"><span class="comment">#s  0 25  0</span></span><br><span class="line"><span class="comment">#v  3  0 22</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># accuracy</span></span><br><span class="line">mean(ird$species[-samp] == labels.nnet)</span><br><span class="line"><span class="comment"># 0.96</span></span><br></pre></td></tr></table></figure>
<p><strong>Update: 4/28/2016:</strong></p>
<p>Google’s tensorflow released a very cool website to visualize neural network in <a target="_blank" rel="noopener" href="http://playground.tensorflow.org/">here</a>. And we has taught almost all technologies as google’s website in this blog so you can build up with R as well :) <a target="_blank" rel="noopener" href="http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.86675&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification"><img src="/oneXPU/uploads/2016/02/tensorflow.png" alt="tensorflow"></a>  </p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>In this post, we have shown how to implement R neural network from scratch. But the code is only implemented the core concepts of DNN, and the reader can do further practices by:</p>
<ul>
<li>  Solving other classification problem, such as a toy case in <a target="_blank" rel="noopener" href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/">here</a></li>
<li>  Selecting various hidden layer size, activation function, loss function</li>
<li>  Extending single hidden layer network to multi-hidden layers</li>
<li>  Adjusting the network to resolve regression problems</li>
<li>  Visualizing the network architecture, weights, and bias by R, an example in <a target="_blank" rel="noopener" href="https://beckmw.wordpress.com/tag/nnet/">here</a>.</li>
</ul>
<p><strong>In the next <a target="_blank" rel="noopener" href="http://www.parallelr.com/r-dnn-parallel-acceleration/">post</a>, I will introduce how to accelerate this code by multicores CPU and NVIDIA GPU.</strong></p>
<hr>
<p>Notes: 1. The entire source code of this post in <a target="_blank" rel="noopener" href="https://github.com/PatricZhao/ParallelR/blob/master/ParDNN/iris_dnn.R">here</a> 2. The PDF version of this post in <a target="_blank" rel="noopener" href="http://www.parallelr.com/materials/2_DNN/ParallelR_R_DNN_1_BUILD_FROM_SCRATCH.pdf">here</a> 3. Pretty R syntax in this blog is <a target="_blank" rel="noopener" href="http://blog.revolutionanalytics.com/2016/08/farewell-inside-rorg.html" title="Created by Pretty R at inside-R.org">Created by inside-R .org</a> (acquired by MS and dead)</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/oneXPU/tags/rstats/" rel="tag"># rstats</a>
              <a href="/oneXPU/tags/R/" rel="tag"># R</a>
              <a href="/oneXPU/tags/deep-learning/" rel="tag"># deep learning</a>
              <a href="/oneXPU/tags/machine-learning/" rel="tag"># machine learning</a>
              <a href="/oneXPU/tags/dnn/" rel="tag"># dnn</a>
              <a href="/oneXPU/tags/data-analytics/" rel="tag"># data analytics</a>
              <a href="/oneXPU/tags/big-data/" rel="tag"># big data</a>
              <a href="/oneXPU/tags/iris/" rel="tag"># iris</a>
              <a href="/oneXPU/tags/neural-network/" rel="tag"># neural network</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/oneXPU/2016/02/02/the-r-parallel-programming-blog/" rel="prev" title="The R Parallel Programming Blog">
      <i class="fa fa-chevron-left"></i> The R Parallel Programming Blog
    </a></div>
      <div class="post-nav-item">
    <a href="/oneXPU/2016/03/08/r-dnn-parallel-acceleration/" rel="next" title="R for Deep Learning (II): Achieve High-Performance DNN with Parallel Acceleration">
      R for Deep Learning (II): Achieve High-Performance DNN with Parallel Acceleration <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Backgrounds"><span class="nav-number">1.</span> <span class="nav-text">Backgrounds</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fundamental-Concepts-and-Components"><span class="nav-number">2.</span> <span class="nav-text">Fundamental Concepts and Components</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Build-Neural-Network-Architecture-Prediction-and-Training"><span class="nav-number">3.</span> <span class="nav-text">Build Neural Network: Architecture, Prediction, and Training</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Testing-and-Visualization"><span class="nav-number">4.</span> <span class="nav-text">Testing and Visualization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Summary"><span class="nav-number">5.</span> <span class="nav-text">Summary</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Patric Zhao</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-categories">
            <a href="/oneXPU/categories/">
          
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/oneXPU/tags/">
          
        <span class="site-state-item-count">56</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Patric Zhao</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/oneXPU/lib/anime.min.js"></script>
  <script src="/oneXPU/lib/velocity/velocity.min.js"></script>
  <script src="/oneXPU/lib/velocity/velocity.ui.min.js"></script>

<script src="/oneXPU/js/utils.js"></script>

<script src="/oneXPU/js/motion.js"></script>


<script src="/oneXPU/js/schemes/muse.js"></script>


<script src="/oneXPU/js/next-boot.js"></script>




  















  

  

</body>
</html>

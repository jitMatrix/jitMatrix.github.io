---
title: test
url: 386.html
id: 386
categories:
  - Uncategorized
date: 2016-02-19 05:07:04
tags:
---

**Recurrent-Neural-Network(RNN) gives state-of-the-art results in automatic speech recognition, stock market prediction, handwriting recognition and other sequence learning problems. In RNN layer, weight matrix is shared among all timesteps, but general cuda-based RNN implementations don't utilize this characteristic and weight matrix has to be loaded every recurrent step. Especially in inference stage, when batch size is small, the performance of RNN is limited by memory bandwidth. In this talk, I will introduce a persistent thread method to accelerate RNN on GPU. In this method, whole weight matrix is stored into GPU registers to avoid repeated memory loads, and a CTA spinlock is implemented to achieve global synchronization. The result shows significant speedup on Maxwell architecture.** Most researchers uses cuBLAS to implement RNN on GPU. But unlike DNN, data dependent behavior of RNN makes it hard to achieve peak utilization, which make RNN become major bottleneck in practical applications. The basic equations representing one forward stage of a RNN layer from timestep t-1 to timestep t is: h\_t = g(W\_r * h\_t-1 + b\_t) (1) b\_t is the result at time t in pervious layer; h\_t-1 is the result at timestep t-1 in RNN layer; weight matrix W\_r is shared among all timesteps in RNN layer. In RNN layer, the result of h\_t depends on the result of h\_t-1. If total timesteps size is N, the general RNN implementations need call GEMV/GEMM(call GEMV, if batch size is 1; or call GEMM, if batch size >1) N times. General implementations doesn't utilize the characteristic that weight matrix W\_r is shared among all timesteps, so weight matrix has to be loaded N times(usually N > 100). When batch size is small, GEMM performance memory is limited by memory bandwidth. If we can keep weight matrix into SM registers, we can reduce hundreds of times weight matrix loads and change performance limitation from memory bandwidth to compute throughput. In this talk, I will present a persistent thread method to speedup RNN layer. My implementation can keep 1152\*1152\*4B=5.0625MB size weight matrix into registers, which takes 84.4% of total register size of GeForce GTX TITAN X(6MB). A CTA spinlock is used to achieve global synchronization. To avoid deadlock, all warps must be active. So occupancy is limited to 8 warps per SM, and each thread can use up to 256 registers (Finally, each thread uses 253 registers, which 216 registers are used to store weights and 37 registers are used to control). The baseline implementation is executing 1152\*1152 size cublasSgemv and 1152 size vectorAdd 256 times on GeForce GTX TITAN X. (1152\*1152 size cublasSgemv can achieve 52% memory bandwidth). The result shows persistent thread method can achieve 15X speedup on GeForce GTX TITAN X. If using fp16, we can keep 1152*sqrt(2) weights. If GPU had more registers, we could support RNN with larger weight matrix. Future works: 1) make persistent thread method support batch size >1 and 2) spread this method to GRU and general LSTM network.
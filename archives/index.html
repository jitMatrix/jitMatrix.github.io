<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/oneXPU/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/oneXPU/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/oneXPU/oneXPU/uploads/favicon/favicon.ico">
  <link rel="mask-icon" href="/oneXPU/images/logo.svg" color="#222">

<link rel="stylesheet" href="/oneXPU/css/main.css">


<link rel="stylesheet" href="/oneXPU/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jitmatrix.github.io","root":"/oneXPU/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="ParallelR">
<meta property="og:url" content="https://jitmatrix.github.io/oneXPU/archives/index.html">
<meta property="og:site_name" content="ParallelR">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Patric Zhao">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://jitmatrix.github.io/oneXPU/archives/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>ParallelR</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/oneXPU/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">ParallelR</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Diving into Parallel Technology</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/oneXPU/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/oneXPU/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-opensource">

    <a href="/oneXPU/opensource/" rel="section"><i class="fa fa-archive fa-fw"></i>Opensource</a>

  </li>
        <li class="menu-item menu-item-presentation">

    <a href="/oneXPU/presentation/" rel="section"><i class="fa fa-archive fa-fw"></i>Presentation</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/oneXPU/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/oneXPU/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/oneXPU/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jitmatrix.github.io/oneXPU/2017/04/07/r-hpac-benchmark-analysis-gpu/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/oneXPU/images/avatar.gif">
      <meta itemprop="name" content="Patric Zhao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ParallelR">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/oneXPU/2017/04/07/r-hpac-benchmark-analysis-gpu/" class="post-title-link" itemprop="url">R benchmark for High-Performance Analytics and Computing (II): GPU Packages</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-04-07 12:38:10" itemprop="dateCreated datePublished" datetime="2017-04-07T12:38:10+00:00">2017-04-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-19 10:35:25" itemprop="dateModified" datetime="2020-12-19T10:35:25+00:00">2020-12-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/GPGPU/" itemprop="url" rel="index"><span itemprop="name">GPGPU</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/GPGPU/Intel-Xeon-Phi/" itemprop="url" rel="index"><span itemprop="name">Intel Xeon Phi</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h1><p>In the previous post (<a target="_blank" rel="noopener" href="http://www.parallelr.com/r-hpac-benchmark-analysis/">here</a>), we have analyzed the performance gain of R in the heterogeneous system by accelerators, including NVIDIA GPU and Intel Xeon Phi. Furthermore, GPU accelerated packages can greatly improve the performance of R. Figure 1 shows the download statistics of CRAN over the years. Obviously, GPU is more and more recognized by the R community. <img src="/oneXPU/uploads/2017/03/Figure1-R-benchmark-2-1024x587.png"></p>
<p>Figure 1 Download statistics of CRAN Package applied to the GPGPU environment over the years</p>
<h1 id="2-GPU-accelerated-packages"><a href="#2-GPU-accelerated-packages" class="headerlink" title="2. GPU-accelerated packages"></a>2. GPU-accelerated packages</h1><p>Matrix operation (BLAS) is one of the most important operations in data analysis, such as co-matrices in recommended systems and convolution calculations in deep learning. The matrix and the vector multiplication and other standard operations in R can be accelerated by GPU significantly. The most simple way to use GPU in R is through the nvBLAS (cuBLAS) library provided by NVIDIA, but other GPU accelerated packages (gputools, gmatrix, gpuR) provides a richer software and hardware interface, as shown in Table 1.</p>
<p>Table 1 Function comparison of the R package supported for GPGPU</p>
<p><img src="/oneXPU/uploads/2017/03/Table1-R-benchmark-2-1024x362.png"></p>
<h3 id="2-1-Data-type"><a href="#2-1-Data-type" class="headerlink" title="2.1 Data type"></a>2.1 Data type</h3><p>Double-precision (DP) is used in nvBLAS and gputools to align with the default double precision mode of R. On the other hand, gmatrix and GPU can be more flexible for the user to choice single-precision (SP) or double-precision (DP) in the computations. And even gmatrix can support integer matrix computing. The SP computation mode can significant leverage the capability in low-mid level NVIDIA GPU cards, such as Tesla M40 and Geforce series GPUs, where the main computing power from SP floating-point operations. Therefore, this is a good news common desktop GPU users.</p>
<h3 id="2-2-Data-transfer"><a href="#2-2-Data-transfer" class="headerlink" title="2.2 Data transfer"></a>2.2 Data transfer</h3><p>When using the GPU as a coprocessor to speed up the application, the cost of data transfer usually takes a significant portion of the time, and the size of the GPU’s built-in memory will also limit whether the application can be executed. One of the main advantages of nvBLAS is that it supports block-based data copy and calculations between CPU and GPU, so the memory required from R code can be large than built-in GPU memory. However, the host-to-device memory copy, the calculation, and the final device-to-host results are performed in a synchronized mode so that the user cannot isolate the data transfer and calculation in nvBLAS. gmatrix and gpuR provide the asynchronous mode of communication and calculation, the user can separate the data copy and the real calculation. For example, gpuR provided in the vcl * series API, it will return in the R console immediately and then R will execute the next CPU command, while GPU is computing. In this way, both CPU and GPU are working simultaneously. And we can get much more performance boost.</p>
<h3 id="2-3-Programming-model"><a href="#2-3-Programming-model" class="headerlink" title="2.3 Programming model"></a>2.3 Programming model</h3><p>Frist, nvBLAS, gmatrix are based on the CUDA programming model and it will show better performance in NVIDIA series of GPUs but the shortage is poor portability. Then, gpuR is based on OpenCL, a standard heterogeneous programming interface, and more flexible. The user program of OpenCL can be executed on much more platforms, such as CPU, GPGPU, Intel Xeon Phi and FPGA.</p>
<h1 id="3-Performance-Benchmark-and-Analysis"><a href="#3-Performance-Benchmark-and-Analysis" class="headerlink" title="3. Performance Benchmark and Analysis"></a>3. Performance Benchmark and Analysis</h1><h3 id="3-1-Test-environment"><a href="#3-1-Test-environment" class="headerlink" title="3.1 Test environment"></a>3.1 Test environment</h3><p>The test is performed on the Ali cloud HPC platform: G2 server with NVIDIA Tesla K40m, G4 server with Tesla M40. Ali cloud HPC provides independent physical server + GPU accelerator card without virtualization overhead for computing-intensive applications. Regarding GPU equipment, K40m is designed with Kepler architecture while M40 with Maxwell architecture. The M40 targets for deep learning markets especially for training. Its SP floating-point peak performance reaches 7TFlops, but DP is only 0.2TFlops.</p>
<p>Table 2 Ali cloud hardware platform configuration</p>
<p><img src="/oneXPU/uploads/2017/03/Table2-R-benchmark-2-1024x351.png"></p>
<p>Table 3 List of M40 and K40m hardware parameters</p>
<p><img src="/oneXPU/uploads/2017/03/Table3-R-benchmark-2-1024x731.png"></p>
<p>Table 4 Software of test used</p>
<p><img src="/oneXPU/uploads/2017/03/Table4-R-benchmark-2-1024x275.png"></p>
<h3 id="3-2-Performance-Analysis-on-K40-for-double-precision"><a href="#3-2-Performance-Analysis-on-K40-for-double-precision" class="headerlink" title="3.2 Performance Analysis on K40 for double precision"></a>3.2 Performance Analysis on K40 for double precision</h3><p>First, let’s compare the double-precision performance of each package on the Tesla series. We use nvblas performance as the baseline and compare the calculation time of three different sizes of matrix multiplications. In the testing code as below, we only counted the execution time of core API (% <em>%, gemm, gpuMatMult) following depth analysis. \</em>R code for gpuR, gmatrix, gputools and nvblas with DP calculation mode</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">library(gpuR)</span><br><span class="line"><span class="keyword">for</span>(i <span class="keyword">in</span> seq(<span class="number">1</span>:<span class="number">7</span>)) &#123;</span><br><span class="line">  ORDER = <span class="number">256</span>*(<span class="number">2</span>^i)</span><br><span class="line">  A = matrix(rnorm(ORDER^<span class="number">2</span>), nrow=ORDER)</span><br><span class="line">  B = matrix(rnorm(ORDER^<span class="number">2</span>), nrow=ORDER)</span><br><span class="line">  gpuA = gpuMatrix(A, type=<span class="string">&quot;double&quot;</span>)</span><br><span class="line">  gpuB = gpuMatrix(B, type=<span class="string">&quot;double&quot;</span>)</span><br><span class="line">  cputime = system.time(&#123;gpuC = gpuA %*% gpuB&#125;)[<span class="number">3</span>]</span><br><span class="line"> &#125;</span><br><span class="line">  </span><br><span class="line">library(gmatrix)</span><br><span class="line"><span class="keyword">for</span>(i <span class="keyword">in</span> seq(<span class="number">1</span>:<span class="number">7</span>)) &#123;</span><br><span class="line">  ORDER = <span class="number">256</span>*(<span class="number">2</span>^i)</span><br><span class="line">  A = gmatrix(rnorm(ORDER^<span class="number">2</span>),ORDER,ORDER)</span><br><span class="line">  B = gmatrix(rnorm(ORDER^<span class="number">2</span>),ORDER,ORDER)</span><br><span class="line">  C = gmatrix(<span class="number">0</span>,ORDER,ORDER)</span><br><span class="line">  cputime = system.time(&#123;gmm(A,B,C)&#125;)[<span class="number">3</span>]</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">library(gputools)</span><br><span class="line"><span class="keyword">for</span>(i <span class="keyword">in</span> seq(<span class="number">1</span>:<span class="number">7</span>)) &#123;</span><br><span class="line">  ORDER = <span class="number">256</span>*(<span class="number">2</span>^i)</span><br><span class="line">  A = matrix(rnorm(ORDER^<span class="number">2</span>), nrow=ORDER)</span><br><span class="line">  B = matrix(rnorm(ORDER^<span class="number">2</span>), nrow=ORDER)</span><br><span class="line">  cputime = system.time(&#123;C = gpuMatMult(A, B)&#125;)[<span class="number">3</span>]</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment"># nvblas, native code + PRE_LOADED</span></span><br><span class="line"><span class="keyword">for</span>(i <span class="keyword">in</span> seq(<span class="number">1</span>:<span class="number">7</span>)) &#123;</span><br><span class="line">  ORDER = <span class="number">256</span>*(<span class="number">2</span>^i)</span><br><span class="line">  A = matrix(rnorm(ORDER^<span class="number">2</span>), nrow=ORDER) </span><br><span class="line">  B = matrix(rnorm(ORDER^<span class="number">2</span>), nrow=ORDER) </span><br><span class="line">  cputime = system.time(&#123;C = A %*% B&#125;)[<span class="number">3</span>] </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="/oneXPU/uploads/2017/03/Figure2-R-benchmark-2-1024x557.png"></p>
<p>Figure 2 Performance of the software package with the size change</p>
<p>In general, nvblas, gputools and gmatrix show very similar performance, because they use cuBLAS as the backend. gpuR’s performance is relatively low and variety with input sizes, such as the 4096 matrix only achieves 20% of nvblas performance but 8192 matrices can reach ~70%. From computation pattern, gputools and gmatrix apply dgemm_sm_heavy_ldg_nn API interfaces of cuBLAS to complete the matrix calculations, and computational efficiency will be slightly higher than nvblas of the block matrix calculation mode. From memory usage, as Figure 2, nvblas is the only one able to complete the large memory (out of cores/memory) calculation. For the largest matrix 32768, GPU packages (gputools, gmatrix, gpuR) will throw an exception of memory overflow. More details In Table 5, input matrix are divided into many small pieces, and then are transmitted to the GPU for computations by nvblas.</p>
<p>Table 5. Analysis of memory copy times from nvprof</p>
<p><img src="/oneXPU/uploads/2017/03/Table5-R-benchmark-2-1024x198.png"> For gmatrix, matrix (A, B, C for C = A*B) are copied to GPU and C matrix stored in the GPU side after the calculation, involving three times host-to-device data transfer and without device-to-host transfer. For gputools matrix (A, B) are copied to GPU, the result matrix ( C ) is copied back to the host side so totally twice host-to-device and once device-to-host data transfer. Because the host-to-device data transfer is faster than device-to-host, gmatrix could get better performance than gputools as table 6 shown. Finally, we take a look at gpuR performance. The matrix calculation leverages OpenCL API that the performance is less optimized on NVIDIA GPU in table 6. GEMM compute kernel _prod_TT is much slower than gputools and gmatrix. Take 8192 for example, the calculation time of cublas API is 911.4 ms and 912.3 ms for gputools and gmatrix while OpenCL is 2172.5 ms for gpuR.</p>
<p>Table 6 Time overhead on GPU side at matrix size of 8192 * 8192</p>
<p><img src="/oneXPU/uploads/2017/03/Table6-R-benchmark-2-1024x277.png"></p>
<h3 id="3-3-Performance-Analysis-on-M40-for-single-precision"><a href="#3-3-Performance-Analysis-on-M40-for-single-precision" class="headerlink" title="3.3 Performance Analysis on M40 for single precision"></a>3.3 Performance Analysis on M40 for single precision</h3><p>Single precision is quite important for data scientists but openBLAS, nvblas, and gputools use default double-precision (DP) calculation mode of R. So, it will lack competition in some hardware such as Tesla M40 where the DP performance is only 0.2T. In this parts, we will show you how to leverage SP performance in R by gmatrix and gpuR. In the blow testing, we take openBLAS performance results as the baseline. *R code of gmatrix and gpuR with SP calculation mode</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">library(gpuR)</span><br><span class="line"><span class="keyword">for</span>(i <span class="keyword">in</span> seq(<span class="number">1</span>:<span class="number">7</span>)) &#123;</span><br><span class="line">  ORDER = <span class="number">256</span>*(<span class="number">2</span>^i)</span><br><span class="line">  A = matrix(rnorm(ORDER^<span class="number">2</span>), nrow=ORDER)</span><br><span class="line">  B = matrix(rnorm(ORDER^<span class="number">2</span>), nrow=ORDER)</span><br><span class="line">  gpuA = gpuMatrix(A, type=<span class="string">&quot;float&quot;</span>)</span><br><span class="line">  gpuB = gpuMatrix(B, type=<span class="string">&quot;float&quot;</span>)</span><br><span class="line">  cputime = system.time(&#123;gpuC = gpuA %*% gpuB&#125;)[<span class="number">3</span>]</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">library(gmatrix)</span><br><span class="line"><span class="keyword">for</span>(i <span class="keyword">in</span> seq(<span class="number">1</span>:<span class="number">7</span>)) &#123;</span><br><span class="line">  ORDER = <span class="number">256</span>*(<span class="number">2</span>^i)</span><br><span class="line">  A = gmatrix(rnorm(ORDER^<span class="number">2</span>),ORDER,ORDER, type=<span class="string">&quot;single&quot;</span>)</span><br><span class="line">  B = gmatrix(rnorm(ORDER^<span class="number">2</span>),ORDER,ORDER, type=<span class="string">&quot;single&quot;</span>)</span><br><span class="line">  C = gmatrix(<span class="number">0</span>,ORDER,ORDER, type=<span class="string">&quot;single&quot;</span>)</span><br><span class="line">  cputime = system.time(&#123;</span><br><span class="line">    gmm(A,B,C);</span><br><span class="line">    h(C);</span><br><span class="line">  &#125;)[<span class="number">3</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>In Figure 3, gmatrix and gpuR with SP calculation model show a very good performance boost. For the 4096 matrix size, gmatrix is <strong>18X faster</strong> than openBLAS and <strong>37X faster</strong> (18.22 / 0.51) than nvblas. <img src="/oneXPU/uploads/2017/03/Figure3-R-benchmark-2-1024x634.png"></p>
<p>Figure 3 Performance with SP mode on M40</p>
<p>More details in Table 7, it is obvious that the computation time of SP is much less than the calculation time of DP. The calculation time of DP is about 6000 ms (nvblas, gputools), while the calculation time of SP is only about 200 ms (gmatrix) and 500 ms (gpuR). From the memory point of view, gpuR on CPU uses SP data type and gmatrix on CPU is still DP. From Table 7, we can see that memory transfer time of gputools and gmatrix is almost same, and gpuR memory transfer time is only half of it (gmatrix 153.4 ms .vs. gpuR 77.7 ms). So, gpuR are more efficient in memory usage for SP and will good for the small size of computations. <em>Note, gmatrix does not use MEM D2H by default. In order to compare memory transfer performance with other packages, H (C) is added into the source code to make a consistent comparison.</em></p>
<p>Table 7 SP/DP performance of each Package on the M40 with matrix size of 8192*8192</p>
<p><img src="/oneXPU/uploads/2017/03/Table7-R-benchmark-2-1024x274.png"></p>
<p>Note: GEMM kernel API on M40 is magma_lds128_dgemm_kernel.</p>
<h3 id="3-4-Asynchronous-Mode"><a href="#3-4-Asynchronous-Mode" class="headerlink" title="3.4 Asynchronous Mode"></a>3.4 Asynchronous Mode</h3><p>For the advanced user, gpuR provides a set of asynchronous mode interface. By using asynchronous interfaces, the R program will immediately return to the CPU program side after calling the interface of vcl *, and the user can continue to perform other tasks on the CPU. When the user explicitly accesses and use vcl * data, if the calculation has not yet completed, R will continue to wait; if the calculation has been completed, users can directly use. Therefore, users can use concurrency of CPU and GPU to hide the communication and computing time on GPU. In Figure 4, we compared the computing time between gpuR in asynchronous mode and gmatrix in synchronous mode (gmatrix shows the best performance in synchronous mode testing). As figure 4 shown, the sync-API execution time increases as the computational task increases but async-API keep a very tiny cost for all input size because the async-API do not include any actual calculations and just returns immediately. So, in the best case, we can hide all GPU execution time with CPU computation with a very tiny overhead. *gpuR running code with SP in asynchronous mode</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">library(gpuR)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span>(i <span class="keyword">in</span> seq(<span class="number">1</span>:<span class="number">7</span>)) &#123;</span><br><span class="line">  ORDER = <span class="number">256</span>*(<span class="number">2</span>^i)</span><br><span class="line">  vclA_f = vclMatrix(A, nrow = ORDER, type=<span class="string">&quot;float&quot;</span>)</span><br><span class="line">  vclB_f = vclMatrix(B, nrow = ORDER, type=<span class="string">&quot;float&quot;</span>)</span><br><span class="line">  cputime = system.time(&#123;vclC_f = vclA_f %*% vclB_f&#125;)[<span class="number">3</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="/oneXPU/uploads/2017/03/Figure4-R-benchmark-2-1024x690.png"></p>
<p>Figure 4. Performance comparison between gpuR in asynchronous mode and gmatrix in synchronization mode</p>
<h1 id="4-Conclusions-and-recommendations"><a href="#4-Conclusions-and-recommendations" class="headerlink" title="4. Conclusions and recommendations"></a>4. Conclusions and recommendations</h1><p>In this blog, we analyze the performance of the most popular GPU computing package. Each package has its own unique, but also have their own advantages and disadvantages. In practices, we need to choose according to specific needs. Based on the calculation platform, the calculation mode and the ease of use, it is recommended as follows:</p>
<ol>
<li>nvblas is suitable for</li>
</ol>
<ul>
<li>NVIDIA GPU card</li>
<li>Double precision calculation</li>
<li>Large memory consumption of the calculation, nvblas provides a very good performance and scalability</li>
<li>Beginners</li>
</ul>
<ol start="2">
<li>gputools is suitable for</li>
</ol>
<ul>
<li>NVIDIA GPU card</li>
<li>Double precision calculation</li>
<li>Easy to use, and same API interface with R</li>
<li>Beginners</li>
</ul>
<ol start="3">
<li>gmatrix is suitable for</li>
</ol>
<ul>
<li>NVIDIA GPU card</li>
<li>Single/Double precision calculation</li>
<li>Multilevel BLAS interface(level 1，2，3)</li>
<li>More extension in GPU (colsum, sort)</li>
<li>Memory transfer optimization but the user needs to know where the memory is saved</li>
<li>Intermediate/Senior users  or R developers</li>
</ul>
<ol start="4">
<li>gpuR is suitable for</li>
</ol>
<ul>
<li>Single/Double precision calculation</li>
<li>Multilevel BLAS interface(level 1，2，3)</li>
<li>Heterogeneous systems work on most of the platforms such as AMD, Intel Xeon Phi, Intel GPUs</li>
<li>Asynchronous calculation mode, you can better hide the communication time</li>
<li>Intermediate/Senior users or R developers</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jitmatrix.github.io/oneXPU/2017/01/24/parallel-computation-with-r-and-xgboost/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/oneXPU/images/avatar.gif">
      <meta itemprop="name" content="Patric Zhao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ParallelR">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/oneXPU/2017/01/24/parallel-computation-with-r-and-xgboost/" class="post-title-link" itemprop="url">Parallel Computation with R and XGBoost</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-01-24 05:05:13" itemprop="dateCreated datePublished" datetime="2017-01-24T05:05:13+00:00">2017-01-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-19 10:35:25" itemprop="dateModified" datetime="2020-12-19T10:35:25+00:00">2020-12-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/General/" itemprop="url" rel="index"><span itemprop="name">General</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/General/MultiCores/" itemprop="url" rel="index"><span itemprop="name">MultiCores</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a target="_blank" rel="noopener" href="https://github.com/dmlc/xgboost">XGBoost</a> is a comprehensive machine learning library for gradient boosting. It began from the Kaggle community for online machine learning challenges, and then maintained by the collaborative efforts from the developers in the community. It is well known for its accuracy, efficiency and flexibility for various interfaces: the computational module is implemented in C++, and currently provides interfaces for R, python, Julia and Java. Its corresponding R package, <a target="_blank" rel="noopener" href="https://github.com/dmlc/xgboost/tree/master/R-package">xgboost</a>, in this sense is non-typical in terms of the design and structure. Although it is common that an R package is a wrapper of another tool, not many packages have the backend supporting many ways of parallel computation. The structure of the Project can be illustrated as follows: <img src="/oneXPU/uploads/2016/11/xgboost-struct-1-1024x570.png"> Although it is common that an R package is a wrapper of another tool, not many packages have the backend supporting many ways of parallel computation. In xgboost, most works are done in the C++ part in the above Figure. Since all interfaces share the same computational backend, there is not really a difference in terms of the accuracy or efficiency of the results from different interfaces. Users only need to prepare the data and parameters in the preferred language, then call the corresponding interface and wait for the training and prediction. This design puts most heavy works to the background, and only asks for the minimum support from each interface. For this reason, we can expect in the future there will be more languages wrapping the XGBoost backend and their users can enjoy the parallel training power. XGBoost implements a gradient boosting trees algorithm. A gradient boosting trees model trains a lot of decision trees or regression trees in a sequence, where only one tree is added to the model at a time, and every new tree depends on the previous trees. This nature limits the level of parallel computation, since we cannot build multiple trees simultaneously. Therefore, the parallel computation is introduced in a lower level, i.e. in the tree-building process at each step. <img src="/oneXPU/uploads/2017/01/xgboost-2.png"> Specifically, the parallel computation takes place in the operation where the model scans through all features on each internal node and set a threshold. Say we have a 4-core CPU for the training computation, then XGBoost separate the features into 4 groups. For the splitting operation on a node, XGBoost distributes the operation on each feature to their corresponding core. The training data is stored in a piece of shared memory, each core only needs to access one group of features, and perform the computation individually. The implementation is done in C++ with the help of OpenMP. It is obvious that users can benefit fully from the parallel computation if the number of features is larger than the number of threads of the CPU. XGBoost also supports training on a cluster, or with external memory. We will briefly introduce them in the following parts.</p>
<hr>
<p>In the following part, we will demonstrate the performance of the R package with different parallel strategies. We hope this introduction can be an example of a computational efficient R package.</p>
<h2 id="1-Multi-threading-on-a-single-machine"><a href="#1-Multi-threading-on-a-single-machine" class="headerlink" title="1. Multi-threading on a single machine"></a><strong>1. Multi-threading on a single machine</strong></h2><p>XGBoost offers the option to parallel the training process in an implicit style on a single machine, which could be a workstation or even your own laptop. This is one of the reasons that the Kaggle community loves it. In R, the switch of multi-threading computation is just a parameter nthread:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">require(xgboost)</span><br><span class="line">x = matrix(rnorm(<span class="number">100</span>*<span class="number">10000</span>), <span class="number">10000</span>, <span class="number">100</span>)</span><br><span class="line">y = x %*% rnorm(<span class="number">100</span>) + rnorm(<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">system.time(&#123;</span><br><span class="line">  bst = xgboost(data = x, label = y, nthread = <span class="number">1</span>, nround = <span class="number">100</span>, verbose = <span class="literal">FALSE</span>)</span><br><span class="line">&#125;)</span><br><span class="line"><span class="comment"># ser system elapsed</span></span><br><span class="line"><span class="comment"># 10.98 0.05 11.06</span></span><br><span class="line"></span><br><span class="line">system.time(&#123;</span><br><span class="line">  bst = xgboost(data = x, label = y, nthread = <span class="number">4</span>, nround = <span class="number">100</span>, verbose = <span class="literal">FALSE</span>)</span><br><span class="line">&#125;)</span><br><span class="line"><span class="comment"># user system elapsed</span></span><br><span class="line"><span class="comment"># 20.80 0.67 3.37</span></span><br></pre></td></tr></table></figure>
<p>In the results from the toy example, there is a noticeable difference between the one-thread and four-thread trainings. As a comparison, we made the following figure from a competition data(<a target="_blank" rel="noopener" href="https://www.kaggle.com/c/higgs-boson/data">https://www.kaggle.com/c/higgs-boson/data</a>) on Kaggle. The experiments were run on a laptop with an i7-4700m CPU. <img src="/oneXPU/uploads/2016/11/SpeedFigure-1024x843.png" alt="speedfigure"> The marks R and python are the vanilla gradient boosting machine implementation. XGBoost is the fastest when using only one thread. By employing 4 threads the process can be almost 4x faster. To reproduce the above results, one can find related scripts at:<a target="_blank" rel="noopener" href="https://github.com/dmlc/xgboost/tree/master/demo/kaggle-higgs">https://github.com/dmlc/xgboost/tree/master/demo/kaggle-higgs</a>. Note that the plot was made in 2015, thus the results may vary due to changes in the packages.</p>
<h2 id="2-Parallel-on-a-Cluster"><a href="#2-Parallel-on-a-Cluster" class="headerlink" title="2. Parallel on a Cluster"></a><strong>2. Parallel on a Cluster</strong></h2><p>For some cases where the size of data is too large to fit into the memory, people may set up a cluster to parallel the training process. However, a uniformed API of multi-nodes parallel computation for different interface languages is still left to be developed. The current standard way to parallel the training is to use the C++ backend with a configuration file which manages the model parameters and then submit it to Yarn. For further information, please read the official documentation: <a target="_blank" rel="noopener" href="http://xgboost.readthedocs.io/en/latest/tutorials/aws_yarn.html">http://xgboost.readthedocs.io/en/latest/tutorials/aws_yarn.html</a>. It is also possible to distribute the computation in one’s own cluster, but there’s no documentation provided yet. One thing worth noticing is that when performing multi-node parallel computation, the data is split by the rows, thus on each node it is (almost) impossible to search for the exact best splitting point. As a result, XGBoost switches to an approximate algorithm mentioned in <a target="_blank" rel="noopener" href="http://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf">this paper</a>. Briefly speaking, the approximate algorithm creates a histogram to represent each feature based on its numerial distribution. It reduces the amount of calculation on slaves, makes the reduce step easier, and maintains the precision at the same time.</p>
<h2 id="3-External-Memory"><a href="#3-External-Memory" class="headerlink" title="3. External Memory"></a><strong>3. External Memory</strong></h2><p>External memory is a compromise of large size of input and insufficient computational resources. The basic idea is simple: store the input data on an SSD, which is cheaper than memory and faster than HDD, and repeatedly load a chunk of data into memory to train the model partially. Comparing to the parallel training on a cluster, this strategy also uses the approximate algorithm, but is more convenient to configure and call, and is also cheaper for most users. To enable the external memory for R, we need to make sure that the compiler on your machine supports it. Usually it is fine with the latest gcc/clang. For windows users with mingw, however, is not able to try it out. The data files also need to be in the libsvm format on the disk. Files used in this demo can be downloaded at <a target="_blank" rel="noopener" href="https://github.com/dmlc/xgboost/tree/master/demo/data">https://github.com/dmlc/xgboost/tree/master/demo/data</a>. Here’s the usual way to load the data into memory with xgboost’s own data structure:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dtrain = xgb.DMatrix(<span class="string">&#x27;agaricus.txt.train&#x27;</span>)</span><br><span class="line"><span class="comment"># [15:57:38] 6513x127 matrix with 143286 entries loaded from agaricus.txt.train</span></span><br><span class="line">dtest = xgb.DMatrix(<span class="string">&#x27;agaricus.txt.test&#x27;</span>)</span><br><span class="line"><span class="comment"># [15:57:38] 1611x127 matrix with 35442 entries loaded from agaricus.txt.test</span></span><br><span class="line"></span><br><span class="line">model = xgboost(data = dtrain, nround = <span class="number">2</span>, objective = <span class="string">&quot;binary:logistic&quot;</span>)</span><br><span class="line"><span class="comment"># [1] train-error:0.000614 </span></span><br><span class="line"><span class="comment"># [2] train-error:0.001228</span></span><br></pre></td></tr></table></figure>
<p>Now if we add the suffix:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">dtrain = xgb.DMatrix(<span class="string">&#x27;agaricus.txt.train#train.cache&#x27;</span>)</span><br><span class="line"><span class="comment"># [15:57:45] SparsePage::Writer Finished writing to train.r0-1.cache.row.page</span></span><br><span class="line"><span class="comment"># [15:57:45] SparsePageSource: Finished writing to train.r0-1.cache</span></span><br><span class="line"><span class="comment"># [15:57:45] 6513x127 matrix with 143286 entries loaded from agaricus.txt.train#train.cache</span></span><br><span class="line">dtest = xgb.DMatrix(<span class="string">&#x27;agaricus.txt.test#test.cache&#x27;</span>)</span><br><span class="line"><span class="comment"># [15:57:45] SparsePage::Writer Finished writing to test.r0-1.cache.row.page</span></span><br><span class="line"><span class="comment"># [15:57:45] SparsePageSource: Finished writing to test.r0-1.cache</span></span><br><span class="line"><span class="comment"># [15:57:45] 1611x127 matrix with 35442 entries loaded from agaricus.txt.test#test.cache</span></span><br><span class="line"></span><br><span class="line">model = xgboost(data = dtrain, nround = <span class="number">2</span>, objective = <span class="string">&quot;binary:logistic&quot;</span>)</span><br><span class="line"><span class="comment"># [15:57:45] SparsePage::Writer Finished writing to train.r0-1.cache.col.page</span></span><br><span class="line"><span class="comment"># [1] train-error:0.000614 </span></span><br><span class="line"><span class="comment"># [2] train-error:0.001228</span></span><br></pre></td></tr></table></figure>
<p>Note the only difference is just the suffix: A “#” and the string following. The suffix can be arbitrary string as the prefix of the generated cache files, as printed in the output. With the suffix, the function automatically marks the file for external memory training. In the external memory mode we can also perform multi-threading training for each chunk of data, because the chunks are taken into the training process in a linear relationship. More details are included in <a target="_blank" rel="noopener" href="http://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf">this paper</a>.</p>
<hr>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>XGBoost puts effort in the three popular parallel computation solutions, multithreading, distributed parallel and <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Out-of-core_algorithm">out-of-cores</a> computations. The idea of this project is to only expose necessary APIs for different language interface design, and hide most computational details in the backend. So far the library is fast and user-friendly, we wish it could inspire more R package developers to balance the design and efficiency. The development will be continued, and contributions on code and ideas are always welcome :)</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jitmatrix.github.io/oneXPU/2016/09/10/r-with-parallel-computing/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/oneXPU/images/avatar.gif">
      <meta itemprop="name" content="Patric Zhao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ParallelR">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/oneXPU/2016/09/10/r-with-parallel-computing/" class="post-title-link" itemprop="url">R with Parallel Computing from User Perspectives</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-09-10 09:10:31" itemprop="dateCreated datePublished" datetime="2016-09-10T09:10:31+00:00">2016-09-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-19 10:35:25" itemprop="dateModified" datetime="2020-12-19T10:35:25+00:00">2020-12-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/Accelerators/" itemprop="url" rel="index"><span itemprop="name">Accelerators</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/Accelerators/General/" itemprop="url" rel="index"><span itemprop="name">General</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/Accelerators/General/GPGPU/" itemprop="url" rel="index"><span itemprop="name">GPGPU</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/Accelerators/General/GPGPU/MultiCores/" itemprop="url" rel="index"><span itemprop="name">MultiCores</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/Accelerators/General/GPGPU/MultiCores/Performance-Optimizaiton/" itemprop="url" rel="index"><span itemprop="name">Performance Optimizaiton</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <hr>
<p><em>This article is originally published in <a target="_blank" rel="noopener" href="http://cos.name/">Capital of Statistic</a> by Chinese [<a target="_blank" rel="noopener" href="http://cos.name/2016/09/r-and-parallel-computing/">link</a>] and I would like to thank <a target="_blank" rel="noopener" href="http://www.sfu.ca/~hetongh/">He Tong</a> for lots of great suggestions.</em> <em>All code in this post can be found on GitHub [<a target="_blank" rel="noopener" href="https://github.com/PatricZhao/ParallelR/tree/master/PP_for_COS">link</a>].</em></p>
<hr>
<p>Data scientists are already very familiar with statistical software like <a target="_blank" rel="noopener" href="https://www.r-project.org/">R</a>, <a target="_blank" rel="noopener" href="http://www.sas.com/en_hk/home.html">SAS</a>, <a target="_blank" rel="noopener" href="http://www.ibm.com/analytics/us/en/technology/spss/">SPSS</a>, <a target="_blank" rel="noopener" href="http://www.mathworks.com/products/matlab">MATLAB</a>; however, some of them are relatively inexperienced in parallel computing. So, in this post, I will introduce you some basic concepts on the use of parallel computing in R.</p>
<h1 id="What-is-Parallel-Computing？"><a href="#What-is-Parallel-Computing？" class="headerlink" title="What is Parallel Computing？"></a>What is Parallel Computing？</h1><p><a target="_blank" rel="noopener" href="https://computing.llnl.gov/tutorials/parallel_comp/">Parallel computing</a>, specifically, should include <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Supercomputer">high-performance computers</a> and <a target="_blank" rel="noopener" href="http://whatis.techtarget.com/definition/parallel-processing-software">parallel software</a>. The peak performance of high-performance computers increases quickly. In the most recent ranking of the world’s TOP500 supercomputers, Chinese Sunway Taihu Light topped the list with 93 PFLOPS (<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Supercomputer">here</a>). For most individuals, small and medium enterprises, high-performance computers are too expensive. So, the application of high-performance computers is indeed limited, mainly in the field of national defense, military, aerospace and research areas. In recent years, with the rapid developments of multicore CPU, cheap cluster, and various accelerators (<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Nvidia_Tesla">NVIDIA GPU</a>, <a target="_blank" rel="noopener" href="http://www.intel.com/content/www/us/en/processors/xeon/xeon-phi-detail.html">Intel Xeon Phi</a>, <a target="_blank" rel="noopener" href="http://www.xilinx.com/training/fpga/fpga-field-programmable-gate-array.htm">FPGA</a>), personal computers has been comparable to high-performance computers. <a href="/oneXPU/uploads/2016/09/sunway-taihulight.jpg"><img src="/oneXPU/uploads/2016/09/sunway-taihulight.jpg" alt="sunway-taihulight"></a> On the other hand, the software changes lag a lot. Imagine what software you’re using  supported parallel operations, Chrome, Visual Studio or R? <a href="/oneXPU/uploads/2016/09/cs.png"><img src="/oneXPU/uploads/2016/09/cs.png" alt="common software"></a> Software parallelization requires more research and development supports. It is called <a target="_blank" rel="noopener" href="https://software.intel.com/en-us/articles/what-is-code-modernization">code modernization</a> for the procedure of changing the serial code to parallel, which sounds a very interesting work. But, in practice, a large number of bug fixes, data structure rewrite, uncertain software behaviors and cross-platform issues greatly increase the software development and maintenance costs.</p>
<h1 id="Why-R-Needs-Parallel-Computing"><a href="#Why-R-Needs-Parallel-Computing" class="headerlink" title="Why R Needs Parallel Computing?"></a>Why R Needs Parallel Computing?</h1><p>Let’s come back to R. As one of the most popular statistical software, R has a lot of advantages, such as a wealth of statistical models, data processing tools, and powerful visualization capabilities. However, with an increasing amount of data, R’s memory usage and computation mode limit R to scale. From the memory perspective, R uses in-memory calculation mode. All data need to be processed in the main memory (RAM). Obviously, its advantages are high computational efficiency and speed, but the drawback is that the size of the problem can be handled by R is very limited (&lt;RAM ). Secondly, R core is a single-threaded program. Thus, in the modern multi-core processors,  R can not effectively use all the computing cores. If the R went to the Sunway CPU of 260 computing cores, single-threaded R only take 1/260 computing power and waste other computing cores of 259/260.</p>
<h2 id="Solution？Parallel-Computing"><a href="#Solution？Parallel-Computing" class="headerlink" title="Solution？Parallel Computing!"></a><strong>Solution？Parallel Computing!</strong></h2><p>Parallel computing technology can solve the problem that single-core and memory capacity can not meet the application needs. Thus, the parallel computing technology will be extremely expansion of the use of R.  From R 2.14 (Feb 2012), ‘<a target="_blank" rel="noopener" href="https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf">parallel</a>‘ package is installed by default. Obviously, R core development team also attached great importance to parallelization. <a href="/oneXPU/uploads/2016/09/RPP.png"><img src="/oneXPU/uploads/2016/09/RPP.png"></a></p>
<h1 id="How-to-Use-Parallel-Computing"><a href="#How-to-Use-Parallel-Computing" class="headerlink" title="How to Use Parallel Computing?"></a>How to Use Parallel Computing?</h1><p>From the user’s view, parallel computing in R can be divided into implicit and explicit computing mode.</p>
<h2 id="Implicit-Mode"><a href="#Implicit-Mode" class="headerlink" title="Implicit Mode"></a>Implicit Mode</h2><p>Implicit computing hides most of the details for the user. It is not necessary to know how to allocate hardware resources, distribute workloads and gather results. The computations will start automatically based on the current hardware resources. Obviously, this mode is the most favorable. We can achieve higher performance without changing the calculation mode and our codes. Common implicit parallel mode includes:</p>
<ul>
<li>  Using Parallel Libraries</li>
</ul>
<p>Parallel libraries, such as <a target="_blank" rel="noopener" href="https://software.intel.com/en-us/intel-mkl">Intel MKL</a>，<a target="_blank" rel="noopener" href="https://developer.nvidia.com/cublas">NVIDIA cuBLAS</a>,  <a target="_blank" rel="noopener" href="http://www.openblas.net/">OpenBLAS</a> are usually provided by the hardware manufacturer with in-depth optimizations based on the corresponding hardwares, so its performance is hugely better than R libraries. It is recommended choosing a high-performance R library at compile time or loading by LD_PRELOAD at runtime. The details of compiling, loading and using BLAS libraries can be found in the one of our previous blog (in <a target="_blank" rel="noopener" href="http://www.parallelr.com/r-hpac-benchmark-analysis/">here</a>). In the first diagram, the matrix calculation experiments, parallel libraries on 1 or 2 CPUs is a hundred times faster than R original library. On the second, we can see the GPU math library shows remarkable speed for some common analysis algorithms as well. <a href="/oneXPU/uploads/2016/09/mm.png"><img src="/oneXPU/uploads/2016/09/mm.png" alt="GEMM"></a> <a href="/oneXPU/uploads/2016/09/ca.png"><img src="/oneXPU/uploads/2016/09/ca.png" alt="GPU for R"></a> Now, let’s run an interesting example in which we didn’t call GEMM function explicitly but still get lots of performance improvements from parallel BLAS library. In below example, we train <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a> dataset by DBN,deep belief network, (SRBM,Stacked Restricted Boltzmann Machine ) with <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/deepnet/index.html">deepnet</a> package. This example refers the blog of “<a target="_blank" rel="noopener" href="http://basicstatistics.tistory.com/entry/training-MNIST-data-with-the-package-deepnet">training MNIST data with the package deepnet</a>“ where the author got the accuracy of 0.004% on training data and 2% on testing data. Because the original network of <code>c(500,500,250,125)</code> is too huge to run, I simplified the network architecture in our case and the code of <code>deepnet_mnist.R</code> in <a target="_blank" rel="noopener" href="https://github.com/PatricZhao/ParallelR/blob/master/PP_for_COS/deepnet_mnist.R">here</a>.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#install.packages(&quot;data.table&quot;)</span></span><br><span class="line"><span class="comment">#install.packages(&quot;deepnet&quot;)</span></span><br><span class="line"> </span><br><span class="line">library(data.table)</span><br><span class="line">library(deepnet)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># download MNIST dataset in below links</span></span><br><span class="line"><span class="comment"># https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/train.csv.gz</span></span><br><span class="line"><span class="comment"># https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/test.csv.gz</span></span><br><span class="line">mnist.train &lt;- as.matrix(fread(<span class="string">&quot;./train.csv&quot;</span>, header=<span class="built_in">F</span>))</span><br><span class="line">mnist.test  &lt;- as.matrix(fread(<span class="string">&quot;./test.csv&quot;</span>, header=<span class="built_in">F</span>))</span><br><span class="line"> </span><br><span class="line"><span class="comment"># V785 is the label</span></span><br><span class="line">x &lt;- mnist.train[, <span class="number">1</span>:<span class="number">784</span>]/<span class="number">255</span></span><br><span class="line">y &lt;- model.matrix(~as.factor(mnist.train[, <span class="number">785</span>])-<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line">system.time(</span><br><span class="line">    nn &lt;- dbn.dnn.train(x,y,</span><br><span class="line">                        hidden=<span class="built_in">c</span>(<span class="number">64</span>),</span><br><span class="line">                        <span class="comment">#hidden=c(500,500,250,125),</span></span><br><span class="line">                        output=<span class="string">&quot;softmax&quot;</span>,</span><br><span class="line">                        batchsize=<span class="number">128</span>, </span><br><span class="line">                        numepochs=<span class="number">100</span>, </span><br><span class="line">                        learningrate = <span class="number">0.1</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Thus, we run this piece of code twice. We got <strong>3.7X **and  **2.5X</strong>speedup (<strong>runtime of <a href="/oneXPU/uploads/2016/09/deepnet_Rnative-1.png">2581</a> sec .vs. <a href="/oneXPU/uploads/2016/09/deepnet_MKL.png">693</a> sec and <a href="/oneXPU/uploads/2016/09/deepnet_OpenBLAS.png">1213</a> sec</strong> )  by Intel MKL and OpenBLAS library on Intel SandyBridge E-2670.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&gt; R CMD BATCH deepnet_mnist.R</span><br><span class="line">&gt; cat deepnet_mnist.Rout</span><br><span class="line">deep nn has been trained.</span><br><span class="line">     user system   elapsed </span><br><span class="line"> 2574.013  1.404  2581.882</span><br><span class="line"> </span><br><span class="line">&gt; env LD_PRELOAD=/.../tools/OpenBLAS/lib/libopenblas.so R CMD BATCH deepnet_mnist.R</span><br><span class="line">&gt; cat deepnet_mnist.Rout</span><br><span class="line">deep nn has been trained.</span><br><span class="line">     user    system  elapsed </span><br><span class="line"> 4752.005 25881.221 1213.644</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Compiled with Intel Compiler and MKL</span></span><br><span class="line">&gt; R CMD BATCH deepnet_mnist.R</span><br><span class="line">&gt; cat deepnet_mnist.Rout</span><br><span class="line">deep nn has been trained.</span><br><span class="line">      user  system elapsed </span><br><span class="line"> 10770.641 290.486 693.146</span><br></pre></td></tr></table></figure>
<ul>
<li>  Using MultiThreading Functions</li>
</ul>
<p><a target="_blank" rel="noopener" href="http://openmp.org/wp/">OpenMP</a> is a multithreading library based on shared memory architecture for application acceleration. The latest R has been opened OpenMP options (-fopenmp) at compile time on Linux, which means that some of the calculations can be run in multithreaded mode. For example , <code>dist</code> is implemented by multithreading with OpenMP. The example code as below (<a target="_blank" rel="noopener" href="https://github.com/PatricZhao/ParallelR/blob/master/PP_for_COS/ImplicitParallel_MT.R">ImplicitParallel_MT.R</a>):</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Comparison of single thread and multiple threads run</span></span><br><span class="line"><span class="comment"># using Internal function to set thread numbers, not very grace, but don&#x27;t find a good way till now.</span></span><br><span class="line"><span class="comment"># Ang suggestion?</span></span><br><span class="line">setNumThreads &lt;- <span class="keyword">function</span>(nums=<span class="number">1</span>) &#123;</span><br><span class="line">  .Internal(setMaxNumMathThreads(nums))</span><br><span class="line">  .Internal(setNumMathThreads(nums))</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment"># dataset from 2^6 to 2^11</span></span><br><span class="line"><span class="keyword">for</span>(i <span class="keyword">in</span> <span class="number">6</span>:<span class="number">11</span>) &#123;</span><br><span class="line">  ORDER &lt;- 2^i</span><br><span class="line">  m &lt;- matrix(rnorm(ORDER*ORDER),ORDER,ORDER)</span><br><span class="line">  setNumThreads(<span class="number">1</span>)</span><br><span class="line">  res &lt;- system.time(d &lt;- dist(m))</span><br><span class="line">  print(res)</span><br><span class="line">  setNumThreads(<span class="number">20</span>)</span><br><span class="line">  res &lt;- system.time(d &lt;- dist(m))</span><br><span class="line">  print(res)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><a href="/oneXPU/uploads/2016/09/dist.png"><img src="/oneXPU/uploads/2016/09/dist.png"></a></p>
<ul>
<li>  Using Parallel Packages</li>
</ul>
<p>In the list of <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/views/HighPerformanceComputing.html">R high-performance computing</a>, there are lots of parallel packages and tools. These parallel packages can be used like any other R packages quickly and conveniently. R users can always focus on the problem itself, without having to think too much about parallelism implementations and performance issues. Take <a target="_blank" rel="noopener" href="http://www.h2o.ai/">H2O.ai</a> for example, it takes Java as the backend to achieve multi-threading and multi-nodes computing. Users only need to load the package, and then initialize H2O with thread number. After that subsequent calculations, such as GBM, GLM, DeepLearning algorithm, will automatically be assigned to multiple threads and multiple CPUs.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">library(h2o)</span><br><span class="line">h2o.init(nthreads = <span class="number">4</span>)</span><br><span class="line"><span class="comment"># Connection successful!</span></span><br><span class="line"><span class="comment"># R is connected to the H2O cluster: </span></span><br><span class="line"><span class="comment"># H2O cluster uptime: 1 hours 53 minutes </span></span><br><span class="line"><span class="comment"># H2O cluster version: 3.8.3.3 </span></span><br><span class="line"><span class="comment"># H2O cluster name: H2O_started_from_R_patricz_ywj416 </span></span><br><span class="line"><span class="comment"># H2O cluster total nodes: 1 </span></span><br><span class="line"><span class="comment"># H2O cluster total memory: 1.55 GB </span></span><br><span class="line"><span class="comment"># H2O cluster total cores: 4 </span></span><br><span class="line"><span class="comment"># H2O cluster allowed cores: 4 </span></span><br><span class="line"><span class="comment"># H2O cluster healthy: TRUE </span></span><br><span class="line"><span class="comment"># H2O Connection ip: localhost </span></span><br><span class="line"><span class="comment"># H2O Connection port: 54321 </span></span><br><span class="line"><span class="comment"># H2O Connection proxy: NA </span></span><br><span class="line"><span class="comment"># R Version: R version 3.3.0 (2016-05-03)</span></span><br></pre></td></tr></table></figure>
<h2 id="Explicit-Mode"><a href="#Explicit-Mode" class="headerlink" title="Explicit Mode"></a>Explicit Mode</h2><p>Explicit parallel computing requires the user to be able to deal with more details, including data partitions, task distributions, and final results collections. Users not only need to understand their own algorithms but also need to have a certain understanding of hardware and software stack. Thus, it’s a little difficult for users. Fortunately, parallel computing framework in R, such as <code>parallel</code>,<code>Rmpi</code> and <code>foreach</code>, provides the simple parallel programming approach by mapping structure. R users only need to transfer the code into the form of <code>*apply</code> or <code>for</code>, and then replace them by parallel APIs such as <code>mc*apply</code> or <code>foreach</code>. For more complex calculation flow, the user can repeat the process of map-and-reduce. <a href="/oneXPU/uploads/2016/09/mapping-1.png"><img src="/oneXPU/uploads/2016/09/mapping-1.png" alt="R Parallel Approaches"></a> Now, we show you a parallel example by solving quadratic equation with <code>*apply</code> and <code>for</code> style. The whole code in <a target="_blank" rel="noopener" href="https://github.com/PatricZhao/ParallelR/blob/master/PP_for_COS/ExplicitParallel.R">ExplicitParallel.R</a>. First, we present a non- vectorized function for solving the equation, which can handle several special cases, such as second quadratic coefficient is zero, or second and first quadratic term are zero, or the number of the square root is negative.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Not vectorized function</span></span><br><span class="line"><span class="comment"># Quadratic Equation: a*x^2 + b*x + c = 0</span></span><br><span class="line">solve.quad.eq &lt;- <span class="keyword">function</span>(a, b, <span class="built_in">c</span>) </span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment"># Not validate eqution: a and b are almost ZERO</span></span><br><span class="line">  <span class="keyword">if</span>(<span class="built_in">abs</span>(a) &lt; <span class="number">1e-8</span> &amp;&amp; <span class="built_in">abs</span>(b) &lt; <span class="number">1e-8</span>) <span class="built_in">return</span>(<span class="built_in">c</span>(<span class="literal">NA</span>, <span class="literal">NA</span>) )</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Not quad equation</span></span><br><span class="line">  <span class="keyword">if</span>(<span class="built_in">abs</span>(a) &lt; <span class="number">1e-8</span> &amp;&amp; <span class="built_in">abs</span>(b) &gt; <span class="number">1e-8</span>) <span class="built_in">return</span>(<span class="built_in">c</span>(-<span class="built_in">c</span>/b, <span class="literal">NA</span>))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># No Solution</span></span><br><span class="line">  <span class="keyword">if</span>(b*b - <span class="number">4</span>*a*<span class="built_in">c</span> &lt; <span class="number">0</span>) <span class="built_in">return</span>(<span class="built_in">c</span>(<span class="literal">NA</span>,<span class="literal">NA</span>))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Return solutions</span></span><br><span class="line">  x.delta &lt;- <span class="built_in">sqrt</span>(b*b - <span class="number">4</span>*a*<span class="built_in">c</span>)</span><br><span class="line">  x1 &lt;- (-b + x.delta)/(<span class="number">2</span>*a)</span><br><span class="line">  x2 &lt;- (-b - x.delta)/(<span class="number">2</span>*a)</span><br><span class="line"></span><br><span class="line">  <span class="built_in">return</span>(<span class="built_in">c</span>(x1, x2))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>And then, we randomly generated three big vectors to storage three coefficients.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Generate data </span></span><br><span class="line">len &lt;- 1e8</span><br><span class="line">a &lt;- runif(len, -<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">a[sample(len, <span class="number">100</span>,replace=<span class="literal">TRUE</span>)] &lt;- <span class="number">0</span></span><br><span class="line"> </span><br><span class="line">b &lt;- runif(len, -<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">c &lt;- runif(len, -<span class="number">10</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<h3 id="apply-IMPLEMENTATION"><a href="#apply-IMPLEMENTATION" class="headerlink" title="*apply IMPLEMENTATION:"></a>*<strong>apply IMPLEMENTATION:</strong></h3><p>First, we look at the serial code. The data is mapped into solver function,<code>solve.quad.eq </code>by <code>lapply</code>, and the results are saved into list finally.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># serial code</span></span><br><span class="line">system.time(</span><br><span class="line">  res1.s &lt;- lapply(<span class="number">1</span>:len, FUN = <span class="keyword">function</span>(x) &#123; solve.quad.eq(a[x], b[x], <span class="built_in">c</span>[x])&#125;)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Next, we use the function of <code>mcLapply</code> (multicores) in <code>parallel</code> package to parallelize calculations in <code>lapply</code>. From the API interface, the usage of <code>mcLapply</code> is really similar with <code>lapply</code> in addition to specifying the core numbers. <code>mcLapply</code> creates multiple copies of the current R session based on Linux fork mechanism, and evenly assign compute tasks into multiple processes regarding with input index. Finally, the master R session will collect the results from all worker sessions. If we specify two worker processes, one process calculated <code>1:(len/2)</code> while another computing <code>(len/2+1):len</code>, and finally two parts of results will be merged into <code>res1.p</code>. However, due to the use of Linux mechanisms, this version can’t be executed on Windows platform.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># parallel, Linux and MAC platform</span></span><br><span class="line">library(parallel)</span><br><span class="line"><span class="comment"># multicores on Linux</span></span><br><span class="line">system.time(</span><br><span class="line">  res1.p &lt;- mclapply(<span class="number">1</span>:len, </span><br><span class="line">                      FUN = <span class="keyword">function</span>(x) &#123; solve.quad.eq(a[x], b[x], <span class="built_in">c</span>[x]) &#125;, </span><br><span class="line">                      mc.cores = <span class="number">4</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>For non-Linux users, we can use <code>parLapply</code> function in <code>parallel</code> package to achieve parallelism. <code>parLapply</code> function supports different platforms including Windows, Linux and Mac with better portability, but its usage is a little complicated than <code>mclapply</code>. Before using <code>parLapply</code> function, we need to create a computing group (cluster) first. Computing group is a software-level concept, which means how many R worker processes we need to create (Note: <code>par*apply</code> package will create several new R processes rather than copies of R master process from <code>mc*apply</code>). Theoretically, the size of the computing group is not affected by the hardware configuration.For example, we can create a group with 1000 R worker processes on any machine. In practice, we usually use the same size of computing group with hardware resources (such as physical cores) so that each worker process of R can be mapped to a physical core. In the following example, we start with <code>detectCores</code> function to determine the number of computing cores in the machine.It is noteworthy that <code>detectCores()</code> returns the number of <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Hyper-threading">Hyper-Threading</a> rather than real physical cores.For example, there are two physical cores on my laptop, and each core can simulate two hyperthreading , so <code>detectCores()</code> return value is 4. However, for many compute-intensive tasks, the Hyper-Threading is not much helpful for improving performance, so we use the parameter of <code>logical=FALSE</code> to get the actual number of physical cores and then create the same number group.Since the worker processes in the group is new R sessions, the data and functions of the parent process is not visible. Therefore, we have to broadcast the data and functions to all worker processes by <code>clusterExport</code> function. Finally <code>parLapply</code> will distribute the tasks to all R worker processes evenly, and then gather results back.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cluster on Windows</span></span><br><span class="line">cores &lt;- detectCores(logical = <span class="literal">FALSE</span>)</span><br><span class="line">cl &lt;- makeCluster(cores)</span><br><span class="line">clusterExport(cl, <span class="built_in">c</span>(<span class="string">&#x27;solve.quad.eq&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>))</span><br><span class="line">system.time(</span><br><span class="line">  res1.p &lt;- parLapply(cl, <span class="number">1</span>:len, <span class="keyword">function</span>(x) &#123; solve.quad.eq(a[x], b[x], <span class="built_in">c</span>[x]) &#125;)</span><br><span class="line">)</span><br><span class="line">stopCluster(cl)</span><br></pre></td></tr></table></figure>
<h3 id="for-IMPLEMENTATION"><a href="#for-IMPLEMENTATION" class="headerlink" title="for IMPLEMENTATION:"></a><strong>for IMPLEMENTATION:</strong></h3><p>The computation approach of <code>for</code> is very similar with <code>*apply</code>. In the following serial implementation, we created a matrix for storage results and update the results one by one in the inner loop.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># for style: serial code</span></span><br><span class="line">res2.s &lt;- matrix(<span class="number">0</span>, nrow=len, ncol = <span class="number">2</span>)</span><br><span class="line">system.time(</span><br><span class="line">  <span class="keyword">for</span>(i <span class="keyword">in</span> <span class="number">1</span>:len) &#123;</span><br><span class="line">      res2.s[i,] &lt;- solve.quad.eq(a[i], b[i], <span class="built_in">c</span>[i])</span><br><span class="line">  &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>For the for loop parallelization, we can use <code>%dopar%</code> in <code>foreach</code> package to distribute the computations to multiple R workers. <code>foreach</code> package provides a method of data mapping, but does not include the establishment of computing group.Therefore, we need to create a computing group by <code>doParallel</code> or <code>doMC</code> package. Creating computing group is as same as before, except setting backend of computations by <code>registerDoParallel</code>. Now we consider the data decomposition. Actually, we want each R worker process to deal with continuous computing tasks. Suppose we have two R worker processes, the process 1 computes from <code>1:len/2</code>, another process for <code>(len/2+1):len</code>. Therefore, in the following example code, we evenly distribute the vectors to computing group and each process calculates the size of <code>chunk.size</code>. Another important skill is using local matrix to save partial results in each process. Last, combine local results together by <code>.combine=&#39;rbind&#39;</code> parameter.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># foreach, work on Linux/Windows/Mac</span></span><br><span class="line">library(foreach)</span><br><span class="line">library(doParallel)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Real physical cores in my computer</span></span><br><span class="line">cores &lt;- detectCores(logical = <span class="literal">FALSE</span>)</span><br><span class="line">cl &lt;- makeCluster(cores)</span><br><span class="line">registerDoParallel(cl, cores=cores)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># clusterSplit are very convience to split data but it takes lots of extra memory</span></span><br><span class="line"><span class="comment"># chunks &lt;- clusterSplit(cl, 1:len)</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># split data by ourselves</span></span><br><span class="line">chunk.size &lt;- len/cores</span><br><span class="line"> </span><br><span class="line">system.time(</span><br><span class="line">  res2.p &lt;- foreach(i=<span class="number">1</span>:cores, .combine=<span class="string">&#x27;rbind&#x27;</span>) %dopar%</span><br><span class="line">  &#123; </span><br><span class="line">    <span class="comment"># local data for results</span></span><br><span class="line">    res &lt;- matrix(<span class="number">0</span>, nrow=chunk.size, ncol=<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">for</span>(x <span class="keyword">in</span> ((i-<span class="number">1</span>)*chunk.size+<span class="number">1</span>):(i*chunk.size)) &#123;</span><br><span class="line">        res[x - (i-<span class="number">1</span>)*chunk.size,] &lt;- solve.quad.eq(a[x], b[x], <span class="built_in">c</span>[x])</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># return local results</span></span><br><span class="line">    res</span><br><span class="line">  &#125;</span><br><span class="line">)</span><br><span class="line"> </span><br><span class="line">stopImplicitCluster()</span><br><span class="line">stopCluster(cl)</span><br></pre></td></tr></table></figure>
<p>Finally, we tested the code on Linux platform with 4 threads and can gain more than **3X speedup ** for every parallel implementation! <a href="/oneXPU/uploads/2016/09/EP.png"><img src="/oneXPU/uploads/2016/09/EP.png" alt="R explicit parallel mode"></a></p>
<h1 id="Challenges-and-Prospects"><a href="#Challenges-and-Prospects" class="headerlink" title="Challenges and Prospects"></a>Challenges and Prospects</h1><p><strong>Challenges</strong> :In practice, the problem needed to be resolved by parallel computing is not such simple as our examples. To parallelize R and its eco-system are still very difficult because,</p>
<ul>
<li>  R is a decentralized and non-commercial software</li>
</ul>
<p>R is not developed by a compact organization or company while most of R’s packages are contributed by users. It means that it is difficult to adjust and unify software architecture and design with the same philosophy. On the other hand, commercial software, such as Matlab, with unified development, maintenance, and management, can be relatively easier to restructure and reconstruct. Therefore, after several times update, the parallelism of commercial software will be much higher.</p>
<ul>
<li>  The infrastructure design of R is still single-threaded</li>
</ul>
<p>R was originally designed for single-threaded so that many of the underlying data structures and functions are not thread-safe. Therefore, lots of codes need to be rewritten or adjust for high-level parallel algorithms. But it likely will destroy the original design patterns.</p>
<ul>
<li>  The packages are highly dependent</li>
</ul>
<p>Assume that we use package B in R, and B depends on some functions of package A. If package B is improved by multithreading first; after that package A is also enhanced by parallelization. So, it is likely to appear hybrid parallel when we use package B. It may lead lots of strange errors (BUGs) and performance decrease if there is no comprehensive design and testing during developments. <strong>Prospects:</strong> How will the future of parallelism in R ?</p>
<ul>
<li>  High-performance components from commercial and research organizations</li>
</ul>
<p>Essentially, software developments are inseparable from the human and financial investments. The packages, such as <a target="_blank" rel="noopener" href="http://www.h2o.ai/">H2O</a>, <a target="_blank" rel="noopener" href="https://github.com/dmlc/mxnet">MXNet</a>, <a target="_blank" rel="noopener" href="https://software.intel.com/en-us/blogs/daal">Intel DAAL</a>, improve the performance significantly from parallelism with long-term supports.</p>
<ul>
<li>  Cloud Platform</li>
</ul>
<p>With the rise of cloud computing ,Data Analyst as a Services (DAAS) and Machine Learning as a Service (MLAS) are more and more popular.The major cloud providers optimize their tools, including R, from hardware deployments, database, high-level algorithms and explore much more parallelism in application level. For example, Microsoft recently launched a series supports for R in their cloud (<a target="_blank" rel="noopener" href="http://www.zdnet.com/article/microsofts-r-strategy/">here</a>). Therefore, parallel in R will be more transparent. The user does the same things in R, but the real computing will be distributed to the cloud.</p>
<hr>
<h2 id="Other-Articles-and-Slides-about-R-and-Parallel-Computing"><a href="#Other-Articles-and-Slides-about-R-and-Parallel-Computing" class="headerlink" title="Other Articles and Slides about R and Parallel Computing"></a><strong>Other Articles and Slides about R and Parallel Computing</strong></h2><ul>
<li>  Max Gordon, How-to go parallel in R – basics + tips, <a target="_blank" rel="noopener" href="http://gforge.se/2015/02/how-to-go-parallel-in-r-basics-tips/">here</a></li>
<li>  Marcus,A brief foray into parallel processing with R, <a target="_blank" rel="noopener" href="https://beckmw.wordpress.com/2014/01/21/a-brief-foray-into-parallel-processing-with-r/">here</a></li>
<li>  John Mount, A gentle introduction to parallel computing in R, <a target="_blank" rel="noopener" href="http://www.win-vector.com/blog/2016/01/parallel-computing-in-r/">here</a></li>
<li>  Guilherme Ludwig, Parallel computing with R, <a target="_blank" rel="noopener" href="http://www.stat.wisc.edu/~gvludwig/327-5/parallel.html#/">here</a></li>
<li>  Norman Matloff, GPU TUTORIAL, WITH R INTERFACING, <a target="_blank" rel="noopener" href="https://matloff.wordpress.com/2015/01/23/gpu-tutorial-with-r-interfacing/">here</a></li>
<li>  Grey, Running R in Parallel (the easy way), <a target="_blank" rel="noopener" href="http://blog.yhat.com/posts/running-r-in-parallel.html">here</a></li>
<li>  NIMBioS,Tutorial: Using R for HPC, <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLRyq_4VPZ9g_g4b3An6VTkRX_c0tAHoAj">video</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jitmatrix.github.io/oneXPU/2016/07/26/r-and-openmp-boosting-compiled-code-on-multi-core-cpu-s/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/oneXPU/images/avatar.gif">
      <meta itemprop="name" content="Patric Zhao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ParallelR">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/oneXPU/2016/07/26/r-and-openmp-boosting-compiled-code-on-multi-core-cpu-s/" class="post-title-link" itemprop="url">R and openMP: boosting compiled code on multi-core cpu-s</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-07-26 18:10:38" itemprop="dateCreated datePublished" datetime="2016-07-26T18:10:38+00:00">2016-07-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-19 10:35:25" itemprop="dateModified" datetime="2020-12-19T10:35:25+00:00">2020-12-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/Accelerators/" itemprop="url" rel="index"><span itemprop="name">Accelerators</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/Accelerators/MultiCores/" itemprop="url" rel="index"><span itemprop="name">MultiCores</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/Accelerators/MultiCores/Vectorization/" itemprop="url" rel="index"><span itemprop="name">Vectorization</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Sometimes you just need more speed. And sometime plain R does not provide it. This article is about boosting your R code with C++ and openMP. OpenMP is a parallel processing framework for shared memory systems. This is an excellent way to use all the cpu cores that are sitting, and often just idling, in any modern desktop and laptop. Below, I will take a simple, even trivial problem—ML estimation of normal distribution parameters—and solve it first in R, thereafter I write the likelihood function in standard single-threaded C++, and finally in parallel using C++ and openMP. Obviously, there are easier ways to find sample mean and variance but this is not the point. Read it, and try to write your own openMP program that does something useful!</p>
<h1 id="R-for-Simplicity"><a href="#R-for-Simplicity" class="headerlink" title="R for Simplicity"></a>R for Simplicity</h1><p>Assume we have a sample of random normals and let’s estimate the parameters (mean and standard deviation) by Maximum Likelihood (ML). We start with pure R. The log-likelihood function may look like this:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">llR &lt;- <span class="keyword">function</span>(par, x) &#123;</span><br><span class="line">  mu &lt;- par[<span class="number">1</span>]</span><br><span class="line">  sigma &lt;- par[<span class="number">2</span>]</span><br><span class="line">  <span class="built_in">sum</span>(-<span class="number">1</span>/<span class="number">2</span>*<span class="built_in">log</span>(<span class="number">2</span>*<span class="built_in">pi</span>) - <span class="built_in">log</span>(sigma) - <span class="number">1</span>/<span class="number">2</span>*((x - mu)^<span class="number">2</span>)/sigma^<span class="number">2</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Note that this code is fully vectorized (<code>(x-mu)</code> is written with no explicit loop) and hence very fast. Obviously, this is a trivial example, but it is easy to understand and parallelize. Now generate some data</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x &lt;- rnorm(<span class="number">1e6</span>)</span><br></pre></td></tr></table></figure>
<p>and start values, a bit off to give the computer more work:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start &lt;- <span class="built_in">c</span>(<span class="number">1</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>Estimate it (using maxLik package):</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">library(maxLik)</span><br><span class="line">system.time(m &lt;- maxLik(llR, start=start, x=x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># user system elapsed</span></span><br><span class="line"><span class="comment"># 2.740  0.184   2.931</span></span><br><span class="line"></span><br><span class="line">summary(m)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------------------------------------</span></span><br><span class="line"><span class="comment"># Maximum Likelihood estimation</span></span><br><span class="line"><span class="comment"># Newton-Raphson maximisation, 6 iterations</span></span><br><span class="line"><span class="comment"># Return code 2: successive function values within tolerance limit</span></span><br><span class="line"><span class="comment"># Log-Likelihood: -1419125</span></span><br><span class="line"><span class="comment"># 2 free parameters</span></span><br><span class="line"><span class="comment"># Estimates:</span></span><br><span class="line"><span class="comment"># Estimate Std. error t value Pr(&gt; t)</span></span><br><span class="line"><span class="comment"># [1,] 0.0010318 0.0010001 1.032 0.302</span></span><br><span class="line"><span class="comment"># [2,] 1.0001867 0.0007072 1414.236 &lt;2e-16 ***</span></span><br><span class="line"><span class="comment"># ---</span></span><br><span class="line"><span class="comment"># Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span></span><br><span class="line"><span class="comment"># --------------------------------------------</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>The code runs 2.6s on an i5-2450M laptop using a single cpu core. First, let’s squeeze more out of the R code. Despite being vectorized, the function can be improved by moving the repeated calculations out of the (vectorized) loop. We can re-write it as:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">llROpt &lt;- <span class="keyword">function</span>(par, x) &#123;</span><br><span class="line">  mu &lt;- par[<span class="number">1</span>]</span><br><span class="line">  sigma &lt;- par[<span class="number">2</span>]</span><br><span class="line">  N &lt;- <span class="built_in">length</span>(x)</span><br><span class="line">  -N*(<span class="number">0.5</span>*<span class="built_in">log</span>(<span class="number">2</span>*<span class="built_in">pi</span>) + <span class="built_in">log</span>(sigma)) - <span class="number">0.5</span>*<span class="built_in">sum</span>((x - mu)^<span class="number">2</span>)/sigma^<span class="number">2</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Now only <code>(x - mu)^2</code> is computed as vectors. Run it:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">library(maxLik)</span><br><span class="line">system.time(m &lt;- maxLik(llROpt, start=start, x=x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># user  system elapsed</span></span><br><span class="line"><span class="comment"># 0.816   0.000   0.818 </span></span><br></pre></td></tr></table></figure>
<p>You see—just a simple optimization gave a more than three–fold speed improvement! I don’t report the results any more, as those are virtually identical.</p>
<h1 id="C-for-Speed"><a href="#C-for-Speed" class="headerlink" title="C for Speed"></a>C for Speed</h1><p>Now let’s implement the same function in C++. R itself is written in C, however there is an excellent library, Rcpp, that makes integrating R and C++ code very easy. It is beyond the scope of this post to teach readers C and explain the differences between C and C++. But remember that Rcpp (and hence C++) offers a substantially easier interface for exchanging data between R and compiled code than the default R API. Let’s save the log-likelihood function in file loglik.cpp. It might look like this:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;Rcpp.h&gt;</span></span></span><br><span class="line">  </span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> Rcpp;</span><br><span class="line">  </span><br><span class="line"><span class="function">RcppExport SEXP <span class="title">loglik</span><span class="params">(SEXP s_beta, SEXP s_x)</span> </span>&#123;</span><br><span class="line">  <span class="function">NumericVector <span class="title">x</span><span class="params">(s_x)</span></span>;</span><br><span class="line">  <span class="function">NumericVector <span class="title">beta</span><span class="params">(s_beta)</span></span>;</span><br><span class="line">  <span class="comment">// make Rcpp vector out of R SEXP</span></span><br><span class="line">  <span class="keyword">double</span> mu = beta[<span class="number">0</span>];</span><br><span class="line">  <span class="comment">// first element is 0 in C++</span></span><br><span class="line">  <span class="keyword">double</span> sigma = beta[<span class="number">1</span>];</span><br><span class="line">  <span class="keyword">double</span> ll = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; x.length(); i++) &#123;</span><br><span class="line">      ll -= (x[i] - mu)*(x[i] - mu);</span><br><span class="line">  &#125;</span><br><span class="line">  ll *= <span class="number">0.5</span>/sigma/sigma;</span><br><span class="line">  ll -= (<span class="number">0.5</span>*<span class="built_in">log</span>(<span class="number">2</span>*M_PI) + <span class="built_in">log</span>(sigma))*x.length();</span><br><span class="line">  <span class="function">NumericVector <span class="title">result</span><span class="params">(<span class="number">1</span>, ll)</span></span>;</span><br><span class="line">  <span class="comment">// create &#x27;numeric&#x27; vector of length 1, filled with</span></span><br><span class="line">  <span class="comment">// ll values</span></span><br><span class="line">  <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>The function takes two parameters, <code>s_beta</code> and <code>s_x</code>. These are passed as R general vectors, denoted <code>SEXP</code> in C. As SEXP-s are complicated to handle, the following two lines transform those to ‘NumericVector’s, essentially equivalent to R ‘numeric()’. The following code is easy to understand. We loop over all iterations and add (x[i] - mu)^2. Loops are cheap in C++. Afterwards, we add the constant terms only once. Note that unlike R, indices in C++ start from zero. This program must be compiled first. Normally the command</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">R CMD SHLIB loglik.cpp</span><br></pre></td></tr></table></figure>
<p>takes care of all the R-specific dependencies, and if everything goes well, it results in the DLL file. Rcpp requires additional include files which must be specified when compiling, the location of which can be queried with <code>Rcpp:::CxxFlags()</code> command in R, or as a bash one-liner, one may compile with</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PKG_CXXFLAGS=$(<span class="built_in">echo</span> <span class="string">&#x27;Rcpp:::CxxFlags()&#x27;</span>| R --vanilla --slave) R CMD SHLIB loglik.cpp</span><br></pre></td></tr></table></figure>
<p>Now we have to create the R-side of the log-likelihood function. It may look like this:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">llc &lt;- <span class="keyword">function</span>(par, x) &#123;</span><br><span class="line">  library(Rcpp)</span><br><span class="line">  dyn.load(<span class="string">&quot;loglik.so&quot;</span>) <span class="comment"># extension &#x27;.so&#x27; is platform-specific!</span></span><br><span class="line">  res &lt;- .Call(<span class="string">&quot;loglik&quot;</span>, par, x)</span><br><span class="line">  res</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>It takes arguments ‘par’ and ‘x’, and passes these down to the DLL. Before we can invoke (<code>.Call</code>) the compiled code, we must load the DLL. You may need to adjust the exact name according to your platform here. Note also that I haven’t introduced any security checks neither at the R nor the C++ side. This is a quick recipe for crashing your session, but let’s avoid it here in order to keep the code simple. Now let’s run it:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">system.time(m &lt;- maxLik(llc, start=start, x=x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># user  system elapsed</span></span><br><span class="line"><span class="comment"># 0.896   0.020   0.913 </span></span><br></pre></td></tr></table></figure>
<p>The C code runs almost exactly as fast as the optimized R. In case of well vectorized computations, there seems to be little scope for improving the speed by switching to C.</p>
<h1 id="Parallelizing-the-code-on-multicore-CPUs"><a href="#Parallelizing-the-code-on-multicore-CPUs" class="headerlink" title="Parallelizing the code on multicore CPUs"></a>Parallelizing the code on multicore CPUs</h1><p>Now it is time to write a parallel version of the program. Take the C++ version as the point of departure and re-write it like this:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;Rcpp.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;omp.h&gt;</span></span></span><br><span class="line">  </span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> Rcpp;</span><br><span class="line">  </span><br><span class="line"><span class="function">RcppExport SEXP <span class="title">loglik_MP</span><span class="params">(SEXP s_beta, SEXP s_x, SEXP s_nCpu)</span> </span>&#123;</span><br><span class="line">    <span class="function">NumericVector <span class="title">x</span><span class="params">(s_x)</span></span>;</span><br><span class="line">    <span class="function">NumericVector <span class="title">beta</span><span class="params">(s_beta)</span></span>;</span><br><span class="line">    <span class="keyword">int</span> n_cpu = IntegerVector(s_nCpu)[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">double</span> mu = beta[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">double</span> sigma = beta[<span class="number">1</span>];</span><br><span class="line">    <span class="keyword">double</span> ll = <span class="number">0</span>;</span><br><span class="line">    omp_set_dynamic(<span class="number">0</span>);         <span class="comment">// Explicitly disable dynamic teams</span></span><br><span class="line">    omp_set_num_threads(n_cpu); <span class="comment">// Use n_cpu threads for all</span></span><br><span class="line">                                <span class="comment">// consecutive parallel regions</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp parallel</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">double</span> ll_thread = <span class="number">0</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp for </span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; x.length(); i++) &#123;</span><br><span class="line">            ll_thread -= (x[i] - mu)*(x[i] - mu);</span><br><span class="line">        &#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp critical</span></span><br><span class="line">        &#123;</span><br><span class="line">            ll += ll_thread;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    ll *= <span class="number">0.5</span>/sigma/sigma;</span><br><span class="line">    ll -= (<span class="number">0.5</span>*<span class="built_in">log</span>(<span class="number">2</span>*M_PI) + <span class="built_in">log</span>(sigma))*x.length();</span><br><span class="line">    <span class="function">NumericVector <span class="title">result</span><span class="params">(<span class="number">1</span>, ll)</span></span>;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>The code structure is rather similar to the previous example. The most notable novelties are the ‘#pragma omp’ directives. These tell the compiler to insert parallelized code here. Not all compilers understand it, and others may need special flags, such as <code>-fopenmp</code> in case of gcc, to enable openMP support. Otherwise, gcc just happily ignores the directives and you will get a single-threaded application. The likelihood function also includes the argument _n_Cpu_, and the commands <code>omp_set_dynamic(0)</code> and <code>omp_set_num_threads(n_cpu)</code>. This allows to manipulate the number of threads explicitly, it is usually not necessary in the production code. For compiling the program, we can add <code>-fopenmp</code> to our one-liner above:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PKG_CXXFLAGS=<span class="string">&quot;<span class="subst">$(echo &#x27;Rcpp:::CxxFlags()</span>&#x27;| R --vanilla --slave) -fopenmp&quot;</span> R CMD SHLIB loglikMP.cpp</span><br></pre></td></tr></table></figure>
<p>assuming it was saved in “loglikMP.cpp”. But now you should seriously consider writing a makefile instead. We use three openMP directives here:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp parallel</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="comment">/* code block */</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>This is the most important omp directive. It forces the code block to be run in multiple threads, by all threads simultaneously. In particular, variable _ll_thread_ is declared in all threads separately and is thus a thread-specific variable. As OMP is a shared-memory parallel framework, all data declared before <em>#pragma omp parallel</em> is accessible by all threads. This is very convenient as long as we only read it. The last directive is closely related:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp critical</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="comment">/* code block */</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>This denotes a piece of threaded code that must be run by only one thread simultaneously. In the example above all threads execute <code>ll += ll_thread</code>, but only one at a time, waiting for the previous thread to finish if necessary. This is because now we are writing to shared memory: variable <em>ll</em> is defined before we split the code into threads. Allowing multiple threads to simultaneously write in the same shared variable almost always leads to trouble. Finally,</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp for</span></span><br><span class="line"><span class="keyword">for</span>(...) &#123; <span class="comment">/* code block */</span> &#125;</span><br></pre></td></tr></table></figure>
<p>splits the for loop between threads in a way that each thread will only go through a fraction of the full loop. For instance, in our case the full loop goes over 1M observations, but in case of 8 threads, each will receive only 125k. As the compiler has to generate code for this type of loop sharing, parallel loops are less flexible than ordinary single-threaded loops. For many data types, summing the thread–specific values we did with <code>#pragma omp critical</code> can be achieved directly in the loop by specifying <code>#pragma omp parallel for reduction(+:ll)</code> instead. As all the parallel work is done at C level, the R code remains essentially unchanged. We may write the corresponding loglik function as</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">llcMP &lt;- <span class="keyword">function</span>(par, nCpu=<span class="number">1</span>, x) &#123;</span><br><span class="line">  library(Rcpp)</span><br><span class="line">  dyn.load(<span class="string">&quot;loglikMP.so&quot;</span>)</span><br><span class="line">  res &lt;- .Call(<span class="string">&quot;loglik_MP&quot;</span>, par, x, <span class="built_in">as.integer</span>(nCpu))</span><br><span class="line">  res</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>How fast is this?</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">system.time(m &lt;- maxLik(llcMP, start=start, nCpu=<span class="number">4</span>, x=x))</span><br><span class="line"><span class="comment"># user  system elapsed</span></span><br><span class="line"><span class="comment"># 0.732   0.016   0.203 </span></span><br></pre></td></tr></table></figure>
<p>On 2-core/4-thread cpu, we got a more than four–fold speed boost. This is impressive, given the cpu does have 2 complete cores only. Obviously, the performance improvement depends on the task. This particular problem is embarrasingly parallel, the threads can work completely independent of each other.</p>
<h1 id="Timing-Examples"><a href="#Timing-Examples" class="headerlink" title="Timing Examples"></a>Timing Examples</h1><p>As an extended timing example, we run all the (optimized) examples above using a Xeon-L5420 cpu with 8 cores, single thread per core. The figure below depicts the compute time for single-threaded R and C++ code, and for C++/openMP code with 8 threads, as a function of data size. <img src="/oneXPU/uploads/2016/07/timings-1-1024x585.png" alt="timings"> The figure reveals several facts. First, for non-parallelized code we can see that</p>
<ol>
<li> Optimized R and C++ code are of virtually identical speed.</li>
<li> compute time grows linearily in data size.</li>
</ol>
<p>For openMP code the figure tells</p>
<ol start="3">
<li> openMP with 8 threads is substantially slower for data size less than about 100k. For larger data, multi-threaded approach is clearly faster.</li>
<li> openMP execution time is almost constant for data size up to 4M. For larger data vectors, it increases linearily. This suggests that for smaller data size, openMP execution time is dominated by thread creation and management overheads, not by computations.</li>
</ol>
<p>Finally, let’s compare the computation times for different number of threads for 8M data size. <img src="/oneXPU/uploads/2016/07/timings_n-2-1024x585.png" alt="timings_n"> The figure shows the run time for single threaded versions of the code (R and C), and multi-threaded openMP versions with 1 to 9 threads (OMP.1 to OMP.9).</p>
<ol>
<li> More cpus give us shorter execution times. 1-thread OMP will run almost 1.7 times slower than 8-threaded version (3.9 and 2.3 s respectively).</li>
<li> The gain of more cpu cores working on the problem levels off quickly. Little noticeable gain is visible for more than 3 cores. It indicates that the calculations are only partly limited by computing-power. Another major bottleneck may be memory speed.</li>
<li> Last, and most strikingly, even the single threaded OMP version of the code is 4.8 times faster than single-threaded C++ version with no OMP (18.6 and 3.9 s respectively)! This is a feature of the particular task, the compiler and the processor architecture. OMP parallel for–loops allow the compiler to deduce that the loops are in fact independent, and use faster SSE instruction set. This substantially boosts the speed but requires more memory bandwidth.</li>
</ol>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>With the examples above I wanted to show that for many tasks, openMP is not hard to use. If you know some C++, parallelizing your code may be quite easy. True, the examples above are easy to parallelize at R-level as well, but there are many tasks where this is not true. Obviously, in the text above I just scratched the surface of openMP. If you consider using it, there are many excellent sources on the web. Take a look!</p>
<p>I am grateful to Peng Zhao for explaining the parallel loops and SSE instruction set.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jitmatrix.github.io/oneXPU/2016/05/09/r-dnn-cuda-multigpu/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/oneXPU/images/avatar.gif">
      <meta itemprop="name" content="Patric Zhao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ParallelR">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/oneXPU/2016/05/09/r-dnn-cuda-multigpu/" class="post-title-link" itemprop="url">R for Deep Learning (III): CUDA and MultiGPUs Acceleration</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-05-09 00:00:08" itemprop="dateCreated datePublished" datetime="2016-05-09T00:00:08+00:00">2016-05-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-19 10:35:25" itemprop="dateModified" datetime="2020-12-19T10:35:25+00:00">2020-12-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/Accelerators/" itemprop="url" rel="index"><span itemprop="name">Accelerators</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/Accelerators/GPGPU/" itemprop="url" rel="index"><span itemprop="name">GPGPU</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/Accelerators/GPGPU/Performance-Optimizaiton/" itemprop="url" rel="index"><span itemprop="name">Performance Optimizaiton</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Notes: 1. The entire source code of this post in <a target="_blank" rel="noopener" href="https://github.com/PatricZhao/ParallelR/blob/master/ParDNN">here</a> 2. The PDF version of this post in <a target="_blank" rel="noopener" href="http://www.parallelr.com/materials/4_CUDA/CUDA_DNN.pdf">here</a></p>
<hr>
<p>In previous two blogs (<a target="_blank" rel="noopener" href="http://www.parallelr.com/r-deep-neural-network-from-scratch/">here</a> and <a target="_blank" rel="noopener" href="http://www.parallelr.com/r-dnn-parallel-acceleration/">here</a>), we illustrated several skills to build and optimize artificial neural network (ANN) with R and speed up by parallel BLAS libraries in modern hardware platform including Intel Xeon and NVIDIA GPU. Nowadays, multiple GPU accelerations are crucial for learning huge networks, one example, as Microsoft won ImageNet competition with huge network up to 1000 layers in 2015, [<a target="_blank" rel="noopener" href="http://www.i-programmer.info/news/105-artificial-intelligence/9266-microsoft-wins-imagenet-using-extremely-deep-neural-networks.html">here</a> and <a target="_blank" rel="noopener" href="http://image-net.org/challenges/LSVRC/2015/results">here</a>]. In this blog, I will focus on applying CUDA implementation into our neural network offloading the computationally intensive parts into GPU and then we can easily extend CUDA implementation from single GPU to multiple GPUs under ‘<a target="_blank" rel="noopener" href="https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf">parallel</a>’ packages of R.  P.S: If you want to go through all of these contents quickly, check out my presentation in GTC16 in <a target="_blank" rel="noopener" href="http://www.parallelr.com/GTC16/GTC16_PatricZhao_Unlock_DNN_Perf_CUDA.pdf">here</a>.  </p>
<h2 id="CUDA-INTEGRATION"><a href="#CUDA-INTEGRATION" class="headerlink" title="CUDA INTEGRATION"></a>CUDA INTEGRATION</h2><p>Now, we begin to introduce our <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Ninja">ninja</a> skills : CUDA After  combined DNN code with CUDA BLAS library and several optimizations, we get the follow results in the table <a target="_blank" rel="noopener" href="http://www.parallelr.com/r-dnn-parallel-acceleration/">in the previous blog</a> and leave one question for readers:</p>
<p><strong>What is your opinion about the next step of optimizations?</strong></p>
<p><a href="/oneXPU/uploads/2016/03/orig.png"></a><a href="/oneXPU/uploads/2016/03/orig-1.png"><img src="/oneXPU/uploads/2016/03/orig-1.png" alt="R_ANN"></a> It’s obvious that the function ‘pmax’ accounts for lots of runtimes (<strong>31.58</strong> secs) following ‘%*%’ (<strong>53.72</strong> secs) since ‘pmax’ is implemented by R and it will be very slow when the data size increase. (btw, you can try to increase the number of neurons in hidden layer to 1024 and profiling the code again to figure out the ratio of ‘pmax’ in the whole computation).  Reviewing the functionality of ‘pmax’ in our DNN case, we implement the ReLU function and get the maximum value among input value and 0 for every neuron in hidden layer. Furthermore, because our algorithm is vectorized by matrices for high performance,  the input of ‘pmax’ is a two-dimensional matrix and we can parallel maximum function into each element easily. So, let’s start to parallel the ReLu function by CUDA. I will skip the details of CUDA programming in this blog, you can refer programming guide in <a target="_blank" rel="noopener" href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/#axzz47CbpOe1j">here</a>. First, we can write the CUDA code to compare the input value with ZERO. Despite it  is a very naïve implementation, it is still very fast than original R code. Throughout this kernel code can be executed in NVIDIA GPU, it is written by CUDA C rather than R. Next, we need to call it from R environment. In general, we have to write wrapper functions to bridge R,  C/C++, and CUDA. <a target="_blank" rel="noopener" href="http://heather.cs.ucdavis.edu/matloff.html">Prof. Matloff</a> and I have written the blog introduced linking R with CUDA step by step regarding with .Call() and .C() function with two-level wrappers from R to C/C++ and C/C++ to CUDA (<a target="_blank" rel="noopener" href="https://devblogs.nvidia.com/parallelforall/accelerate-r-applications-cuda/">here</a>  and <a target="_blank" rel="noopener" href="http://blog.revolutionanalytics.com/2015/01/parallel-programming-with-gpus-and-r.html">here</a>).  A brief summary about the major difference of .C() and .Call() is shown in below table. <a href="/oneXPU/uploads/2016/03/C_Call.png"><img src="/oneXPU/uploads/2016/03/C_Call.png" alt="R_Cal_ function"></a></p>
<p>From the performance view, the .Call() function is selected since little overhead between R and C/C++ by avoiding explicit copying data from R to C/C++ and then from C/C++ back to R. In below code, we access data by a pointer and heavily use R internal structures with very efficient way.</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// difinition for R</span></span><br><span class="line"><span class="keyword">extern</span> <span class="string">&quot;C&quot;</span> &#123;</span><br><span class="line">   <span class="function">SEXP <span class="title">pmax_cuda</span><span class="params">(SEXP A, SEXP threshold, SEXP devID)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment">//CUDA: simple implementation of pmax</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">pmax_kernel</span><span class="params">(<span class="keyword">double</span> *A, </span></span></span><br><span class="line"><span class="function"><span class="params">                            <span class="keyword">const</span> <span class="keyword">int</span> M, </span></span></span><br><span class="line"><span class="function"><span class="params">                            <span class="keyword">const</span> <span class="keyword">int</span> N, </span></span></span><br><span class="line"><span class="function"><span class="params">                            <span class="keyword">const</span> <span class="keyword">double</span> threshold)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> tid = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">  <span class="keyword">if</span>(tid &amp;lt; M*N) &#123; A[tid] = (A[tid]&amp;gt;threshold)?A[tid]:<span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment">// Wrapper code between R and CUDA </span></span><br><span class="line"><span class="function">SEXP <span class="title">pmax_cuda</span><span class="params">(SEXP A, SEXP threshold, SEXP devID)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   <span class="comment">// data structure for GPU</span></span><br><span class="line">   <span class="keyword">double</span> *A_host = <span class="literal">NULL</span>;</span><br><span class="line">   <span class="keyword">double</span> *A_d = <span class="literal">NULL</span>;</span><br><span class="line">   <span class="keyword">double</span> gw = <span class="number">0</span>;</span><br><span class="line">   <span class="keyword">int</span> mm = <span class="number">0</span>, nn = <span class="number">0</span>;</span><br><span class="line">   <span class="keyword">int</span> gpuID = <span class="number">0</span>;</span><br><span class="line">  </span><br><span class="line">   <span class="comment">// data transfer from R to C by pointers</span></span><br><span class="line">   A_host = REAL(A);</span><br><span class="line">   SEXP Rdim = getAttrib(A, R_DimSymbol);</span><br><span class="line">   mm = INTEGER(Rdim)[<span class="number">0</span>];</span><br><span class="line">   nn = INTEGER(Rdim)[<span class="number">1</span>];</span><br><span class="line">   gw = REAL(threshold)[<span class="number">0</span>];</span><br><span class="line">   gpuID = INTEGER(devID)[<span class="number">0</span>];</span><br><span class="line"> </span><br><span class="line">   <span class="comment">// for multiple GPU case </span></span><br><span class="line">   cudaSetDevice(gpuID);</span><br><span class="line">  </span><br><span class="line">   <span class="comment">// return value, allocated in C and can be used in R directly</span></span><br><span class="line">   SEXP Rval;</span><br><span class="line">   PROTECT(Rval = allocVector(REALSXP, mm*nn));</span><br><span class="line"> </span><br><span class="line">   <span class="comment">// GPU memory allocation</span></span><br><span class="line">   cudaMalloc(&amp;amp;A_d, mm*nn*<span class="keyword">sizeof</span>(<span class="keyword">double</span>));</span><br><span class="line">   <span class="keyword">if</span>(<span class="literal">NULL</span> == A_d) &#123;</span><br><span class="line">     <span class="built_in">printf</span>(<span class="string">&quot;\nNo RAM space in GPU!\n&quot;</span>);</span><br><span class="line">     UNPROTECT(<span class="number">1</span>);</span><br><span class="line">     <span class="keyword">return</span> R_NilValue;</span><br><span class="line">   &#125;</span><br><span class="line">  </span><br><span class="line">   <span class="comment">// memory copy from CPU to GPU</span></span><br><span class="line">   cudaMemcpy(A_d, A_host, mm*nn*<span class="keyword">sizeof</span>(<span class="keyword">double</span>), cudaMemcpyHostToDevice); </span><br><span class="line">  </span><br><span class="line">   <span class="comment">// CUDA: pmax, really computation parts</span></span><br><span class="line">   pmax_kernel&amp;lt;&amp;lt;&amp;lt;(mm*nn<span class="number">-1</span>)/<span class="number">512</span>+<span class="number">1</span>, <span class="number">512</span>&amp;gt;&amp;gt;&amp;gt;(A_d, mm, nn, gw);</span><br><span class="line">   cudaMemcpy(REAL(Rval), A_d, mm*nn*<span class="keyword">sizeof</span>(<span class="keyword">double</span>), cudaMemcpyDeviceToHost); </span><br><span class="line">   cudaDeviceSynchronize();</span><br><span class="line"> </span><br><span class="line">   <span class="comment">// Free unused memory of GPU</span></span><br><span class="line">   <span class="keyword">if</span>(A_d) &#123;cudaFree(A_d); A_d=<span class="literal">NULL</span>;&#125;</span><br><span class="line"> </span><br><span class="line">   UNPROTECT(<span class="number">1</span>);</span><br><span class="line">   <span class="keyword">return</span> Rval;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Next, compile the C/C++ and CUDA code together to a shared object file (.so) or dynamic link library (.dll) for loading in R.</p>
<blockquote>
<p>nvcc -O3 -arch=sm_35 -G -I…/CUDA-v7.5.18/include -I…/R-3.2.0/bin/lib64/R/include/ -L…/R/lib64/R/lib –shared -Xcompiler -fPIC -o cudaR.so cudaR.cu</p>
</blockquote>
<p>Finally, the CUDA version of ‘pmax’ can be called in R as simple as R builtin function with R’s wrapper, and,  for infrastructure engineer, writing a nice wrapper is still an important job :)</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># preload static object file</span></span><br><span class="line">dyn.load(<span class="string">&quot;cudaR.so&quot;</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># GPU version of pmax</span></span><br><span class="line">pmax.cuda &lt;- <span class="keyword">function</span>(A, threshold, devID=<span class="number">0L</span>)</span><br><span class="line">&#123;</span><br><span class="line">  rst &lt;- .Call(<span class="string">&quot;pmax_cuda&quot;</span>,</span><br><span class="line">                A,</span><br><span class="line">                threshold,</span><br><span class="line">                <span class="built_in">as.integer</span>(devID)</span><br><span class="line">	      )</span><br><span class="line">  <span class="built_in">dim</span>(rst) &lt;- <span class="built_in">dim</span>(A)</span><br><span class="line">  <span class="built_in">return</span>(rst)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Show our performance now!  By replacing ‘pmax’ with  new ‘pmax.cuda’,  the execution time of pmax reduces to <strong>6.7</strong> seconds from original 31.58 so it’s <strong>5X speedup</strong> and totally the <strong>1.2X speedup</strong> gains. <a href="/oneXPU/uploads/2016/03/cuda.png"><img src="/oneXPU/uploads/2016/03/cuda-300x291.png" alt="cuda pmax "></a>  </p>
<h2 id="Scale-out-to-MultiGPU"><a href="#Scale-out-to-MultiGPU" class="headerlink" title="Scale out to MultiGPU"></a><strong>Scale out to MultiGPU</strong></h2><p>Parallel computing is not a novel concept in R community. Data scientist is familiar with parallel strategies both in speedup their model construction and inference. In fact, the requirement of parallel computing in R is even higher than C/C++.  The C/C++ implementations always focus on low-level instructions and optimizations such as memory locality, communication, computation efficiency and much more while R aims to fast, easy and portability from high-level programming. Popular R packages handle most of low-level details and R users only focus on dataset decomposition and functional programming. Specifically, in this blog, we will show you parallel training of DNN with ‘<a target="_blank" rel="noopener" href="https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf">parallel</a>’ package and extend it to multiGPUs.</p>
<h4 id="HOGWILD"><a href="#HOGWILD" class="headerlink" title="HOGWILD!"></a>HOGWILD!</h4><p><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/1106.5730v2.pdf">HOGWILD!</a> is a data parallel model designed for stochastic gradient descent. It’s a lock-free approach with the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/MapReduce">MapReduce</a>-like parallel-processing framework and can be used in DNN training . Thus, the training processing is designed as below: 1.Launch N workers 2.Each worker updates local weights/bias based on parts (1/N) of data 3.Master collects and averages all weights/bias from each worker 4.Each worker updates its weights/bias from master</p>
<h4 id="Parallel-in-R"><a href="#Parallel-in-R" class="headerlink" title="Parallel in R"></a>Parallel in R</h4><p>In R, this flow can be implemented by ‘multicores’ packages (currently, ‘parallel’ package includes both ‘multicores’ and ‘snow’ in CRAN). In below, flow chart is the standard workflow of ‘multicore’ packages with our DNN. ‘mclapply’ function creates two processors which shares memory by copy-on-write and each processor train the network by parts of data. After several steps, the master processor will do a reduce step to collect  weights from two child processors and average them.In next iteration, two children will use the new weights. <a href="/oneXPU/uploads/2016/03/mclappy.png"><img src="/oneXPU/uploads/2016/03/mclappy-1024x555.png" alt="mclappy with GPU"></a> Now, let’s see the details of how R code handles this data parallel model based on below real codes . 1. ‘mclapply’ creates N (devNum) workers based on ‘mc.cores’ and each worker will execute the same function, train.dnn.cublas, with different index (1:devNum); 2. the data is divided into N (devNum) parts and each worker will load their data simultaneously by their ID then the computation, even writing, can be ideally parallelized; 3. all workers exit when ‘mclapply’ is done and the results from every worker will be saved in a list (res).  Master continues to remain parts and then calculate the average of all weights and bias. 4. in the next loop, the ‘mclapply’ will use the averaged model (para.model) to train again.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Parallel Training</span></span><br><span class="line">res &lt;- mclapply(<span class="number">1</span>:devNum, <span class="keyword">function</span>(id) &#123; train.dnn.cublas(x, y, </span><br><span class="line">                                         omodel=para.model,</span><br><span class="line">                                         taindata=traindata[N.start[id]:N.end[id],],</span><br><span class="line">                                         devType=“GPU”, devID=(id-<span class="number">1</span>), . . .) &#125;,</span><br><span class="line">                mc.cores=devNum, </span><br><span class="line">                mc.preschedule=<span class="literal">TRUE</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Construct new model with parallel weights</span></span><br><span class="line">D &lt;- res[[<span class="number">1</span>]]$D</span><br><span class="line">H &lt;- res[[<span class="number">1</span>]]$H</span><br><span class="line">K &lt;- res[[<span class="number">1</span>]]$K</span><br><span class="line"><span class="keyword">for</span>(i <span class="keyword">in</span> <span class="number">2</span>:devNum) &#123;</span><br><span class="line">        res[[<span class="number">1</span>]]$W1 &lt;- res[[<span class="number">1</span>]]$W1 + res[[i]]$W1</span><br><span class="line">        res[[<span class="number">1</span>]]$W2 &lt;- res[[<span class="number">1</span>]]$W2 + res[[i]]$W2</span><br><span class="line">        res[[<span class="number">1</span>]]$b1 &lt;- res[[<span class="number">1</span>]]$b1 + res[[i]]$b1</span><br><span class="line">        res[[<span class="number">1</span>]]$b2 &lt;- res[[<span class="number">1</span>]]$b2 + res[[i]]$b2</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">para.model &lt;- <span class="built_in">list</span>( D = D,</span><br><span class="line">                    H = H,</span><br><span class="line">                    K = K,</span><br><span class="line">                    <span class="comment"># weights and bias</span></span><br><span class="line">                    W1= res[[<span class="number">1</span>]]$W1/devNum, </span><br><span class="line">                    b1= res[[<span class="number">1</span>]]$b1/devNum, </span><br><span class="line">                    W2= res[[<span class="number">1</span>]]$W2/devNum, </span><br><span class="line">                    b2= res[[<span class="number">1</span>]]$b2/devNum)</span><br></pre></td></tr></table></figure>
<h4 id="Extent-to-MultiGPU"><a href="#Extent-to-MultiGPU" class="headerlink" title="Extent to MultiGPU"></a>Extent to MultiGPU</h4><p>Then scale to multiple GPUs, the workflow is almost as similar as CPU and only different is that each worker needs to set GPU ID explicitly and then run previous CUDA accelerated code. In other words, users still are able to access the same CUDA codes that they usually use (almost) without any change!  In our implementation, we adopt the strategy of a consistent one-to-one match between CPU worker with GPU by setting GPU index as below.</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// for multiple GPU case</span></span><br><span class="line">cudaSetDevice(gpuID);</span><br></pre></td></tr></table></figure>
<h4 id="Performance-Showcase"><a href="#Performance-Showcase" class="headerlink" title="Performance Showcase"></a>Performance Showcase</h4><p>Finally, we analyzed the performance of CPU and GPU code. The line plot shows the strong scalability of native R code (1 hidden layer and 512 neurons). And compared with H2O, native R exhibits good scalability with the number of thread increase. Look at GPU parts, one thread with one GPU is faster than 20 threads CPU implementation (both native R and H2O). Next, look at GPU scalability in bar plot where <strong>5 times</strong> speedup under 6 GPUs are reached and our algorithm achieved <strong>160 times</strong> speedup compared with original R code. Testing on : CPU:  Ivy Bridge E5-2690 v2 @ 3.00GHz, dual socket 10-core, 128G RAM;  GPU: NVIDIA K40m,  12G RAM <a href="/oneXPU/uploads/2016/03/MultiGPU_Runtime.png"><img src="/oneXPU/uploads/2016/03/MultiGPU_Runtime.png" alt="MultiGPU_Runtime"></a></p>
<p><a href="/oneXPU/uploads/2016/03/MultiGPU_Speedup.png"><img src="/oneXPU/uploads/2016/03/MultiGPU_Speedup.png" alt="MultiGPU_Speedup"></a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jitmatrix.github.io/oneXPU/2016/05/04/r-gpu-programming-for-all-with-gpur/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/oneXPU/images/avatar.gif">
      <meta itemprop="name" content="Patric Zhao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ParallelR">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/oneXPU/2016/05/04/r-gpu-programming-for-all-with-gpur/" class="post-title-link" itemprop="url">R - GPU Programming for All with 'gpuR'</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-05-04 13:52:08" itemprop="dateCreated datePublished" datetime="2016-05-04T13:52:08+00:00">2016-05-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-19 10:35:25" itemprop="dateModified" datetime="2020-12-19T10:35:25+00:00">2020-12-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/GPGPU/" itemprop="url" rel="index"><span itemprop="name">GPGPU</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/GPGPU/MultiCores/" itemprop="url" rel="index"><span itemprop="name">MultiCores</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/GPGPU/MultiCores/Performance-Optimizaiton/" itemprop="url" rel="index"><span itemprop="name">Performance Optimizaiton</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>GPUs (Graphic Processing Units) have become much more popular in recent years for computationally intensive calculations.  Despite these gains, the use of this hardware has been very limited in the R programming language.  Although possible, the prospect of programming in either OpenCL or CUDA is difficult for many programmers unaccustomed to working with such a low-level interface.  Creating bindings for R’s high-level programming that abstracts away the complex GPU code would make using GPUs far more accessible to R users.  This is the core idea behind the <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/gpuR/index.html">gpuR</a> package.  There are three novel aspects of <code>gpuR</code>:</p>
<ol>
<li> Applicable on ‘ALL’ GPUs</li>
<li> Abstracts away CUDA/OpenCL code to easily incorporate in to existing R algorithms</li>
<li> Separates copy/compute functions to allow objects to persist on GPU</li>
</ol>
<h3 id="Broad-application"><a href="#Broad-application" class="headerlink" title="Broad application:"></a><strong>Broad application</strong>:</h3><p>The ‘gpuR’ package was created to bring the power of GPU computing to any R user with a GPU device.  Although there are a handful of packages that provide some GPU capability (e.g. <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/gputools/index.html">gputools</a>, <a target="_blank" rel="noopener" href="http://cran.fhcrc.org/web/packages/cudaBayesreg/index.html">cudaBayesreg</a>, <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/HiPLARM/index.html">HiPLARM</a>, <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/HiPLARb/index.html">HiPLARb</a>, and <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/gmatrix/index.html">gmatrix</a>) all are strictly limited to NVIDIA GPUs.  As such, a backend that is based upon OpenCL would allow all users to benefit from GPU hardware.  The ‘gpuR’ package therefore utilizes the <a target="_blank" rel="noopener" href="http://viennacl.sourceforge.net/">ViennaCL</a> linear algebra library which contains auto-tuned OpenCL kernels (among others) that can be leveraged for GPUs.  The headers have been conveniently repackaged in the <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/RViennaCL/index.html">RViennaCL </a>package.  It also allows for a CUDA backend for those with NVIDIA GPUs that may see further improved performance (contained within the companion <a target="_blank" rel="noopener" href="https://github.com/cdeterman/gpuRcuda">gpuRcuda</a> package not yet formally released).</p>
<h3 id="Abstract-away-GPU-code"><a href="#Abstract-away-GPU-code" class="headerlink" title="Abstract away GPU code:"></a><strong>Abstract away GPU code:</strong></h3><p>The <code>gpuR</code> package uses the S4 object oriented system to have explicit classes and methods that all the user to simply cast their <code>matrix</code> or <code>vector</code> and continue programming in R as normal.  For example:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">ORDER = <span class="number">1024</span></span><br><span class="line"> </span><br><span class="line">A = matrix(rnorm(ORDER^<span class="number">2</span>), nrow=ORDER)</span><br><span class="line">B = matrix(rnorm(ORDER^<span class="number">2</span>), nrow=ORDER)</span><br><span class="line">gpuA = gpuMatrix(A, type=<span class="string">&quot;double&quot;</span>)</span><br><span class="line">gpuB = gpuMatrix(B, type=<span class="string">&quot;double&quot;</span>)</span><br><span class="line"> </span><br><span class="line">C = A %*% B</span><br><span class="line">gpuC = gpuA %*% gpuB</span><br><span class="line"> </span><br><span class="line">all.equal(C == gpuC[])</span><br><span class="line">[<span class="number">1</span>] <span class="literal">TRUE</span></span><br></pre></td></tr></table></figure>
<p>The <code>gpuMatrix</code> object points to a matrix in RAM which is then computed by the GPU when a desired function is called.  This avoids R’s habit of copying the memory of objects.  For example:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">library(pryr)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Initially points to same object</span></span><br><span class="line">x = matrix(rnorm(<span class="number">16</span>), <span class="number">4</span>)</span><br><span class="line">y = x</span><br><span class="line"> </span><br><span class="line">address(x)</span><br><span class="line">[<span class="number">1</span>] <span class="string">&quot;0x16177f28&quot;</span></span><br><span class="line"> </span><br><span class="line">address(y)</span><br><span class="line">[<span class="number">1</span>] <span class="string">&quot;0x16177f28&quot;</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># But once modify the second object it creates a copy</span></span><br><span class="line">y[<span class="number">1</span>,<span class="number">1</span>] = <span class="number">0</span></span><br><span class="line"> </span><br><span class="line">address(x)</span><br><span class="line">[<span class="number">1</span>] <span class="string">&quot;0x16177f28&quot;</span></span><br><span class="line"> </span><br><span class="line">address(y)</span><br><span class="line">[<span class="number">1</span>] <span class="string">&quot;0x15fbb1d8</span></span><br></pre></td></tr></table></figure>
<p>In contrast, the same syntax for a <code>gpuMatrix</code> will modify the original object in-place without any copy.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">library(pryr)</span><br><span class="line">library(gpuR)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Initially points to same object</span></span><br><span class="line">x = gpuMatrix(rnorm(<span class="number">16</span>), <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">y = x</span><br><span class="line"> </span><br><span class="line">x@address</span><br><span class="line">[<span class="number">1</span>] &lt;pointer: <span class="number">0x6baa040</span>&gt;</span><br><span class="line"> </span><br><span class="line">y@address</span><br><span class="line">[<span class="number">1</span>] &lt;pointer: <span class="number">0x6baa040</span>&gt;</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Modification affects both objects without copy</span></span><br><span class="line">y[<span class="number">1</span>,<span class="number">1</span>] = <span class="number">0</span></span><br><span class="line"> </span><br><span class="line">x@address</span><br><span class="line">[<span class="number">1</span>] &lt;pointer: <span class="number">0x6baa040</span>&gt;</span><br><span class="line"> </span><br><span class="line">y@address</span><br><span class="line">[<span class="number">1</span>] &lt;pointer: <span class="number">0x6baa040</span>&gt;</span><br></pre></td></tr></table></figure>
<p>Each new variable assigned to this object will only copy the pointer thereby making the program more memory efficient.  However, the <code>gpuMatrix&gt;</code> class does still require allocating GPU memory and copying data to device for each function call. The most commonly used methods have been overloaded such as  %*%, +, -, *, /, crossprod, tcrossprod, and trig functions among others.  In this way, an R user can create these objects and leverage GPU resources without the need to know a bunch more functions that would break existing algorithms.</p>
<h3 id="Distinct-Copy-Compute-Functionality"><a href="#Distinct-Copy-Compute-Functionality" class="headerlink" title="Distinct Copy/Compute Functionality:"></a><strong>Distinct Copy/Compute Functionality:</strong></h3><p>For the <code>gpuMatix</code> and <code>gpuVector</code> classes there are companion <code>vclMatrix</code> and <code>vclVector</code> class that point to objects that persist in the GPU RAM.  In this way, the user explicitly decides when data needs to be moved back to the host.  By avoiding unnecessary data transfer between host and device performance can significantly improve.  For example:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">vclA = vclMatrix(rnorm(<span class="number">10000</span>), nrow = <span class="number">100</span>)</span><br><span class="line">vclB = vclMatrix(rnorm(<span class="number">10000</span>), nrow = <span class="number">100</span>)</span><br><span class="line">vclC = vclMatrix(rnorm(<span class="number">10000</span>), nrow = <span class="number">100</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># GEMM</span></span><br><span class="line">vclD = vclA %*% vclB</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Element-wise addition</span></span><br><span class="line">vclD = vclD + vclC</span><br></pre></td></tr></table></figure>
<p>In this code, the three initial matrices already exist in the GPU memory so no data transfer takes place in the GEMM call.  Furthermore, the returned matrix remains in the GPU memory.  In this case, the ‘vclD’ object is still in GPU RAM. As such, the element-wise addition call also happens directly on the GPU with no data transfers. It is worth also noting that the user can still modify elements, rows, or columns with the exact same syntax as a normal R matrix.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vclD[<span class="number">1</span>,<span class="number">1</span>] = <span class="number">42</span></span><br><span class="line">vclD[,<span class="number">2</span>] = <span class="built_in">rep</span>(<span class="number">12</span>, <span class="number">100</span>)</span><br><span class="line">vclD[<span class="number">3</span>,] = <span class="built_in">rep</span>(<span class="number">23</span>, <span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p>These operations simply copy the <em>new</em> elements to the GPU and modify the object in-place within the GPU memory. The ‘vclD’ object is never copied to the host.</p>
<h3 id="Benchmarks"><a href="#Benchmarks" class="headerlink" title="Benchmarks:"></a>Benchmarks:</h3><p>With all that in mind, how does gpuR perform?  Here are some general benchmarks of the popular GEMM operation.  I currently only have access to a single NVIDIA GeForce GTX 970 for these simulations.  Users should expect to see differences with high performance GPUs (e.g. AMD FirePro, NVIDIA Tesla, etc.). Speedup relative to CPU will also vary depending upon user hardware.</p>
<h4 id="1-Default-dGEMM-vs-Base-R"><a href="#1-Default-dGEMM-vs-Base-R" class="headerlink" title="(1) Default dGEMM vs Base R"></a>(1) Default dGEMM vs Base R</h4><p>R is known to only support two numeric types (integer and double).  As such, Figure 1 shows the fold speedup achieved by using the <code>gpuMatrix</code> and <code>vclMatrix</code> classes.  Since R is already known to not be the fastest language, an implementation with the OpenBLAS backend is included as well for reference using a 4 core Intel i5-2500 CPU @ 3.30GHz.  As can be seen there is a dramatic speedup from just using OpenBLAS or the gpuMatrix class (essentially equivalent).  Of interest is the impact of the transfer time from host-device-host that is typical in many GPU implementations.  This cost is eliminated by using the <code>vclMatrix</code> class which continues to scale with matrix size. [caption id=”attachment_768” align=”aligncenter” width=”640”]<img src="/oneXPU/uploads/2016/05/dgemm-1024x588.png" alt="dgemm"> Figure 1 - Fold speedup achieved using openblas (CPU) as well as the gpuMatrix/vclMatrix (GPU) classes provided in gpuR.[/caption]  </p>
<h4 id="2-sGEMM-vs-Base-R"><a href="#2-sGEMM-vs-Base-R" class="headerlink" title="(2) sGEMM vs Base R"></a>(2) sGEMM vs Base R</h4><p>In many GPU benchmarks there is often float operations measured as well.  As noted above, R does not provide this by default.  One way to go around this is to use the <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/RcppArmadillo/index.html">RcppArmadillo</a> or RcppEigen packages and explicitly casting R objects as float types.  The armadillo library will also default to using the BLAS backend provided (i.e. OpenBLAS).  Float types are implemented <code>gpuR</code> by setting <code>type = &quot;float&quot;</code> in the matrix calls (e.g. <code>vclMatrix(mat, type = &quot;float&quot;)</code>) in Figure 2 shows the impact of using float data types.  OpenBLAS continues to provide a noticeable speedup but <code>gpuMatrix</code> begins to outperform once matrix order exceeds 1500.  The <code>vclMatrix</code> continues to demonstrate the value in retaining objects in GPU memory and avoiding memory transfers.   [caption id=”attachment_769” align=”aligncenter” width=”640”]<img src="/oneXPU/uploads/2016/05/sgemm-1024x588.png" alt="sgemm"> Figure 2 - Float type GEMM comparisons. Fold speedup achieved using openblas (via RcppArmadillo) as well as the gpuMatrix/vclMatrix (GPU) classes provided in gpuR.[/caption]   To give an additional view on the performance achieved by <code>gpuMatrix</code> and <code>vclMatrix</code> is comparing directly against the OpenBLAS performance.  The <code>gpuMatrix</code> reaches ~2-3 fold speedup over OpenBLAS whereas <code>vclMatrix</code> scales to over 100 fold speedup!  It is curious as to why the performance with vcl<code>Matrix</code> is so much faster (only differing in host-device-host transfers).  Further optimization with <code>gpuMatrix</code> will need to be explored (fresh eyes are welcome) accepting limitations in the BUS transfer speed.  Performance will certainly improve with improved hardware capabilities such as NVIDIA’s NVLink. [caption id=”attachment_831” align=”aligncenter” width=”737”]<img src="/oneXPU/uploads/2016/05/sgemm_openblas-1024x528.png" alt="sgemm_openblas"> Figure 3 - Fold speedup achieved over openblas (via RcppArmadillo) float type GEMM comparisons vs the gpuMatrix/vclMatrix (GPU) classes provided in gpuR.[/caption]</p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>The <code>gpuR</code> package has been created to bring GPU computing to as many R users as possible.  It is the intention to use <code>gpuR</code> to more easily supplement current and future algorithms that could benefit from GPU acceleration.  The <code>gpuR</code> package is currently available on <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/gpuR/index.html">CRAN</a>.  The development version can be found on my <a target="_blank" rel="noopener" href="https://github.com/cdeterman/gpuR">github</a> in addition to existing issues and wiki pages (assisting primarily in installation).  Future developments include solvers (e.g. QR, SVD, cholesky, etc.), scaling across multiple GPUs,  ‘sparse’ class objects, and custom OpenCL kernels. As noted above, this package is intended to be used with a multitude of hardware and operating systems (it has been tested on Windows, Mac, and multiple Linux flavors).  I only have access to a limited set of hardware (I can’t access every GPU, let along the most expensive).  As such, the development of <code>gpuR</code> depends upon the R user community.  Volunteers who possess different hardware are always welcomed and encouraged to submit issues regarding any discovered bugs.  I have begun a gitter account for users to report on successful usage with alternate hardware.  Suggestions and general conversation about gpuR is welcome.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jitmatrix.github.io/oneXPU/2016/04/15/r-hpac-benchmark-analysis/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/oneXPU/images/avatar.gif">
      <meta itemprop="name" content="Patric Zhao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ParallelR">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/oneXPU/2016/04/15/r-hpac-benchmark-analysis/" class="post-title-link" itemprop="url">R benchmark for High-Performance Analytics and Computing (I):Accelerators</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-04-15 05:10:18" itemprop="dateCreated datePublished" datetime="2016-04-15T05:10:18+00:00">2016-04-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-19 10:35:25" itemprop="dateModified" datetime="2020-12-19T10:35:25+00:00">2020-12-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/Accelerators/" itemprop="url" rel="index"><span itemprop="name">Accelerators</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/Accelerators/GPGPU/" itemprop="url" rel="index"><span itemprop="name">GPGPU</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/Accelerators/GPGPU/MultiCores/" itemprop="url" rel="index"><span itemprop="name">MultiCores</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Objectives-of-Experiments"><a href="#Objectives-of-Experiments" class="headerlink" title="Objectives of Experiments"></a>Objectives of Experiments</h2><p>R is more and more popular in various fields, including the high-performance analytics and computing (HPAC) fields. Nowadays, the architecture of HPC system can be classified as pure CPU system, CPU + Accelerators (GPGPU/FPGA) heterogeneous system, CPU + Coprocessors system. In software side, high performance scientific libraries, such as basic linear algebra subprograms (BLAS), will significantly influence the performance of R for HPAC applications. So, in the first post of R benchmark series, the experiments mainly contain two aspects: <em>(1)  Performance on different architectures of HPC system,</em> <em>(2)  Performance on different BLAS libraries.</em> </p>
<h2 id="Benchmark-and-Testing-Goals"><a href="#Benchmark-and-Testing-Goals" class="headerlink" title="Benchmark and Testing Goals"></a>Benchmark and Testing Goals</h2><p>In this post, we choose R-25 benchmark (available in <a target="_blank" rel="noopener" href="http://r.research.att.com/benchmarks/">here</a> ) which includes the most popular, widely acknowledged functions in the high performance analytic field. The testing script includes fifteen common computational intensive tasks (in Table-1) grouped into three categories: <em>(1) Matrix Calculation (1-5)</em> <em>(2) Matrix function (6-10)</em> <em>(3) Programmation (11-15)</em></p>
<p>Table-1 R-25 Benchmark Description</p>
<p><strong>Task Number</strong></p>
<p><strong>R-25 Benchmark Description</strong></p>
<p>1 Creation,transposition,deformation of a 2500*2500 matrix</p>
<p>2 2400*2400 normal distributed random matrix</p>
<p>3 Sorting of 7,000,000 random values</p>
<p>4 2800*2800 cross-product matrix</p>
<p>5 Linear regression over a 3000*3000 matrix</p>
<p>6 FFT over 2,400,000 random values</p>
<p>7 Eigenvalues of a 640*640 random values</p>
<p>8 Determinant of a 2500*2500 random matrix</p>
<p>9 Cholesky decomposition of a 3000*3000 matrix</p>
<p>10 Inverse of a 1600*1600 random matrix</p>
<p>11 3,500,000 Fibonacci numbers calculation(vector calculation)</p>
<p>12 Creation of a 3000*3000 Hilbert matrix(matrix calculation)</p>
<p>13 Grand common divisors of 400,000 pairs(recursion)</p>
<p>14 Creation of a 500*500 Toeplitz matrix(loops)</p>
<p>15 Escoufier’s method on a 45*45 matrix(mixed)</p>
<p>In our benchmark, we measured the performance of R-25 benchmark on various hardware platforms, including Intel Xeon CPU processors, NVIDIA GPGPU cards and Intel Xeon Phi coprocessors. Meanwhile, R built with different BLAS libraries results in different performance, so we tested R with self-contained BLAS, OpenBLAS, Intel MKL and CUDA BLAS. <strong>Because the performance of self-contained BLAS is</strong> <strong>hugely**</strong> lower than the other BLAS library and in practice HPAC users of R always built R with high performance BLAS, the testing results running with self-contained BLAS is negligible. ** Moreover, in order to investigate the performance of functions or algorithms such as GEMM that HPC users mostly used, we explore the speed-up when varying the size of the matrices and number of elements as known as <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Scalability">scalability</a>.  </p>
<h2 id="System-Descriptions"><a href="#System-Descriptions" class="headerlink" title="System Descriptions"></a>System Descriptions</h2><p>To evaluate the applicability of different methods for improving R performance in a HPC environment, the hardware and software of platform we used listed in the Table-2 and Table-3. <a href="/oneXPU/uploads/2016/04/table2.png"><img src="/oneXPU/uploads/2016/04/table2.png" alt="hardware configuration"></a> <a href="/oneXPU/uploads/2016/04/table3.png"><img src="/oneXPU/uploads/2016/04/table3.png" alt="software configuration"></a>  </p>
<h2 id="Results-and-Discussions"><a href="#Results-and-Discussions" class="headerlink" title="Results and Discussions"></a>Results and Discussions</h2><h4 id="1-General-Comparisons"><a href="#1-General-Comparisons" class="headerlink" title="(1) General Comparisons"></a><em>(1) General Comparisons</em></h4><p>Fig. 1 shows the speedup of R using different BLAS libraries and different hosts. The default R running with OpenBLAS is shown in red as <strong>our baseline</strong> for comparison so that its speedup is constantly equal to one. Intel Xeon E5-2670 has eight physical cores in one chipset, so there are 16 physical cores in one server node.<a target="_blank" rel="noopener" href="https://software.intel.com/en-us/articles/parallelism-in-the-intel-math-kernel-library">Intel MKL library</a> supports the single thread mode (Sequential) or OpenMP threading mode. MKL with OpenMP threading mode defaultly uses all physical cores in one node(here is 16).Fig.1 shows the results of using Intel MKL for 1 thread and 16 threads with automatic parallel execution are shown in blue. There are five subtasks showing a significant benefit from either optimized sequential math library or the automatic parallelization with MKL including <strong>crossprod</strong> (matrix size 2800*2800), <strong>linear regression</strong>, <strong>matrix decomposition</strong>, <strong>computing inverse</strong> and <strong>determinant of a matrix</strong>. Other non-computational intensive tasks received very little performance gains from parallel execution with MKL. <a href="/oneXPU/uploads/2016/04/image004.png"><img src="/oneXPU/uploads/2016/04/image004.png" alt="Speedup compared with OpenBLAS"></a></p>
<p>Fig.1 Performance comparison among  Intel MKL and NVIDIA BLAS against R+OpenBLAS</p>
<p>We also exploited parallelism with CUDA BLAS (libnvblas.so) on NVIDIA GPU platform. Since drop-in library (nvblas) only accelerated the level 3 BLAS functions and overhead of preloading, the result (green column) in Fig.2 showed little benefit and even worse performance for some computing tasks against Intel MKL accelerations.</p>
<p><a href="/oneXPU/uploads/2016/04/image005-3.png"><img src="/oneXPU/uploads/2016/04/image005-3-1024x674.png" alt="Speedup against Xeon"></a></p>
<p>Fig.2 Performance comparison for CPU and GPU with NVIDIA BLAS and Intel MKL</p>
<h4 id="2-Scalability-on-NVIDIA-GPU"><a href="#2-Scalability-on-NVIDIA-GPU" class="headerlink" title="(2) Scalability on NVIDIA GPU"></a><em>(2) Scalability on NVIDIA GPU</em></h4><p>The performance using two GPU devices (green column) is not superior to using one GPU device (blue column) , even the results of some subtasks on one GPU device gains more. Taking the function <strong>crossproduct</strong> with computing-intensive as an example is to explain the difference between one GPU device and two GPU device, as followed the Fig. 3. The advantage of the performance of the two card is gradually displayed as the size of the matrix increases. The sub-vertical axis shows the ratio of the elapsed time on two devices to one device. A ratio greater than 1 indicates that the two card performance is better than 1 cards,and the greater the ratio of the two cards, the better the performance of the card.  </p>
<p><a href="/oneXPU/uploads/2016/04/image007.png"><img src="/oneXPU/uploads/2016/04/image007.png" alt="Scalability on GPU with R"></a>Fig.3 Scalability for 1X and 2X NVIDIA K40m GPU for ‘crossprod’ function</p>
<h4 id="3-Heterogeneous-Parallel-Models-on-Intel-Xeon-Phi-MIC"><a href="#3-Heterogeneous-Parallel-Models-on-Intel-Xeon-Phi-MIC" class="headerlink" title="(3) Heterogeneous Parallel Models on Intel Xeon Phi (MIC)"></a><em>(3) Heterogeneous Parallel Models on Intel Xeon Phi (MIC)</em></h4><p>To compare the parallelism supported by pure CPU (Intel Xeon processor) and Intel Xeon Phi  coprocessor, we conducted <strong>batch runs</strong> (  10 times for the average elapsed time) with the different matrix size of matrix production. MKL supports <a target="_blank" rel="noopener" href="https://software.intel.com/sites/default/files/11MIC42_How_to_Use_MKL_Automatic_Offload_0.pdf"><strong>automatic offload</strong></a> computation to Intel Xeon Phi card, but before using you must know , Automatic offload functions in MKL</p>
<ul>
<li>  Level-3 BLAS: GEMM, TRSM, TRMM, SYMM</li>
<li>  LAPACK 3 amigos : LU, QR, Cholesky</li>
</ul>
<p>Matrix size for offloading</p>
<ul>
<li>  GEMM: M, N &gt;2048, K&gt;256</li>
<li>  SYMM: M, N &gt;2048</li>
<li>  TRSM/TRMM: M, N &gt;3072</li>
<li>  LU: M, N&gt;8192</li>
</ul>
<p>Here, we use <code>**a%*%a</code>** substituted for the function `crossprod` used in R-benchmark-25.R because <code>_crossprod_</code> can not be auto-offloaded to Intel Xeon Phi.  We compared the elapsed time running on CPU+Xeon Phi with running on pure CPU. In Fig.4, the vertical axis is the ratio of running elapsed time with CPU+Xeon Phi running mode to elapsed time with pure CPU running mode. The results showed the greater size of the matrix, the better performance CPU+Xeon Phi gains. The matrix size less than 4000 could get the best performance on pure CPU.  </p>
<p><a href="/oneXPU/uploads/2016/04/image009-2.png"><img src="/oneXPU/uploads/2016/04/image009-2-1024x674.png" alt="Heterogeneous Computing with Xeon and Xeon Phi for R"></a></p>
<p>Fig.4 Heterogeneous Computing with Intel Xeon and Intel Xeon Phi</p>
<p>Fig.5  shows the 80% computation on Xeon Phi could get the best performance as the matrix size is growing, 70% computation on Xeon Phi could get the steadily better performance when the matrix size larger than 2000. <a href="/oneXPU/uploads/2016/04/image008.png"><img src="/oneXPU/uploads/2016/04/image008-1024x674.png" alt="Scalability for Xeon and Xeon Phi for R"></a></p>
<p>Fig.5 Different computation ratio on Intel Xeon Phi result in different performance</p>
<h4 id="4-Comparison-NVIDIA-GPU-with-Intel-Xeon-Phi"><a href="#4-Comparison-NVIDIA-GPU-with-Intel-Xeon-Phi" class="headerlink" title="(4) Comparison NVIDIA GPU with Intel Xeon Phi"></a><em>(4) Comparison NVIDIA GPU with Intel Xeon Phi</em></h4><p>Here, we plotted the results of NVIDIA GPU and Intel Xeon Phi compared to Intel Xeon in Fig.6. In general, 80% running on Xeon Phi(2X 7110P)+Xeon CPU(2X E5-2670)  gets similar performance to 1X K40m+2X E5-2670(2X 7110P ~ 1X K40m). When the matrix size is less than 12000, GPU gets better performance than Xeon Phi. And after that, Intel Xeon Phi shows the similar performance with NVIDIA K40m. For this benchmark, it can clearly seen that NVIDIA’s Tesla GPU(2X K40m) outperforms significantly.At 16000 of matrix size, nearly 3.9x faster than the 8-core dual E5-2670(Sandy-Bridge CPU) and 2.3x faster than the 80% running on Xeon Phi. The Xeon Phi is 2.8x faster than the Sandy-Bridge.  </p>
<p><a href="/oneXPU/uploads/2016/04/111.png"><img src="/oneXPU/uploads/2016/04/111.png" alt="Intel Xeon Phi .vs. NVIDIA GPU"></a></p>
<p>Fig.6 Comparison NVIDIA GPU with Intel Xeon Phi</p>
<h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a><strong>Conclusions</strong></h2><p>In this article, we tested the R-benchmark-25.R script on the different hardware platform with different BLAS libraries. From our analysis, we concluded (1) R built with  Intel MKL (either sequential or threaded) can accelerate lots of computationally intensive algorithms of HPAC and get  the best performance, such as linear regression, PCA, SVD (2) R is performed faster on GPU for matrix production (GEMM) since it’s really computational intensive algorithm and GPU has more computing cores than Intel Xeon or Xeon Phi (3) R executed in the heterogeneous platforms (CPU+GPU or CPU+MIC) can gain more performance improvements (4) R can get more benefits from multiple GPUs, especially for large GEMM operations.   In the next post, we will further investigate the benchmark performance with different R parallel packages and commercial productions of R .  </p>
<hr>
<h2 id="Appendix-How-to-build-R-with-different-BLAS-library"><a href="#Appendix-How-to-build-R-with-different-BLAS-library" class="headerlink" title="Appendix : How to build R with different BLAS library"></a><strong>Appendix : How to build R with different BLAS library</strong></h2><h2 id="STOCK-R"><a href="#STOCK-R" class="headerlink" title="STOCK R"></a>STOCK R</h2><p>(1) Stock R build</p>
<p>Download base R package from the R project website, the current package is R-3.2.3.</p>
<p>Enter into the R root directory, and execute</p>
<blockquote>
<p>$./configure –with-readline=no –with-x=no –prefix=$HOME/R-3.2.3-ori</p>
</blockquote>
<blockquote>
<p>$make -j4</p>
</blockquote>
<blockquote>
<p>$make install</p>
</blockquote>
<p>(2) Add R bin directory and library directory to the environment variables PATH and LD_LIBRARY_PATH seperately, just like as:</p>
<blockquote>
<p>export PATH=$HOME/R-3.2.3-ori/bin:$PATH</p>
</blockquote>
<blockquote>
<p>export LD_LIBRARY_PATH=$HOME/R-3.2.3-ori/lib64/R/lib:$LD_LIBRARY_PATH</p>
</blockquote>
<h2 id="R-WITH-OPENBLAS"><a href="#R-WITH-OPENBLAS" class="headerlink" title="R WITH OPENBLAS"></a>R WITH OPENBLAS</h2><p>(1) OpenBLAS build</p>
<p>Download OpenBlas-0.2.15.tar.gz from <a target="_blank" rel="noopener" href="http://www.openblas.net/">http://www.openblas.net/</a></p>
<p>Change directory to OpenBLAS Home directory, and execute</p>
<blockquote>
<p>$make</p>
</blockquote>
<blockquote>
<p>$make PREFIX=$OPENBLAS_INSTALL_DIRECTORY install</p>
</blockquote>
<p>(2) Set the OpenBLAS library environment</p>
<p>(3) Run benchmark</p>
<blockquote>
<p>$LD_PRELOAD=$OPENBLAS_HOME/lib/libopenblas.so R</p>
</blockquote>
<h2 id="R-WITH-INTEL-MKL"><a href="#R-WITH-INTEL-MKL" class="headerlink" title="R WITH INTEL MKL"></a>R WITH INTEL MKL</h2><p>(1）Obtain Intel parallel studio software from Intel website</p>
<p>(2) Install the parallel studio</p>
<p>(3) Set the Intel compiler and MKL library environment</p>
<p>(4) Build R with MKL</p>
<p>Link MKL libraries configuration file mkl.conf as follows:</p>
<p>a. Sequencial MKL or MKL single thread</p>
<p>#make sure intel compiler is installed and loaded which can be set in .bashrc</p>
<h2 id="as-e-g"><a href="#as-e-g" class="headerlink" title="as e.g."></a>as e.g.</h2><p>source /opt/intel/bin/compilervars.sh intel64<br>MKL_LIB_PATH=/opt/intel/mkl/lib/intel64## Use intel compiler<br>CC=’icc -std=c99′<br>CFLAGS=’-g -O3 -wd188 -ip ‘F77=’ifort’<br>FFLAGS=’-g -O3 ‘CXX=’icpc’<br>CXXFLAGS=’-g -O3 ‘FC=’ifort’<br>FCFLAGS=’-g -O3 ‘## MKL sequential, ICC<br>MKL=” -L${MKL_LIB_PATH} <br>-Wl,–start-group <br>-lmkl_intel_lp64 <br>-lmkl_sequential <br>-lmkl_core <br>-Wl,–end-group”<br>b.  OpenMP Threading MKL</p>
<p>#make sure intel compiler is installed and loaded which can be set in .bashrc</p>
<h2 id="as-e-g-1"><a href="#as-e-g-1" class="headerlink" title="as e.g."></a>as e.g.</h2><p>source /opt/intel/bin/compilervars.sh intel64<br>MKL_LIB_PATH=/opt/intel/mkl/lib/intel64## Use intel compiler<br>CC=’icc -std=c99′<br>CFLAGS=’-g -O3 -wd188 -ip ‘F77=’ifort’<br>FFLAGS=’-g -O3 ‘CXX=’icpc’<br>CXXFLAGS=’-g -O3 ‘FC=’ifort’<br>FCFLAGS=’-g -O3 ‘## MKL With Intel MP threaded , ICC<br>MKL=” -L${MKL_LIB_PATH} <br>-Wl,–start-group <br>-lmkl_intel_lp64 <br>-lmkl_intel_thread <br>-lmkl_core <br>-Wl,–end-group <br>-liomp5 -lpthread”<br>build R with following command,</p>
<blockquote>
<p>$./configure –prefix=$HOME/R-3.2.3-mkl-icc –with-readline=no –with-x=no –with-blas=”$MKL” –with-lapack CC=’icc -std=c99′ CFLAGS=’-g -O3 -wd188 -ip ‘ F77=’ifort’ FFLAGS=’-g -O3 ‘ CXX=’icpc’ CXXFLAGS=’-g -O3 ‘ FC=’ifort’ FCFLAGS=’-g -O3 ‘</p>
</blockquote>
<blockquote>
<p>$make -j 4; make install</p>
</blockquote>
<p>(5) Set $HOME/R-3.2.3-mkl-icc environment</p>
<p> R WITH CUDA BLAS<br>(1) Install the driver and CUDA tools with version  up to 6.5 for NVIDIA Tesla Cards</p>
<p>(2)Set the CUDA environment</p>
<p>(3)Edit the nvblas.conf file</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This is the configuration file to use NVBLAS Library</span></span><br><span class="line"><span class="comment"># Setup the environment variable NVBLAS_CONFIG_FILE to specify your own config file.</span></span><br><span class="line"><span class="comment"># By default, if NVBLAS_CONFIG_FILE is not defined,</span></span><br><span class="line"><span class="comment"># NVBLAS Library will try to open the file “nvblas.conf” in its current directory</span></span><br><span class="line"><span class="comment"># Example : NVBLAS_CONFIG_FILE /home/cuda_user/my_nvblas.conf</span></span><br><span class="line"><span class="comment"># The config file should have restricted write permissions accesses# Specify which output log file (default is stderr)</span></span><br><span class="line">NVBLAS_LOGFILE nvblas.log<span class="comment">#Put here the CPU BLAS fallback Library of your choice</span></span><br><span class="line"><span class="comment">#It is strongly advised to use full path to describe the location of the CPU Library</span></span><br><span class="line">NVBLAS_CPU_BLAS_LIB /opt/R-3.2.3-ori/lib64/R/lib/libRblas.so</span><br><span class="line"><span class="comment">#NVBLAS_CPU_BLAS_LIB &amp;lt;mkl_path_installtion&amp;gt;/libmkl_rt.so# List of GPU devices Id to participate to the computation</span></span><br><span class="line"><span class="comment"># Use ALL if you want all your GPUs to contribute</span></span><br><span class="line"><span class="comment"># Use ALL0, if you want all your GPUs of the same type as device 0 to contribute</span></span><br><span class="line"><span class="comment"># However, NVBLAS consider that all GPU have the same performance and PCI bandwidth</span></span><br><span class="line"><span class="comment"># By default if no GPU are listed, only device 0 will be used#NVBLAS_GPU_LIST 0 2 4</span></span><br><span class="line"><span class="comment">#NVBLAS_GPU_LIST ALL</span></span><br><span class="line">NVBLAS_GPU_LIST ALL<span class="comment"># Tile Dimension</span></span><br><span class="line">NVBLAS_TILE_DIM 2048<span class="comment"># Autopin Memory</span></span><br><span class="line">NVBLAS_AUTOPIN_MEM_ENABLED<span class="comment">#List of BLAS routines that are prevented from running on GPU (use for debugging purpose</span></span><br><span class="line"><span class="comment"># The current list of BLAS routines supported by NVBLAS are</span></span><br><span class="line"><span class="comment"># GEMM, SYRK, HERK, TRSM, TRMM, SYMM, HEMM, SYR2K, HER2K#NVBLAS_GPU_DISABLED_SGEMM</span></span><br><span class="line"><span class="comment">#NVBLAS_GPU_DISABLED_DGEMM</span></span><br><span class="line"><span class="comment">#NVBLAS_GPU_DISABLED_CGEMM</span></span><br><span class="line"><span class="comment">#NVBLAS_GPU_DISABLED_ZGEMM# Computation can be optionally hybridized between CPU and GPU</span></span><br><span class="line"><span class="comment"># By default, GPU-supported BLAS routines are ran fully on GPU</span></span><br><span class="line"><span class="comment"># The option NVBLAS_CPU_RATIO_&amp;lt;BLAS_ROUTINE&amp;gt; give the ratio [0,1]</span></span><br><span class="line"><span class="comment"># of the amount of computation that should be done on CPU</span></span><br><span class="line"><span class="comment"># CAUTION : this option should be used wisely because it can actually</span></span><br><span class="line"><span class="comment"># significantly reduced the overall performance if too much work is given to CPU#NVBLAS_CPU_RATIO_CGEMM 0.07</span></span><br></pre></td></tr></table></figure>
<p>Set NVBLAS_CONFIG_FILE to the nvblas.conf location</p>
<p>(4) Run the benchmark</p>
<blockquote>
<p>LD_PRELOAD=/opt/cuda-7.5/lib64/libnvblas.so R</p>
</blockquote>
<h2 id="R-WITH-MKL-ON-INTEL-XEON-PHI"><a href="#R-WITH-MKL-ON-INTEL-XEON-PHI" class="headerlink" title="R WITH MKL ON INTEL XEON PHI"></a>R WITH MKL ON INTEL XEON PHI</h2><p>(1) Build R with MKL</p>
<p>Build R with MKL is same to Threaded MKL at 6</p>
<p>(2) Enable MKL  MIC Automatic Offload Mode</p>
<blockquote>
<p>export MKL_MIC_ENABLE=1</p>
</blockquote>
<blockquote>
<p>export MIC_KMP_AFFINITY=compact</p>
</blockquote>
<p>Otherwise , you can set the workload division between host CPU and MIC card. If one host has two MIC cards, you could set:</p>
<blockquote>
<p>export MKL_HOST_WORKDIVISION=0.2</p>
</blockquote>
<blockquote>
<p>export MKL_MIC_0_WORKDIVISION=0.4</p>
</blockquote>
<blockquote>
<p>export MKL_MIC_1_WORKDIVISION=0.4</p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jitmatrix.github.io/oneXPU/2016/03/08/r-dnn-parallel-acceleration/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/oneXPU/images/avatar.gif">
      <meta itemprop="name" content="Patric Zhao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ParallelR">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/oneXPU/2016/03/08/r-dnn-parallel-acceleration/" class="post-title-link" itemprop="url">R for Deep Learning (II): Achieve High-Performance DNN with Parallel Acceleration</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-03-08 14:18:34" itemprop="dateCreated datePublished" datetime="2016-03-08T14:18:34+00:00">2016-03-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-19 10:35:25" itemprop="dateModified" datetime="2020-12-19T10:35:25+00:00">2020-12-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/GPGPU/" itemprop="url" rel="index"><span itemprop="name">GPGPU</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/GPGPU/MultiCores/" itemprop="url" rel="index"><span itemprop="name">MultiCores</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/GPGPU/MultiCores/Performance-Optimizaiton/" itemprop="url" rel="index"><span itemprop="name">Performance Optimizaiton</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <hr>
<p><em>I would like to thank  <a target="_blank" rel="noopener" href="http://junma5.weebly.com/">Jun Ma</a>  and all other technical reviewers and readers for their informative comments and suggestions in this post.</em></p>
<hr>
<blockquote>
<p><em><strong><a href="courses.csail.mit.edu/18.337/2015/docs/50YearsDataScience.pdf">50 years of Data Science</a></strong>, David Donoho, 2015</em></p>
<p><em>**Computing with Data.**Every data scientist should know and use several languages for data analysis and data processing. These can include popular languages like R and Python …</em> <em>Beyond basic knowledge of languages, data scientists need to keep current on new idioms for efficiently using those languages and need to understand the deeper issues associated with computational efficiency.</em></p>
</blockquote>
<p>  In the previous post, <a target="_blank" rel="noopener" href="http://www.parallelr.com/r-deep-neural-network-from-scratch/">R for deep learning (I)</a>, I introduced the core components of neural networks and illustrated how to implement it from scratch by R. Now, I will focus on computational performance and efficiency of R’s implementation, especially for the parallel algorithm on multicores CPU and <a target="_blank" rel="noopener" href="http://www.nvidia.com/object/tesla-supercomputing-solutions.html">NVIDIA GPU</a> architectures.  </p>
<h2 id="Performance-Profiling"><a href="#Performance-Profiling" class="headerlink" title="Performance Profiling"></a>Performance Profiling</h2><p>In this post, we are going to a little big dataset, <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a>, for performance analysis. <a target="_blank" rel="noopener" href="http://yann.lecun.com/exdb/mnist/">MNIST</a> widely used to measure the accuracy of classification by handwritten digits in machine learning field, and also used for the competition in <a target="_blank" rel="noopener" href="https://www.kaggle.com/c/digit-recognizer">Kaggle</a> (data in download page or <a target="_blank" rel="noopener" href="http://www.parallelr.com/materials/3_ParDNN/">here</a>). <a target="_blank" rel="noopener" href="http://yann.lecun.com/">Yann</a> has provided the classification results based on various machine learning algorithms on his <a target="_blank" rel="noopener" href="http://yann.lecun.com/exdb/mnist/">page</a>. <a href="/oneXPU/uploads/2016/02/mnist.jpg"><img src="/oneXPU/uploads/2016/02/mnist.jpg" alt="mnist"></a></p>
<p>Picture.1 Handwritten Digits in MNIST dataset</p>
<p>The MNIST database contains 60,000 training images and 10,000 testing images. Each of image is represented by 28*28 points so totally  784 points. In this post, I will train the neural network with 784 points as input features and the number of 0-9 as output classes,  then compare the runtime of our R DNN code with <a target="_blank" rel="noopener" href="http://www.h2o.ai/verticals/algos/deep-learning/">H2O deep learning</a> implementations for 2-layers networks of the various number of hidden units (HU).</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># h2o</span></span><br><span class="line">library(h2o)</span><br><span class="line"><span class="comment"># single thread</span></span><br><span class="line">h2o.init()</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">train_file &lt;- <span class="string">&quot;https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/train.csv.gz&quot;</span></span><br><span class="line">test_file &lt;- <span class="string">&quot;https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/test.csv.gz&quot;</span></span><br><span class="line"> </span><br><span class="line">train &lt;- h2o.importFile(train_file)</span><br><span class="line">test  &lt;- h2o.importFile(test_file)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># To see a brief summary of the data, run the following command</span></span><br><span class="line">summary(train)</span><br><span class="line">summary(test)</span><br><span class="line"> </span><br><span class="line">y &lt;- <span class="string">&quot;C785&quot;</span></span><br><span class="line">x &lt;- setdiff(<span class="built_in">names</span>(train), y)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># We encode the response column as categorical for multinomial</span></span><br><span class="line"><span class="comment">#classification</span></span><br><span class="line">train[,y] &lt;- as.factor(train[,y])</span><br><span class="line">test[,y]  &lt;- as.factor(test[,y])</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Train a Deep Learning model and valid</span></span><br><span class="line">system.time(</span><br><span class="line">  model_cv &lt;- h2o.deeplearning(x = x,</span><br><span class="line">                               y = y,</span><br><span class="line">                               training_frame = train,</span><br><span class="line">                               distribution = <span class="string">&quot;multinomial&quot;</span>,</span><br><span class="line">                               activation = <span class="string">&quot;Rectifier&quot;</span>,</span><br><span class="line">                               hidden = <span class="built_in">c</span>(<span class="number">32</span>),</span><br><span class="line">                               l1 = <span class="number">1e-5</span>,</span><br><span class="line">                               epochs = <span class="number">200</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>As I know, H2O is  the fast and most popular deep learning package in R platform implemented by Java in the backend. Thus, it will be valuable to know how much performance gap between native R code and  the mature package. As below barplot shown, the hidden units of 32, 64 and 128 are tested in 200 steps. <a href="/oneXPU/uploads/2016/02/Runtime.RDNN_.H2O.png"><img src="/oneXPU/uploads/2016/02/Runtime.RDNN_.H2O.png" alt="Runtime.RDNN.H2O"></a> Obviously,  the R DNN is significantly slow than H2O, and the runtime increases with the number of hidden units quickly. For more details, we break down the R DNN runtime into each function call  by Rprof() and summaryRprof() which report out the final results including 4 parts: <em>total.time</em>, <em>total.pct</em>, <em>self.time</em> and <em>self.pct</em>. The <em>self.time</em> and <em>self.pct</em> columns represent the elapsed time for each function, excluding the time from its inner called functions. The <em>total.time</em> and <em>total.pct</em> columns mean the total elapsed time for each function including the time spent on function calls [<a target="_blank" rel="noopener" href="https://www.packtpub.com/application-development/r-high-performance-programming">Aloysius Lim</a>]. From the profiling results, we can see the top 1 time-consuming function is *<em>“%</em>%”** which represents matrix multiplications and usually people call it <strong>GEMM</strong> (GEneral Matrix Multiplication).  </p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">&gt; Rprof()</span><br><span class="line">&gt; mnist.model &lt;- train.dnn(x=<span class="number">1</span>:<span class="number">784</span>, y=<span class="number">785</span>, traindata=train, hidden=<span class="number">64</span>, maxit=<span class="number">200</span>, display=<span class="number">50</span>)</span><br><span class="line">&gt; Rprof(<span class="literal">NULL</span>)</span><br><span class="line">&gt; summaryRprof()</span><br><span class="line">$by.self</span><br><span class="line">                   self.time self.pct total.time total.pct</span><br><span class="line"><span class="string">&quot;%*%&quot;</span>                <span class="number">1250.08</span>    <span class="number">90.19</span>    <span class="number">1250.08</span>     <span class="number">90.19</span></span><br><span class="line"><span class="string">&quot;t.default&quot;</span>            <span class="number">61.62</span>     <span class="number">4.45</span>      <span class="number">61.62</span>      <span class="number">4.45</span></span><br><span class="line"><span class="string">&quot;pmax&quot;</span>                 <span class="number">24.40</span>     <span class="number">1.76</span>      <span class="number">28.42</span>      <span class="number">2.05</span></span><br><span class="line"><span class="string">&quot;aperm.default&quot;</span>        <span class="number">11.60</span>     <span class="number">0.84</span>      <span class="number">11.60</span>      <span class="number">0.84</span></span><br><span class="line"><span class="string">&quot;array&quot;</span>                <span class="number">10.36</span>     <span class="number">0.75</span>      <span class="number">10.36</span>      <span class="number">0.75</span></span><br><span class="line"><span class="string">&quot;train.dnn&quot;</span>             <span class="number">9.74</span>     <span class="number">0.70</span>    <span class="number">1386.00</span>    <span class="number">100.00</span></span><br><span class="line"><span class="string">&quot;&lt;=&quot;</span>                    <span class="number">5.72</span>     <span class="number">0.41</span>       <span class="number">5.72</span>      <span class="number">0.41</span></span><br><span class="line"><span class="string">&quot;mostattributes&lt;-&quot;</span>      <span class="number">4.02</span>     <span class="number">0.29</span>       <span class="number">4.02</span>      <span class="number">0.29</span></span><br><span class="line"><span class="string">&quot;exp&quot;</span>                   <span class="number">3.60</span>     <span class="number">0.26</span>       <span class="number">3.60</span>      <span class="number">0.26</span></span><br><span class="line"><span class="string">&quot;sweep&quot;</span>                 <span class="number">1.58</span>     <span class="number">0.11</span>     <span class="number">676.32</span>     <span class="number">48.80</span></span><br><span class="line"><span class="string">&quot;is.data.frame&quot;</span>         <span class="number">1.28</span>     <span class="number">0.09</span>       <span class="number">1.28</span>      <span class="number">0.09</span></span><br><span class="line"><span class="string">&quot;colSums&quot;</span>               <span class="number">0.86</span>     <span class="number">0.06</span>       <span class="number">0.86</span>      <span class="number">0.06</span></span><br><span class="line"><span class="string">&quot;/&quot;</span>                     <span class="number">0.52</span>     <span class="number">0.04</span>       <span class="number">0.52</span>      <span class="number">0.04</span></span><br><span class="line"><span class="string">&quot;rowSums&quot;</span>               <span class="number">0.36</span>     <span class="number">0.03</span>       <span class="number">0.36</span>      <span class="number">0.03</span></span><br><span class="line"><span class="string">&quot;unname&quot;</span>                <span class="number">0.18</span>     <span class="number">0.01</span>       <span class="number">1.46</span>      <span class="number">0.11</span></span><br><span class="line"><span class="string">&quot;-&quot;</span>                     <span class="number">0.04</span>     <span class="number">0.00</span>       <span class="number">0.04</span>      <span class="number">0.00</span></span><br><span class="line"><span class="string">&quot;t&quot;</span>                     <span class="number">0.02</span>     <span class="number">0.00</span>      <span class="number">61.64</span>      <span class="number">4.45</span></span><br><span class="line"><span class="string">&quot;sum&quot;</span>                   <span class="number">0.02</span>     <span class="number">0.00</span>       <span class="number">0.02</span>      <span class="number">0.00</span></span><br><span class="line"> </span><br><span class="line">$by.total</span><br><span class="line">                   total.time total.pct self.time self.pct</span><br><span class="line"><span class="string">&quot;train.dnn&quot;</span>           <span class="number">1386.00</span>    <span class="number">100.00</span>      <span class="number">9.74</span>     <span class="number">0.70</span></span><br><span class="line"><span class="string">&quot;%*%&quot;</span>                 <span class="number">1250.08</span>     <span class="number">90.19</span>   <span class="number">1250.08</span>    <span class="number">90.19</span></span><br><span class="line"><span class="string">&quot;sweep&quot;</span>                <span class="number">676.32</span>     <span class="number">48.80</span>      <span class="number">1.58</span>     <span class="number">0.11</span></span><br><span class="line"><span class="string">&quot;t&quot;</span>                     <span class="number">61.64</span>      <span class="number">4.45</span>      <span class="number">0.02</span>     <span class="number">0.00</span></span><br><span class="line"><span class="string">&quot;t.default&quot;</span>             <span class="number">61.62</span>      <span class="number">4.45</span>     <span class="number">61.62</span>     <span class="number">4.45</span></span><br><span class="line"><span class="string">&quot;pmax&quot;</span>                  <span class="number">28.42</span>      <span class="number">2.05</span>     <span class="number">24.40</span>     <span class="number">1.76</span></span><br><span class="line"><span class="string">&quot;aperm&quot;</span>                 <span class="number">21.96</span>      <span class="number">1.58</span>      <span class="number">0.00</span>     <span class="number">0.00</span></span><br><span class="line"><span class="string">&quot;aperm.default&quot;</span>         <span class="number">11.60</span>      <span class="number">0.84</span>     <span class="number">11.60</span>     <span class="number">0.84</span></span><br><span class="line"><span class="string">&quot;array&quot;</span>                 <span class="number">10.36</span>      <span class="number">0.75</span>     <span class="number">10.36</span>     <span class="number">0.75</span></span><br><span class="line"><span class="string">&quot;&lt;=&quot;</span>                     <span class="number">5.72</span>      <span class="number">0.41</span>      <span class="number">5.72</span>     <span class="number">0.41</span></span><br><span class="line"><span class="string">&quot;mostattributes&lt;-&quot;</span>       <span class="number">4.02</span>      <span class="number">0.29</span>      <span class="number">4.02</span>     <span class="number">0.29</span></span><br><span class="line"><span class="string">&quot;exp&quot;</span>                    <span class="number">3.60</span>      <span class="number">0.26</span>      <span class="number">3.60</span>     <span class="number">0.26</span></span><br><span class="line"><span class="string">&quot;unname&quot;</span>                 <span class="number">1.46</span>      <span class="number">0.11</span>      <span class="number">0.18</span>     <span class="number">0.01</span></span><br><span class="line"><span class="string">&quot;is.data.frame&quot;</span>          <span class="number">1.28</span>      <span class="number">0.09</span>      <span class="number">1.28</span>     <span class="number">0.09</span></span><br><span class="line"><span class="string">&quot;data.matrix&quot;</span>            <span class="number">1.28</span>      <span class="number">0.09</span>      <span class="number">0.00</span>     <span class="number">0.00</span></span><br><span class="line"><span class="string">&quot;colSums&quot;</span>                <span class="number">0.86</span>      <span class="number">0.06</span>      <span class="number">0.86</span>     <span class="number">0.06</span></span><br><span class="line"><span class="string">&quot;/&quot;</span>                      <span class="number">0.52</span>      <span class="number">0.04</span>      <span class="number">0.52</span>     <span class="number">0.04</span></span><br></pre></td></tr></table></figure>
<h2 id="Parallel-Acceleration"><a href="#Parallel-Acceleration" class="headerlink" title="Parallel Acceleration"></a>Parallel Acceleration</h2><p>From above analysis,  the matrix multiplication (“%*%”) accounts for about 90% computation time in the training stage of the neural network. Thus, the key of DNN acceleration is to speed up matrix multiplication.  Fortunately, there are already several parallel libraries for matrix multiplication and we can deploy it to R easily.  In this post, I will introduce three basic linear algebra subprograms (<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms">BLAS</a>) libraries, <a target="_blank" rel="noopener" href="http://www.openblas.net/">openBLAS</a>, <a target="_blank" rel="noopener" href="https://software.intel.com/en-us/intel-mkl">Intel MKL</a>, and <a target="_blank" rel="noopener" href="https://developer.nvidia.com/cublas">cuBLAS</a>. (In another <a target="_blank" rel="noopener" href="http://www.parallelr.com/r-hpac-benchmark-analysis/">blog</a>, moreover, we show their performance on modern hardware architectures and instructions of installation). The former two are multithread accelerated libraries which need to be built with R together (instructions in <a target="_blank" rel="noopener" href="http://www.r-bloggers.com/compile-r-and-openblas-from-source-guide/">here</a> and <a target="_blank" rel="noopener" href="https://software.intel.com/en-us/articles/using-intel-mkl-with-r">here</a>). On the other hand, it is indeed easy to apply NVIDIA cuBLAS library in R on Linux system by  the drop-on wrap, <a target="_blank" rel="noopener" href="http://docs.nvidia.com/cuda/nvblas/">nvBLAS</a>, which can be preloaded into R in order to hijack original Rblas.so. By this way, we can leverage NVIDIA GPU’s power into R with zero programming efforts :) The below picture shows the architecture of R with BLAS libraries. Typically, R will call their own BLAS, Rblas.so,on Linux system which handles all kinds of linear algebra functions (<a target="_blank" rel="noopener" href="http://www.netlib.org/blas/#_blas_routines">here</a>), but it is a single thread implementation. The trick to speed up the linear algebra calculations is to update the R standard BLAS to modern multithread libraries. For R developers, there is almost a ‘free lunch’  without rewriting their codes for parallel accelerations. <a href="/oneXPU/uploads/2016/02/R_BLAS.png"><img src="/oneXPU/uploads/2016/02/R_BLAS-300x144.png" alt="R_BLAS"></a> As below code shown, we tested prebuilt R  with openBLAS,  Intel MKL and  nvBLAS  for 2-layers neural network of 32, 64 and 128 neurons.From the below bar chart, the runtime of R DNN are dramatically decreased and it is nearly <strong>2X</strong> faster than H2O and <strong>9X</strong> speedup (from 2816 to 365 for 128 hidden neurons network) than original R code!</p>
<blockquote>
<p># you must create a configuration file nvBLAS.conf in the current directory, example in <a target="_blank" rel="noopener" href="http://docs.nvidia.com/cuda/nvblas/#configuration_example">here</a>. &gt;LD_PRELOAD=libnvblas.so /home/patricz/tools/R-3.2.0/bin/bin/R CMD <a target="_blank" rel="noopener" href="http://inside-r.org/packages/cran/batch">BATCH</a> MNIST_DNN.R</p>
</blockquote>
<p><a href="/oneXPU/uploads/2016/02/rdnn_gemm.png"><img src="/oneXPU/uploads/2016/02/rdnn_gemm.png" alt="rdnn_mnist_blas_gemm"></a></p>
<blockquote>
<p>Note: Testing hardware: Ivy Bridge E5-2690 v2 @ 3.00GHz, dual socket 10-core (total 20 cores), 128G RAM;   NVIDIA GPU K40m;  Software:  CUDA 7.5,  OpenBLAS 0.2.8,  Intel MKL 11.1</p>
</blockquote>
<h2 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h2><p>Till now, it seems everything is great and we gain lots of performance increments from BLAS libraries under multicores CPU and NVIDIA GPU system. <strong>But can we do a little more for performance optimizations?</strong> Let’s look into the profiling again and probe the top performance limiters. The below table shows the breakdown of R DNN code under acceleration by nvBLAS where the GEMM performance is 10X faster ( from original 1250 to 114 seconds ).  But the next two top time-consuming functions, “sweep()” and “t()”, account for 27% (<em>self.pct</em>) runtime.Therefore, it makes sense to do further optimization for them.</p>
<blockquote>
<p><strong>A question for readers:</strong> <strong>The total time of ‘sweep’ is 87.28 but the self-time is only 1.8, so what are the real computation parts and is it a reasonable choose to optimize it?</strong></p>
</blockquote>
<p><a href="/oneXPU/uploads/2016/02/runtime.breakdown.nvblas.png"><img src="/oneXPU/uploads/2016/02/runtime.breakdown.nvblas.png" alt="DNN.runtime.breakdown.nvblas"></a> From the source code, there are several function calls of t()  to transfer matrix before multiplication; however, R has already provided the inner function `crossprod` and `tcrossprod` for this kind of operations.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># original:  t() with matrix multiplication</span></span><br><span class="line">dW2     &lt;- t(hidden.layer) %*% dscores </span><br><span class="line">dhidden &lt;- dscores %*% t(W2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Opt1: use builtin function</span></span><br><span class="line">dW2     &lt;- crossprod(hidden.layer, dscores)</span><br><span class="line">dhidden &lt;- tcrossprod(dscores, W2)</span><br></pre></td></tr></table></figure>
<p>Secondly, the `sweep()` is performed to add the matrix with bias. Alternatively, we can combine weight and bias together and then use matrix multiplications, as below code shown. But the backside is that we have to create the new matrix for combinations of matrix and bias which increases memory pressure.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Opt2: combine data and add 1 column for bias</span></span><br><span class="line"><span class="comment">#  extra matrix for combinations</span></span><br><span class="line">X1   &lt;- cbind(X, <span class="built_in">rep</span>(<span class="number">1</span>, nrow(X)))</span><br><span class="line">W1b1 &lt;- rbind(W1, b1)</span><br><span class="line">W2b2 &lt;- rbind(W2, b2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Opt2: remove `sweep` </span></span><br><span class="line"><span class="comment">#hidden.layer &lt;- sweep(X %*% W1 ,2, b1, &#x27;+&#x27;)</span></span><br><span class="line">hidden.layer &lt;- X1 %*% W1b1</span><br><span class="line"></span><br><span class="line"><span class="comment">#score &lt;- sweep(hidden.layer %*% W2, 2, b2, &#x27;+&#x27;)</span></span><br><span class="line">hidden.layer1 &lt;- cbind(hidden.layer, <span class="built_in">rep</span>(<span class="number">1</span>,nrow(hidden.layer)))</span><br><span class="line">score &lt;- hidden.layer1 %*% W2b2</span><br></pre></td></tr></table></figure>
<p><a href="/oneXPU/uploads/2016/03/opt2.png"><img src="/oneXPU/uploads/2016/03/opt2.png" alt="bias optimization"></a> Now, we profile and compare the results again with below table. We save the computation time of ‘t.default’ and ‘aperm.default’ after removing ‘t()’ and ‘sweep()’ functions. Totally, the performance is <strong>DOUBLE</strong> again! <a href="/oneXPU/uploads/2016/02/DNN.Optimization.png"><img src="/oneXPU/uploads/2016/02/DNN.Optimization.png" alt="DNN.Optimization"></a></p>
<blockquote>
<p><strong>Another question:</strong> <strong>What is your opinion about the next step of optimizations? Any good idea and tell me your numbers  :)</strong></p>
</blockquote>
<p> </p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>In this post, we have introduced the coarse granularity parallelism skill to accelerate R code by BLAS libraries on multicores CPU and NVIDIA GPU architectures. Till now, we have got <strong>+10X</strong> speedup under NVIDIA GPU and <strong>2X</strong> faster than 20-threads H2O packages for a relatively small network; however, there are still lots of space to improve, take a try. <a href="/oneXPU/uploads/2016/02/RDNN_nvBLAS_runtime.png"><img src="/oneXPU/uploads/2016/02/RDNN_nvBLAS_runtime.png" alt="RDNN_nvBLAS_runtime"></a> And finally we train MNIST dataset with a 2 layer neural network of 300 hidden unit on Tesla K40m GPU, and we reach 94.7% accuracy (5.3% error rate)  in an hour while <a target="_blank" rel="noopener" href="http://yann.lecun.com/exdb/mnist/">Yann</a> got 4.7% error rate with smaller learning rate ( lr=0.001)  which will cost longer training time but more accurate.   <a href="/oneXPU/uploads/2016/03/MNIST.png"><img src="/oneXPU/uploads/2016/03/MNIST.png" alt="neural network training results"></a> <strong>In the next <a target="_blank" rel="noopener" href="http://www.parallelr.com/r-dnn-cuda-multigpu/">blog post</a>, I will continue deep learning topic and focus on CUDA integration and multiple GPUs accelerations.</strong></p>
<hr>
<p>Notes: 1. The entire source code of this post in <a target="_blank" rel="noopener" href="https://github.com/PatricZhao/ParallelR">here</a> 2. The PDF version of this post in <a target="_blank" rel="noopener" href="http://www.parallelr.com/materials/3_ParDNN/3_.DNN_Parallel_Acceleration.pdf">here</a> 3. Pretty R syntax in this blog is <a target="_blank" rel="noopener" href="http://www.inside-r.org/pretty-r" title="Created by Pretty R at inside-R.org">Created by inside-R .org</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jitmatrix.github.io/oneXPU/2016/02/13/r-deep-neural-network-from-scratch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/oneXPU/images/avatar.gif">
      <meta itemprop="name" content="Patric Zhao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ParallelR">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/oneXPU/2016/02/13/r-deep-neural-network-from-scratch/" class="post-title-link" itemprop="url">R for Deep Learning (I):  Build Fully Connected Neural Network from Scratch</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-02-13 14:08:47" itemprop="dateCreated datePublished" datetime="2016-02-13T14:08:47+00:00">2016-02-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-19 10:35:25" itemprop="dateModified" datetime="2020-12-19T10:35:25+00:00">2020-12-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/General/" itemprop="url" rel="index"><span itemprop="name">General</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Backgrounds"><a href="#Backgrounds" class="headerlink" title="Backgrounds"></a>Backgrounds</h2><p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Deep_learning">Deep Neural Network (DNN)</a> has made a great progress in recent years in image recognition, natural language processing and automatic driving fields, such as Picture.1 shown from 2012  to 2015 DNN improved <a target="_blank" rel="noopener" href="http://image-net.org/challenges/LSVRC/2015/">IMAGNET</a>’s accuracy from ~80% to ~95%, which really beats traditional computer vision (CV) methods. <a href="/oneXPU/uploads/2016/02/ces2016.png"><img src="/oneXPU/uploads/2016/02/ces2016.png" alt="Jensen&#39;s CES2016 talk" title="From NVIDIA CEO Jensen&#39;s talk in CES16"></a></p>
<p>Picture.1 - <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLZHnYvH1qtObAdk3waYMalhA8bdbhyTm2">From NVIDIA CEO Jensen’s talk in CES16</a></p>
<p>In this post, we will focus on fully connected neural networks which are commonly called DNN in data science. The biggest advantage of DNN is to extract and learn features automatically by deep layers architecture, especially for these complex and high-dimensional data that feature engineers can’t capture easily, examples in <a target="_blank" rel="noopener" href="http://blog.kaggle.com/2014/08/01/learning-from-the-best/">Kaggle</a>. Therefore, DNN is also very attractive to data scientists and there are lots of successful cases as well in classification, time series, and recommendation system, such as <a target="_blank" rel="noopener" href="http://blog.dominodatalab.com/using-r-h2o-and-domino-for-a-kaggle-competition/">Nick’s post</a> and <a target="_blank" rel="noopener" href="http://www.r-bloggers.com/using-neural-networks-for-credit-scoring-a-simple-example/">credit scoring</a> by DNN. In CRAN and R’s community, there are several popular and mature DNN packages including <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/nnet/index.html">nnet</a>, <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/neuralnet/">nerualnet</a>, <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/h2o/index.html">H2O</a>, <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/darch/index.html">DARCH</a>, <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/deepnet/index.html">deepnet </a>and <a target="_blank" rel="noopener" href="https://github.com/dmlc/mxnet">mxnet</a>,  and I strong recommend <a target="_blank" rel="noopener" href="http://www.h2o.ai/verticals/algos/deep-learning/">H2O DNN algorithm and R interface</a>. <a href="/oneXPU/uploads/2016/02/matrureDNNPackages-2.png"><img src="/oneXPU/uploads/2016/02/matrureDNNPackages-2.png"></a>   So, <strong>why we need to build DNN from scratch at all?</strong> - Understand how neural network works Using existing DNN package, you only need one line R code for your DNN model in most of the time and there is <a target="_blank" rel="noopener" href="http://www.r-bloggers.com/fitting-a-neural-network-in-r-neuralnet-package/">an example</a> by neuralnet. For the inexperienced user, however, the processing and results may be difficult to understand.  Therefore, it will be a valuable practice to implement your own network in order to understand more details from mechanism and computation views. - Build specified network with your new ideas DNN is one of rapidly developing area. Lots of novel works and research results are published in the top journals and Internet every week, and the users also have their specified neural network configuration to meet their problems such as different activation functions, loss functions, regularization, and connected graph. On the other hand, the existing packages are definitely behind the latest researches, and almost all existing packages are written in C/C++, Java so it’s not flexible to apply latest changes and your ideas into the packages. - Debug and visualize network and data As we mentioned, the existing DNN package is highly assembled and written by low-level languages so that it’s a nightmare to debug the network layer by layer or node by node. Even it’s not easy to visualize the results in each layer, monitor the data or weights changes during training, and show the discovered patterns in the network.  </p>
<h2 id="Fundamental-Concepts-and-Components"><a href="#Fundamental-Concepts-and-Components" class="headerlink" title="Fundamental Concepts and Components"></a>Fundamental Concepts and Components</h2><p><a target="_blank" rel="noopener" href="http://lo.epfl.ch/files/content/sites/lo/files/shared/1990/IEEE_78_1637_Oct1990.pdf">Fully connected neural network</a>, called DNN in data science, is that adjacent network layers are fully connected to each other. Every neuron in the network is connected to every neuron in adjacent layers. A very simple and typical neural network is shown below with 1 input layer, 2 hidden layers, and 1 output layer. Mostly, when researchers talk about network’s architecture, it refers to the configuration of DNN, such as how many layers in the network, how many neurons in each layer, what kind of activation, loss function, and regularization are used. <a href="/oneXPU/uploads/2016/02/dnn_architecture.png"><img src="/oneXPU/uploads/2016/02/dnn_architecture.png" alt="arch"></a> Now, we will go through the basic components of DNN and show you how it is implemented in R. <strong>Weights and Bias</strong> Take above DNN architecture, for example, there are 3 groups of weights from the input layer to first hidden layer, first to second hidden layer and second hidden layer to output layer. Bias unit links to every hidden node and which affects the output scores, but without interacting with the actual data. In our R implementation, we represent weights and bias by the matrix. Weight size is defined by,</p>
<blockquote>
<p>  (number of neurons layer M) X (number of neurons in layer M+1)</p>
</blockquote>
<p>and weights are initialized by random number from rnorm. Bias is just a one dimension matrix with the same size of  neurons and set to zero. Other initialization approaches, such as calibrating the variances with 1/sqrt(n) and sparse initialization, are introduced in <a target="_blank" rel="noopener" href="http://cs231n.github.io/neural-networks-2/#init">weight initialization</a> part of Stanford CS231n. Pseudo R code:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">weight.i &lt;- 0.01*matrix(rnorm(layer size of (i) * layer size of (i+<span class="number">1</span>)),</span><br><span class="line">                        nrow=layer size of (i),</span><br><span class="line">                        ncol=layer size of (i+<span class="number">1</span>))</span><br><span class="line">bias.i &lt;- matrix(<span class="number">0</span>, nrow=<span class="number">1</span>, ncol = layer size of (i+<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>Another common implementation approach combines weights and bias together so that the dimension of input is N+1 which indicates N input features with 1 bias, as below code:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">weight &lt;- 0.01*matrix(rnorm((layer size of (i) +<span class="number">1</span>) * layer size of (i+<span class="number">1</span>)),</span><br><span class="line">                            nrow=layer size of (i) +<span class="number">1</span>,</span><br><span class="line">                            ncol=layer size of (i+<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p><strong>Neuron</strong> A neuron is a basic unit in the DNN which is biologically inspired model of the human neuron. A single neuron performs weight and input multiplication and addition (FMA), which is as same as the linear regression in data science, and then FMA’s result is passed to the activation function. The commonly used activation functions include <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid</a>, <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLu</a>, <a target="_blank" rel="noopener" href="https://reference.wolfram.com/language/ref/Tanh.html">Tanh</a> and Maxout. In this post, I will take the rectified linear unit (ReLU)  as activation function,  <code>f(x) = max(0, x)</code>. For other types of activation function, you can refer <a target="_blank" rel="noopener" href="http://cs231n.github.io/neural-networks-1/#actfun">here</a>. <a href="/oneXPU/uploads/2016/02/neuron.png"><img src="/oneXPU/uploads/2016/02/neuron.png" alt="neuron"></a> In R, we can implement neuron by various methods, such as <code>sum(xi*wi)</code>. But, more efficient representation is by matrix multiplication. R code:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">neuron.ij &lt;- <span class="built_in">max</span>(<span class="number">0</span>, input %*% weight + bias)</span><br></pre></td></tr></table></figure>
<p><strong><em>Implementation Tips</em></strong> _In practice, we always update all neurons in a layer with a batch of examples for performance consideration. Thus, the above code will not work correctly.</p>
<ol>
<li>Matrix Multiplication and Addition</li>
</ol>
<p>As below code shown,  <code>input %*% weights</code> and <code>bias</code> with different dimensions and  it can’t  be added directly. Two solutions are provided. The first one repeats bias_ ncol _times, however, it will waste lots of memory in big data input. Therefore, the second approach is better.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dimension: 2X2</span></span><br><span class="line">input &lt;- matrix(<span class="number">1</span>:<span class="number">4</span>, nrow=<span class="number">2</span>, ncol=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># dimension: 2x3</span></span><br><span class="line">weights &lt;- matrix(<span class="number">1</span>:<span class="number">6</span>, nrow=<span class="number">2</span>, ncol=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># dimension: 1*3</span></span><br><span class="line">bias &lt;- matrix(<span class="number">1</span>:<span class="number">3</span>, nrow=<span class="number">1</span>, ncol=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># doesn&#x27;t work since unmatched dimension</span></span><br><span class="line">input %*% weights + bias</span><br><span class="line">Error input %*% weights + bias : non-conformable arrays </span><br><span class="line"> </span><br><span class="line"><span class="comment"># solution 1: repeat bias aligned to 2X3 </span></span><br><span class="line">s1 &lt;- input %*% weights + matrix(<span class="built_in">rep</span>(bias, each=<span class="number">2</span>), ncol=<span class="number">3</span>) </span><br><span class="line"> </span><br><span class="line"><span class="comment"># solution 2: sweep addition</span></span><br><span class="line">s2 &lt;- sweep(input %*% weights ,<span class="number">2</span>, bias, <span class="string">&#x27;+&#x27;</span>)</span><br><span class="line"> </span><br><span class="line">all.equal(s1, s2)</span><br><span class="line"><span class="comment"># [1] TRUE</span></span><br></pre></td></tr></table></figure>
<ol start="2">
<li>Element-wise max value for a matrix</li>
</ol>
<p>Another trick in here is to replace <code>max</code> by <code>pmax</code> to get element-wise maximum value instead of a global one, and be careful of the order in <code>pmax</code> :)</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the original matrix</span></span><br><span class="line">&gt; s1</span><br><span class="line">     [,<span class="number">1</span>] [,<span class="number">2</span>] [,<span class="number">3</span>]</span><br><span class="line">[<span class="number">1</span>,]    <span class="number">8</span>   <span class="number">17</span>   <span class="number">26</span></span><br><span class="line">[<span class="number">2</span>,]   <span class="number">11</span>   <span class="number">24</span>   <span class="number">37</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># max returns global maximum </span></span><br><span class="line">&gt; <span class="built_in">max</span>(<span class="number">0</span>, s1)</span><br><span class="line">[<span class="number">1</span>] <span class="number">37</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># s1 is aligned with a scalar, so the matrix structure is lost</span></span><br><span class="line">&gt; pmax(<span class="number">0</span>, s1)</span><br><span class="line">[<span class="number">1</span>]  <span class="number">8</span> <span class="number">11</span> <span class="number">17</span> <span class="number">24</span> <span class="number">26</span> <span class="number">37</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># correct </span></span><br><span class="line"><span class="comment"># put matrix in the first, the scalar will be recycled to match matrix structure</span></span><br><span class="line">&gt; pmax(s1, <span class="number">0</span>)</span><br><span class="line">     [,<span class="number">1</span>] [,<span class="number">2</span>] [,<span class="number">3</span>]</span><br><span class="line">[<span class="number">1</span>,]    <span class="number">8</span>   <span class="number">17</span>   <span class="number">26</span></span><br><span class="line">[<span class="number">2</span>,]   <span class="number">11</span>   <span class="number">24</span>   <span class="number">37</span></span><br></pre></td></tr></table></figure>
<p><strong>Layer</strong></p>
<ul>
<li>Input Layer</li>
</ul>
<p>the input layer is relatively fixed with only 1 layer and the unit number is equivalent to the number of features in the input data.</p>
<ul>
<li>Hidden layers</li>
</ul>
<p>Hidden layers are very various and it’s the core component in DNN. But in general,  more hidden layers are needed to capture desired patterns in case the problem is more complex (non-linear).</p>
<ul>
<li>Output Layer</li>
</ul>
<p>The unit in output layer most commonly does not have an activation because it is usually taken to represent the class scores in classification and arbitrary real-valued numbers in regression. For classification, the number of output units matches the number of categories of prediction while there is only one output node for regression.  </p>
<h2 id="Build-Neural-Network-Architecture-Prediction-and-Training"><a href="#Build-Neural-Network-Architecture-Prediction-and-Training" class="headerlink" title="Build Neural Network: Architecture, Prediction, and Training"></a>Build Neural Network: Architecture, Prediction, and Training</h2><p>Till now, we have covered the basic concepts of deep neural network and we are going to build a neural network now, which includes determining the network architecture, training network and then predict new data with the learned network. To make things simple, we use a small data set, Edgar Anderson’s Iris Data (<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Iris_flower_data_set">iris</a>) to do classification by DNN.</p>
<p><strong>Network Architecture</strong></p>
<p>IRIS is well-known built-in dataset in stock R for machine learning. So you can take a look at this dataset by the <code>summary</code> at the console directly as below.</p>
<p>R code:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">summary(iris)</span><br><span class="line">  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width          Species  </span><br><span class="line"> Min.   :<span class="number">4.300</span>   Min.   :<span class="number">2.000</span>   Min.   :<span class="number">1.000</span>   Min.   :<span class="number">0.100</span>   setosa    :<span class="number">50</span>  </span><br><span class="line"> <span class="number">1</span>st Qu.:<span class="number">5.100</span>   <span class="number">1</span>st Qu.:<span class="number">2.800</span>   <span class="number">1</span>st Qu.:<span class="number">1.600</span>   <span class="number">1</span>st Qu.:<span class="number">0.300</span>   versicolor:<span class="number">50</span>  </span><br><span class="line"> Median :<span class="number">5.800</span>   Median :<span class="number">3.000</span>   Median :<span class="number">4.350</span>   Median :<span class="number">1.300</span>   virginica :<span class="number">50</span>  </span><br><span class="line"> Mean   :<span class="number">5.843</span>   Mean   :<span class="number">3.057</span>   Mean   :<span class="number">3.758</span>   Mean   :<span class="number">1.199</span>                  </span><br><span class="line"> <span class="number">3</span>rd Qu.:<span class="number">6.400</span>   <span class="number">3</span>rd Qu.:<span class="number">3.300</span>   <span class="number">3</span>rd Qu.:<span class="number">5.100</span>   <span class="number">3</span>rd Qu.:<span class="number">1.800</span>                  </span><br><span class="line"> Max.   :<span class="number">7.900</span>   Max.   :<span class="number">4.400</span>   Max.   :<span class="number">6.900</span>   Max.   :<span class="number">2.500</span></span><br></pre></td></tr></table></figure>
<p>From the summary, there are four features and three categories of Species. So we can design a DNN architecture as below. <a target="_blank" rel="noopener" href="http://www.parallelr.com/r-deep-neural-network-from-scratch/iris_network/"><img src="/oneXPU/uploads/2016/02/iris_network.png" alt="iris_network"></a>   And then we will keep our DNN model in a list, which can be used for retrain or prediction, as below. Actually, we can keep more interesting parameters in the model with great flexibility.</p>
<p>R code:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">List of <span class="number">7</span></span><br><span class="line"> $ D : int <span class="number">4</span></span><br><span class="line"> $ H : num <span class="number">6</span></span><br><span class="line"> $ K : int <span class="number">3</span></span><br><span class="line"> $ W1: num [<span class="number">1</span>:<span class="number">4</span>, <span class="number">1</span>:<span class="number">6</span>] <span class="number">1.34994</span> <span class="number">1.11369</span> -<span class="number">0.57346</span> -<span class="number">1.12123</span> -<span class="number">0.00107</span> ...</span><br><span class="line"> $ b1: num [<span class="number">1</span>, <span class="number">1</span>:<span class="number">6</span>] <span class="number">1.336621</span> -<span class="number">0.509689</span> -<span class="number">0.000277</span> -<span class="number">0.473194</span> <span class="number">0</span> ...</span><br><span class="line"> $ W2: num [<span class="number">1</span>:<span class="number">6</span>, <span class="number">1</span>:<span class="number">3</span>] <span class="number">1.31464</span> -<span class="number">0.92211</span> -<span class="number">0.00574</span> -<span class="number">0.82909</span> <span class="number">0.00312</span> ...</span><br><span class="line"> $ b2: num [<span class="number">1</span>, <span class="number">1</span>:<span class="number">3</span>] <span class="number">0.581</span> <span class="number">0.506</span> -<span class="number">1.088</span></span><br></pre></td></tr></table></figure>
<p><strong>Prediction</strong> Prediction, also called classification or inference in machine learning field, is concise compared with training, which walks through the network layer by layer from input to output by matrix multiplication. In output layer, the activation function doesn’t need. And for classification, the probabilities will be calculated by <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a>  while for regression the output represents the real value of predicted.   This process is called feed forward or feed propagation.</p>
<p>R code:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Prediction</span></span><br><span class="line">predict.dnn &lt;- <span class="keyword">function</span>(model, data = X.test) &#123;</span><br><span class="line">  <span class="comment"># new data, transfer to matrix</span></span><br><span class="line">  new.data &lt;- data.matrix(data)</span><br><span class="line"> </span><br><span class="line">  <span class="comment"># Feed Forwad</span></span><br><span class="line">  hidden.layer &lt;- sweep(new.data %*% model$W1 ,<span class="number">2</span>, model$b1, <span class="string">&#x27;+&#x27;</span>)</span><br><span class="line">  <span class="comment"># neurons : Rectified Linear</span></span><br><span class="line">  hidden.layer &lt;- pmax(hidden.layer, <span class="number">0</span>)</span><br><span class="line">  score &lt;- sweep(hidden.layer %*% model$W2, <span class="number">2</span>, model$b2, <span class="string">&#x27;+&#x27;</span>)</span><br><span class="line"> </span><br><span class="line">  <span class="comment"># Loss Function: softmax</span></span><br><span class="line">  score.exp &lt;- <span class="built_in">exp</span>(score)</span><br><span class="line">  probs &lt;-sweep(score.exp, <span class="number">1</span>, rowSums(score.exp), <span class="string">&#x27;/&#x27;</span>) </span><br><span class="line"> </span><br><span class="line">  <span class="comment"># select max possiblity</span></span><br><span class="line">  labels.predicted &lt;- max.col(probs)</span><br><span class="line">  <span class="built_in">return</span>(labels.predicted)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Training</strong></p>
<p>Training is to search the optimization parameters (weights and bias) under the given network architecture and minimize the classification error or residuals.  This process includes two parts: feed forward and back propagation. Feed forward is going through the network with input data (as prediction parts) and then compute data loss in the output layer by <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Loss_function">loss function</a> (cost function). “<em>Data loss</em> measures the compatibility between a prediction (e.g. the class scores in classification) and the ground truth label.” In our example code, we selected cross-entropy function to evaluate data loss, see detail in <a target="_blank" rel="noopener" href="http://cs231n.github.io/neural-networks-2/#losses">here</a>. After getting data loss, we need to minimize the data loss by changing the weights and bias. The very popular method is to back-propagate the loss into every layers and neuron by <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a> or  <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">stochastic gradient descent</a> which requires derivatives of data loss for each parameter (W1, W2, b1, b2). And <a target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-08-Backprop/">back propagation</a> will be different for different activation functions and see <a target="_blank" rel="noopener" href="http://mochajl.readthedocs.org/en/latest/user-guide/neuron.html">here</a> and <a target="_blank" rel="noopener" href="http://like.silk.to/studymemo/ChainRuleNeuralNetwork.pdf">here</a> for their derivatives formula and method, and <a target="_blank" rel="noopener" href="http://cs231n.github.io/neural-networks-3/">Stanford CS231n</a> for more training tips. In our example, the point-wise derivative for ReLu is: <a target="_blank" rel="noopener" href="http://www.parallelr.com/r-deep-neural-network-from-scratch/class-relu/"><img src="/oneXPU/uploads/2016/02/class.relu_.png" alt="class.relu"></a></p>
<p>R code:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train: build and train a 2-layers neural network </span></span><br><span class="line">train.dnn &lt;- <span class="keyword">function</span>(x, y, traindata=data, testdata=<span class="literal">NULL</span>,</span><br><span class="line">                  <span class="comment"># set hidden layers and neurons</span></span><br><span class="line">                  <span class="comment"># currently, only support 1 hidden layer</span></span><br><span class="line">                  hidden=<span class="built_in">c</span>(<span class="number">6</span>), </span><br><span class="line">                  <span class="comment"># max iteration steps</span></span><br><span class="line">                  maxit=<span class="number">2000</span>,</span><br><span class="line">                  <span class="comment"># delta loss </span></span><br><span class="line">                  abstol=<span class="number">1e-2</span>,</span><br><span class="line">                  <span class="comment"># learning rate</span></span><br><span class="line">                  lr = <span class="number">1e-2</span>,</span><br><span class="line">                  <span class="comment"># regularization rate</span></span><br><span class="line">                  reg = <span class="number">1e-3</span>,</span><br><span class="line">                  <span class="comment"># show results every &#x27;display&#x27; step</span></span><br><span class="line">                  display = <span class="number">100</span>,</span><br><span class="line">                  random.seed = <span class="number">1</span>)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment"># to make the case reproducible.</span></span><br><span class="line">  set.seed(random.seed)</span><br><span class="line"> </span><br><span class="line">  <span class="comment"># total number of training set</span></span><br><span class="line">  N &lt;- nrow(traindata)</span><br><span class="line"> </span><br><span class="line">  <span class="comment"># extract the data and label</span></span><br><span class="line">  <span class="comment"># don&#x27;t need atribute </span></span><br><span class="line">  X &lt;- unname(data.matrix(traindata[,x]))</span><br><span class="line">  Y &lt;- traindata[,y]</span><br><span class="line">  <span class="keyword">if</span>(is.factor(Y)) &#123; Y &lt;- <span class="built_in">as.integer</span>(Y) &#125;</span><br><span class="line">  <span class="comment"># updated: 10.March.2016: create index for both row and col</span></span><br><span class="line">  Y.len   &lt;- <span class="built_in">length</span>(unique(Y))</span><br><span class="line">  Y.set   &lt;- sort(unique(Y))</span><br><span class="line">  Y.index &lt;- cbind(<span class="number">1</span>:N, match(Y, Y.set))</span><br><span class="line"> </span><br><span class="line">  <span class="comment"># number of input features</span></span><br><span class="line">  D &lt;- ncol(X)</span><br><span class="line">  <span class="comment"># number of categories for classification</span></span><br><span class="line">  K &lt;- <span class="built_in">length</span>(unique(Y))</span><br><span class="line">  H &lt;-  hidden</span><br><span class="line"> </span><br><span class="line">  <span class="comment"># create and init weights and bias </span></span><br><span class="line">  W1 &lt;- 0.01*matrix(rnorm(D*H), nrow=D, ncol=H)</span><br><span class="line">  b1 &lt;- matrix(<span class="number">0</span>, nrow=<span class="number">1</span>, ncol=H)</span><br><span class="line"> </span><br><span class="line">  W2 &lt;- 0.01*matrix(rnorm(H*K), nrow=H, ncol=K)</span><br><span class="line">  b2 &lt;- matrix(<span class="number">0</span>, nrow=<span class="number">1</span>, ncol=K)</span><br><span class="line"> </span><br><span class="line">  <span class="comment"># use all train data to update weights since it&#x27;s a small dataset</span></span><br><span class="line">  batchsize &lt;- N</span><br><span class="line">  <span class="comment"># updated: March 17. 2016</span></span><br><span class="line">  <span class="comment"># init loss to a very big value</span></span><br><span class="line">  loss &lt;- 100000</span><br><span class="line"> </span><br><span class="line">  <span class="comment"># Training the network</span></span><br><span class="line">  i &lt;- 0</span><br><span class="line">  <span class="keyword">while</span>(i &lt; maxit &amp;&amp; loss &gt; abstol ) &#123;</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># iteration index</span></span><br><span class="line">    i &lt;- i +<span class="number">1</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># forward ....</span></span><br><span class="line">    <span class="comment"># 1 indicate row, 2 indicate col</span></span><br><span class="line">    hidden.layer &lt;- sweep(X %*% W1 ,<span class="number">2</span>, b1, <span class="string">&#x27;+&#x27;</span>)</span><br><span class="line">    <span class="comment"># neurons : ReLU</span></span><br><span class="line">    hidden.layer &lt;- pmax(hidden.layer, <span class="number">0</span>)</span><br><span class="line">    score &lt;- sweep(hidden.layer %*% W2, <span class="number">2</span>, b2, <span class="string">&#x27;+&#x27;</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># softmax</span></span><br><span class="line">    score.exp &lt;- <span class="built_in">exp</span>(score)</span><br><span class="line">    probs &lt;-sweep(score.exp, <span class="number">1</span>, rowSums(score.exp), <span class="string">&#x27;/&#x27;</span>) </span><br><span class="line"> </span><br><span class="line">    <span class="comment"># compute the loss</span></span><br><span class="line">    corect.logprobs &lt;- -<span class="built_in">log</span>(probs[Y.index])</span><br><span class="line">    data.loss  &lt;- <span class="built_in">sum</span>(corect.logprobs)/batchsize</span><br><span class="line">    reg.loss   &lt;- 0.5*reg* (<span class="built_in">sum</span>(W1*W1) + <span class="built_in">sum</span>(W2*W2))</span><br><span class="line">    loss &lt;- data.loss + reg.loss</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># display results and update model</span></span><br><span class="line">    <span class="keyword">if</span>( i %% display == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span>(!<span class="built_in">is.null</span>(testdata)) &#123;</span><br><span class="line">            model &lt;- <span class="built_in">list</span>( D = D,</span><br><span class="line">                           H = H,</span><br><span class="line">                           K = K,</span><br><span class="line">                           <span class="comment"># weights and bias</span></span><br><span class="line">                           W1 = W1, </span><br><span class="line">                           b1 = b1, </span><br><span class="line">                           W2 = W2, </span><br><span class="line">                           b2 = b2)</span><br><span class="line">            labs &lt;- predict.dnn(model, testdata[,-y])      </span><br><span class="line">            <span class="comment"># updated: 10.March.2016</span></span><br><span class="line">            accuracy &lt;- mean(<span class="built_in">as.integer</span>(testdata[,y]) == Y.set[labs])</span><br><span class="line">            cat(i, loss, accuracy, <span class="string">&quot;\n&quot;</span>)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            cat(i, loss, <span class="string">&quot;\n&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># backward ....</span></span><br><span class="line">    dscores &lt;- probs</span><br><span class="line">    dscores[Y.index] &lt;- dscores[Y.index] -<span class="number">1</span></span><br><span class="line">    dscores &lt;- dscores / batchsize</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    dW2 &lt;- t(hidden.layer) %*% dscores </span><br><span class="line">    db2 &lt;- colSums(dscores)</span><br><span class="line"> </span><br><span class="line">    dhidden &lt;- dscores %*% t(W2)</span><br><span class="line">    dhidden[hidden.layer &lt;= <span class="number">0</span>] &lt;- <span class="number">0</span></span><br><span class="line"> </span><br><span class="line">    dW1 &lt;- t(X) %*% dhidden</span><br><span class="line">    db1 &lt;- colSums(dhidden) </span><br><span class="line"> </span><br><span class="line">    <span class="comment"># update ....</span></span><br><span class="line">    dW2 &lt;- dW2 + reg*W2</span><br><span class="line">    dW1 &lt;- dW1  + reg*W1</span><br><span class="line"> </span><br><span class="line">    W1 &lt;- W1 - lr * dW1</span><br><span class="line">    b1 &lt;- b1 - lr * db1</span><br><span class="line"> </span><br><span class="line">    W2 &lt;- W2 - lr * dW2</span><br><span class="line">    b2 &lt;- b2 - lr * db2</span><br><span class="line"> </span><br><span class="line">  &#125;</span><br><span class="line"> </span><br><span class="line">  <span class="comment"># final results</span></span><br><span class="line">  <span class="comment"># creat list to store learned parameters</span></span><br><span class="line">  <span class="comment"># you can add more parameters for debug and visualization</span></span><br><span class="line">  <span class="comment"># such as residuals, fitted.values ...</span></span><br><span class="line">  model &lt;- <span class="built_in">list</span>( D = D,</span><br><span class="line">                 H = H,</span><br><span class="line">                 K = K,</span><br><span class="line">                 <span class="comment"># weights and bias</span></span><br><span class="line">                 W1= W1, </span><br><span class="line">                 b1= b1, </span><br><span class="line">                 W2= W2, </span><br><span class="line">                 b2= b2)</span><br><span class="line"> </span><br><span class="line">  <span class="built_in">return</span>(model)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Testing-and-Visualization"><a href="#Testing-and-Visualization" class="headerlink" title="Testing and Visualization"></a>Testing and Visualization</h2><p>We have built the simple 2-layers DNN model and now we can test our model. First, the dataset is split into two parts for training and testing, and then use the training set to train model while testing set to measure the generalization ability of our model.</p>
<p>R code</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">########################################################################</span></span><br><span class="line"><span class="comment"># testing</span></span><br><span class="line"><span class="comment">#######################################################################</span></span><br><span class="line">set.seed(<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 0. EDA</span></span><br><span class="line">summary(iris)</span><br><span class="line">plot(iris)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 1. split data into test/train</span></span><br><span class="line">samp &lt;- <span class="built_in">c</span>(sample(<span class="number">1</span>:<span class="number">50</span>,<span class="number">25</span>), sample(<span class="number">51</span>:<span class="number">100</span>,<span class="number">25</span>), sample(<span class="number">101</span>:<span class="number">150</span>,<span class="number">25</span>))</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 2. train model</span></span><br><span class="line">ir.model &lt;- train.dnn(x=<span class="number">1</span>:<span class="number">4</span>, y=<span class="number">5</span>, traindata=iris[samp,], testdata=iris[-samp,], hidden=<span class="number">6</span>, maxit=<span class="number">2000</span>, display=<span class="number">50</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 3. prediction</span></span><br><span class="line">labels.dnn &lt;- predict.dnn(ir.model, iris[-samp, -<span class="number">5</span>])</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 4. verify the results</span></span><br><span class="line">table(iris[-samp,<span class="number">5</span>], labels.dnn)</span><br><span class="line"><span class="comment">#          labels.dnn</span></span><br><span class="line"><span class="comment">#            1  2  3</span></span><br><span class="line"><span class="comment">#setosa     25  0  0</span></span><br><span class="line"><span class="comment">#versicolor  0 24  1</span></span><br><span class="line"><span class="comment">#virginica   0  0 25</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#accuracy</span></span><br><span class="line">mean(<span class="built_in">as.integer</span>(iris[-samp, <span class="number">5</span>]) == labels.dnn)</span><br><span class="line"><span class="comment"># 0.98</span></span><br></pre></td></tr></table></figure>
<p>The data loss in train set and the accuracy in test as below: <a target="_blank" rel="noopener" href="http://www.parallelr.com/r-deep-neural-network-from-scratch/iris_loss_accuracy-2/"><img src="/oneXPU/uploads/2016/02/iris_loss_accuracy-1.png"></a> Then we compare our DNN model with ‘nnet’ package as below codes.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">library(nnet)</span><br><span class="line">ird &lt;- data.frame(rbind(iris3[,,<span class="number">1</span>], iris3[,,<span class="number">2</span>], iris3[,,<span class="number">3</span>]),</span><br><span class="line">                  species = factor(<span class="built_in">c</span>(<span class="built_in">rep</span>(<span class="string">&quot;s&quot;</span>,<span class="number">50</span>), <span class="built_in">rep</span>(<span class="string">&quot;c&quot;</span>, <span class="number">50</span>), <span class="built_in">rep</span>(<span class="string">&quot;v&quot;</span>, <span class="number">50</span>))))</span><br><span class="line">ir.nn2 &lt;- nnet(species ~ ., data = ird, subset = samp, size = <span class="number">6</span>, rang = <span class="number">0.1</span>,</span><br><span class="line">               decay = <span class="number">1e-2</span>, maxit = <span class="number">2000</span>)</span><br><span class="line"> </span><br><span class="line">labels.nnet &lt;- predict(ir.nn2, ird[-samp,], type=<span class="string">&quot;class&quot;</span>)</span><br><span class="line">table(ird$species[-samp], labels.nnet)</span><br><span class="line"><span class="comment">#  labels.nnet</span></span><br><span class="line"><span class="comment">#   c  s  v</span></span><br><span class="line"><span class="comment">#c 22  0  3</span></span><br><span class="line"><span class="comment">#s  0 25  0</span></span><br><span class="line"><span class="comment">#v  3  0 22</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># accuracy</span></span><br><span class="line">mean(ird$species[-samp] == labels.nnet)</span><br><span class="line"><span class="comment"># 0.96</span></span><br></pre></td></tr></table></figure>
<p><strong>Update: 4/28/2016:</strong></p>
<p>Google’s tensorflow released a very cool website to visualize neural network in <a target="_blank" rel="noopener" href="http://playground.tensorflow.org/">here</a>. And we has taught almost all technologies as google’s website in this blog so you can build up with R as well :) <a target="_blank" rel="noopener" href="http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.86675&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification"><img src="/oneXPU/uploads/2016/02/tensorflow.png" alt="tensorflow"></a>  </p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>In this post, we have shown how to implement R neural network from scratch. But the code is only implemented the core concepts of DNN, and the reader can do further practices by:</p>
<ul>
<li>  Solving other classification problem, such as a toy case in <a target="_blank" rel="noopener" href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/">here</a></li>
<li>  Selecting various hidden layer size, activation function, loss function</li>
<li>  Extending single hidden layer network to multi-hidden layers</li>
<li>  Adjusting the network to resolve regression problems</li>
<li>  Visualizing the network architecture, weights, and bias by R, an example in <a target="_blank" rel="noopener" href="https://beckmw.wordpress.com/tag/nnet/">here</a>.</li>
</ul>
<p><strong>In the next <a target="_blank" rel="noopener" href="http://www.parallelr.com/r-dnn-parallel-acceleration/">post</a>, I will introduce how to accelerate this code by multicores CPU and NVIDIA GPU.</strong></p>
<hr>
<p>Notes: 1. The entire source code of this post in <a target="_blank" rel="noopener" href="https://github.com/PatricZhao/ParallelR/blob/master/ParDNN/iris_dnn.R">here</a> 2. The PDF version of this post in <a target="_blank" rel="noopener" href="http://www.parallelr.com/materials/2_DNN/ParallelR_R_DNN_1_BUILD_FROM_SCRATCH.pdf">here</a> 3. Pretty R syntax in this blog is <a target="_blank" rel="noopener" href="http://blog.revolutionanalytics.com/2016/08/farewell-inside-rorg.html" title="Created by Pretty R at inside-R.org">Created by inside-R .org</a> (acquired by MS and dead)</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jitmatrix.github.io/oneXPU/2016/02/02/the-r-parallel-programming-blog/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/oneXPU/images/avatar.gif">
      <meta itemprop="name" content="Patric Zhao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ParallelR">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/oneXPU/2016/02/02/the-r-parallel-programming-blog/" class="post-title-link" itemprop="url">The R Parallel Programming Blog</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-02-02 22:53:13" itemprop="dateCreated datePublished" datetime="2016-02-02T22:53:13+00:00">2016-02-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-19 10:35:25" itemprop="dateModified" datetime="2020-12-19T10:35:25+00:00">2020-12-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/Accelerators/" itemprop="url" rel="index"><span itemprop="name">Accelerators</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/Accelerators/General/" itemprop="url" rel="index"><span itemprop="name">General</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/Accelerators/General/MPI/" itemprop="url" rel="index"><span itemprop="name">MPI</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/Accelerators/General/MPI/MultiCores/" itemprop="url" rel="index"><span itemprop="name">MultiCores</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/Accelerators/General/MPI/MultiCores/Performance-Optimizaiton/" itemprop="url" rel="index"><span itemprop="name">Performance Optimizaiton</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/oneXPU/categories/Accelerators/General/MPI/MultiCores/Performance-Optimizaiton/Vectorization/" itemprop="url" rel="index"><span itemprop="name">Vectorization</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <hr>
<h2 id="DISCLAIMER"><a href="#DISCLAIMER" class="headerlink" title="DISCLAIMER"></a>DISCLAIMER</h2><p><em>This is a personal weblog. The opinions expressed here represent my own and not those of my employer. <strong>Further,</strong> the opinions expressed by the ParallelR Bloggers and those providing comments are theirs alone and do not reflect the opinions of  <a target="_blank" rel="noopener" href="http://parallelr.com/">ParallelR</a>.</em></p>
<hr>
<p>Today, <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Parallel_computing">parallel computing</a> truly is a mainstream technology. But, <a target="_blank" rel="noopener" href="http://www.r-project.org/">stock R</a> is still a single-thread and main memory (RAM) limited software, which really restricts its usage and efficiency against the challenges from very complex model architectures, dynamically configurable analytics models and big data input with billions of parameters and samples.</p>
<p>Therefore, <a target="_blank" rel="noopener" href="http://www.parallelr.com/">ParallelR</a> dedicated on accelerate R by parallel technologies, and our blog will deliver massive parallel technologies and programming tips with real cases in Machine Learning, Data Analysis, Finance fields. And we will cover rich of topics from data vectorization, usages of parallel packages, (<a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/snow/index.html">snow</a>, <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/doParallel/index.html">doparallel</a>, <a target="_blank" rel="noopener" href="https://cran.r-project.org/web/packages/Rmpi/index.html">Rmpi</a>, <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/1.5.2/sparkr.html">SparkR</a>) , to parallel algorithm design and implementation by <a target="_blank" rel="noopener" href="http://www.openmp.org/">OpenMP</a>, <a target="_blank" rel="noopener" href="http://www.openacc.org/">OpenACC</a>, <a target="_blank" rel="noopener" href="http://developer.nvidia.com/gpu-accelerated-libraries">CPU/GPU accelerated libraries</a>, <a target="_blank" rel="noopener" href="http://www.nvidia.com/object/cuda_home_new.html">CUDA C/C++</a> and <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/POSIX_Threads">Pthread</a> in R.</p>
<p>At <a target="_blank" rel="noopener" href="http://www.parallelr.com/blog/">ParallelR Blog</a> you will find useful information about productive, high-performance programming techniques based on commercialized computer architecture, ranging from multicores CPU, GPU, <a target="_blank" rel="noopener" href="http://www.intel.com/content/www/us/en/processors/xeon/xeon-phi-detail.html">Intel Xeon Phi</a>, <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a> to HPC Cluster. As well, you will learn how you can use your existing skills in R in new ways, represented your R codes with structured computational models.</p>
<p>ParallelR Blog is created by Peng Zhao. Peng have rich experience in heterogeneous and parallel computing areas including multi-cores, multi-nodes and accelerators (GPGPU, Intel Xeon Phi) for parallel algorithm design, implementation, debugging and optimizations.</p>
<p> <br>This is Peng. Handsome, Right?<br><a href="/oneXPU/uploads/2016/02/PengZhao@ParallelR.png"><img src="/oneXPU/uploads/2016/02/PengZhao@ParallelR.png"></a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Patric Zhao</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-categories">
            <a href="/oneXPU/categories/">
          
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/oneXPU/tags/">
          
        <span class="site-state-item-count">56</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Patric Zhao</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/oneXPU/lib/anime.min.js"></script>
  <script src="/oneXPU/lib/velocity/velocity.min.js"></script>
  <script src="/oneXPU/lib/velocity/velocity.ui.min.js"></script>

<script src="/oneXPU/js/utils.js"></script>

<script src="/oneXPU/js/motion.js"></script>


<script src="/oneXPU/js/schemes/muse.js"></script>


<script src="/oneXPU/js/next-boot.js"></script>




  















  

  

</body>
</html>
